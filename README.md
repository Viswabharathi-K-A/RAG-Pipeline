ğŸ“š Retrieval-Augmented Generation (RAG) Pipeline
PDF â†’ Chunks â†’ Embeddings â†’ Vector DB â†’ LLM

This project implements a complete Retrieval-Augmented Generation (RAG) pipeline inside a single Jupyter Notebook:
notebooks/RAG_pipeline.ipynb.

The notebook demonstrates the full workflow:

ğŸ“„ Load PDFs

âœ‚ï¸ Split text into chunks

ğŸ¤– Generate embeddings

ğŸ—„ï¸ Store them in ChromaDB

ğŸ” Convert query to embeddings

ğŸ“ˆ Retrieve relevant chunks using cosine similarity

ğŸ§  Use Groq LLM (llama-3.1-8b-instant) for grounded answers

This is a simple, transparent, and educational implementation of RAG.

ğŸ“ Project Structure
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/                 # Input PDF documents
â”‚   â””â”€â”€ vector_store/         # ChromaDB persistent storage
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ RAG_pipeline.ipynb    # Complete RAG pipeline in one notebook
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


Everythingâ€”PDF loading, chunking, embeddings, vector storage, retrieval, and LLM answeringâ€”is implemented inside one notebook.

ğŸ”§ Pipeline Breakdown
1ï¸âƒ£ PDF Loading (ğŸ“„ â†’ ğŸ“)

PDFs in data/pdfs/ are loaded using PyMuPDF or PyPDF, and raw text is extracted for processing.

2ï¸âƒ£ Text Chunking (âœ‚ï¸ â†’ ğŸ“¦)

Uses RecursiveCharacterTextSplitter to break large text into meaningful chunks.

Default settings:

chunk_size = 1000

chunk_overlap = 200

Chunking improves context retrieval and reduces information loss.

3ï¸âƒ£ Embedding Generation (ğŸ§¬)

A SentenceTransformers model (e.g., all-MiniLM-L6-v2) converts:

Document chunks â†’ embedding vectors

Query â†’ vector representation

Both use the same embedding model to ensure consistent similarity scoring.

4ï¸âƒ£ Vector Store (ğŸ—„ï¸ ChromaDB)

Stores embeddings and metadata persistently in:

data/vector_store/


Stored items include:

Chunk text

Embedding vector

Metadata (file name, page number, etc.)

5ï¸âƒ£ Retrieval (ğŸ” Cosine Similarity)

The notebook implements a custom similarity-based retriever:

Embed the query

Compute cosine similarity against all chunk embeddings

Rank chunks by similarity score

Return the top-k relevant chunks

This provides full transparency into the retrieval logic.

6ï¸âƒ£ LLM Answer Generation (ğŸ¤– â†’ ğŸ§ )

The relevant chunks and the user query are sent to Groq LLM (gemma2-9b-it) using LangChain.

Prompt template used:

Use the following context to answer the question concisely.

Context:
{retrieved_chunks}

Question:
{query}

Answer:


The final output is grounded in your private documents.

â–¶ï¸ How to Run the Project:

1ï¸âƒ£ Create & activate a virtual environment
uv venv
source .venv/bin/activate        # Linux/Mac
.\.venv\Scripts\activate         # Windows

2ï¸âƒ£ Install dependencies
uv add -r requirements.txt

3ï¸âƒ£ Launch Jupyter
jupyter notebook
or
jupyter lab

4ï¸âƒ£ Add your PDFs
Place your files in:
data/pdfs/

5ï¸âƒ£ Run RAG_pipeline.ipynb

The notebook will:
Extract text <br>
Create chunks <br>
Generate embeddings <br>
Populate ChromaDB <br>
Accept your queries <br>
Retrieve relevant context <br>
Produce an LLM-powered response <br>

ğŸ“¦ Requirements
Example requirements.txt:

langchain
langchain-core
langchain-community
langchain-text-splitters
langchain-groq
chromadb
sentence-transformers
pymupdf
pypdf
python-dotenv

â­ Features

ğŸ““ All logic in a single easy-to-understand notebook

ğŸ” Transparent retrieval (you can see how similarity is calculated)

ğŸ—‚ï¸ Persistent vector store using ChromaDB

âš¡ Fast inference using Groq LLM

ğŸ§ª End-to-end RAG system for experimentation



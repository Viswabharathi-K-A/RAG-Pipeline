{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653cf3bb",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9588ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document datastructure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cd4e497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Viswabharathi', 'date_created': '2025-11-04'}, page_content='this is the main text content used for creating RAG')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content = \"this is the main text content used for creating RAG\",\n",
    "    metadata = {\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Viswabharathi\",\n",
    "        \"date_created\":\"2025-11-04\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e6b79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c21d395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Welcome! Are you completely new to programming? If not then we presume you will be looking for information about why and how to get started with Python. Fortunately an experienced programmer in any programming language (whatever it may be) can pick up Python very quickly. It's also easy for beginners to use and learn, so jump in!\n",
    "\n",
    "Installing\n",
    "Installing Python is generally easy, and nowadays many Linux and UNIX distributions include a recent Python. Even some Windows computers (notably those from HP) now come with Python already installed. If you do need to install Python and aren't confident about the task you can find a few notes on the BeginnersGuide/Download wiki page, but installation is unremarkable on most platforms.\n",
    "\n",
    "Learning\n",
    "Before getting started, you may want to find out which IDEs and text editors are tailored to make Python editing easy, browse the list of introductory books, or look at code samples that you might find helpful.\n",
    "\n",
    "There is a list of tutorials suitable for experienced programmers on the BeginnersGuide/Tutorials page. There is also a list of resources in other languages which might be useful if English is not your first language.\n",
    "\n",
    "The online documentation is your first port of call for definitive information. There is a fairly brief tutorial that gives you basic information about the language and gets you started. You can follow this by looking at the library reference for a full description of Python's many libraries and the language reference for a complete (though somewhat dry) explanation of Python's syntax. If you are looking for common Python recipes and patterns, you can browse the ActiveState Python Cookbook\n",
    "\n",
    "Looking for Something Specific?\n",
    "If you want to know whether a particular application, or a library with particular functionality, is available in Python there are a number of possible sources of information. The Python web site provides a Python Package Index (also known as the Cheese Shop, a reference to the Monty Python script of that name). There is also a search page for a number of sources of Python-related information. Failing that, just Google for a phrase including the word ''python'' and you may well get the result you need. If all else fails, ask on the python newsgroup and there's a good chance someone will put you on the right track.\n",
    "\n",
    "Frequently Asked Questions\n",
    "If you have a question, it's a good idea to try the FAQ, which answers the most commonly asked questions about Python.\n",
    "\n",
    "Looking to Help?\n",
    "If you want to help to develop Python, take a look at the developer area for further information. Please note that you don't have to be an expert programmer to help. The documentation is just as important as the compiler, and still needs plenty of work!\n",
    "    \n",
    "    \n",
    "    \"\"\",\n",
    "\"../data/text_files/Machine_learning.txt\": \"\"\"\n",
    "Over the past two decades Machine Learning has become one of the mainstays of information technology and with that, a rather central, albeit usually\n",
    "hidden, part of our life. With the ever increasing amounts of data becoming\n",
    "available there is good reason to believe that smart data analysis will become\n",
    "even more pervasive as a necessary ingredient for technological progress.\n",
    "The purpose of this chapter is to provide the reader with an overview over\n",
    "the vast range of applications which have at their heart a machine learning\n",
    "problem and to bring some degree of order to the zoo of problems. After\n",
    "that, we will discuss some basic tools from statistics and probability theory,\n",
    "since they form the language in which many machine learning problems must\n",
    "be phrased to become amenable to solving. Finally, we will outline a set of\n",
    "fairly basic yet effective algorithms to solve an important problem, namely\n",
    "that of classification. More sophisticated tools, a discussion of more general\n",
    "problems and a detailed analysis will follow in later parts of the book.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d613522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content=\"Welcome! Are you completely new to programming? If not then we presume you will be looking for information about why and how to get started with Python. Fortunately an experienced programmer in any programming language (whatever it may be) can pick up Python very quickly. It's also easy for beginners to use and learn, so jump in!\\n\\nInstalling\\nInstalling Python is generally easy, and nowadays many Linux and UNIX distributions include a recent Python. Even some Windows computers (notably those from HP) now come with Python already installed. If you do need to install Python and aren't confident about the task you can find a few notes on the BeginnersGuide/Download wiki page, but installation is unremarkable on most platforms.\\n\\nLearning\\nBefore getting started, you may want to find out which IDEs and text editors are tailored to make Python editing easy, browse the list of introductory books, or look at code samples that you might find helpful.\\n\\nThere is a list of tutorials suitable for experienced programmers on the BeginnersGuide/Tutorials page. There is also a list of resources in other languages which might be useful if English is not your first language.\\n\\nThe online documentation is your first port of call for definitive information. There is a fairly brief tutorial that gives you basic information about the language and gets you started. You can follow this by looking at the library reference for a full description of Python's many libraries and the language reference for a complete (though somewhat dry) explanation of Python's syntax. If you are looking for common Python recipes and patterns, you can browse the ActiveState Python Cookbook\\n\\nLooking for Something Specific?\\nIf you want to know whether a particular application, or a library with particular functionality, is available in Python there are a number of possible sources of information. The Python web site provides a Python Package Index (also known as the Cheese Shop, a reference to the Monty Python script of that name). There is also a search page for a number of sources of Python-related information. Failing that, just Google for a phrase including the word ''python'' and you may well get the result you need. If all else fails, ask on the python newsgroup and there's a good chance someone will put you on the right track.\\n\\nFrequently Asked Questions\\nIf you have a question, it's a good idea to try the FAQ, which answers the most commonly asked questions about Python.\\n\\nLooking to Help?\\nIf you want to help to develop Python, take a look at the developer area for further information. Please note that you don't have to be an expert programmer to help. The documentation is just as important as the compiler, and still needs plenty of work!\\n\\n\\n    \")]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2033589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\Machine_learning.txt'}, page_content='\\nOver the past two decades Machine Learning has become one of the mainstays of information technology and with that, a rather central, albeit usually\\nhidden, part of our life. With the ever increasing amounts of data becoming\\navailable there is good reason to believe that smart data analysis will become\\neven more pervasive as a necessary ingredient for technological progress.\\nThe purpose of this chapter is to provide the reader with an overview over\\nthe vast range of applications which have at their heart a machine learning\\nproblem and to bring some degree of order to the zoo of problems. After\\nthat, we will discuss some basic tools from statistics and probability theory,\\nsince they form the language in which many machine learning problems must\\nbe phrased to become amenable to solving. Finally, we will outline a set of\\nfairly basic yet effective algorithms to solve an important problem, namely\\nthat of classification. More sophisticated tools, a discussion of more general\\nproblems and a detailed analysis will follow in later parts of the book.\\n\\n\\n'),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content=\"Welcome! Are you completely new to programming? If not then we presume you will be looking for information about why and how to get started with Python. Fortunately an experienced programmer in any programming language (whatever it may be) can pick up Python very quickly. It's also easy for beginners to use and learn, so jump in!\\n\\nInstalling\\nInstalling Python is generally easy, and nowadays many Linux and UNIX distributions include a recent Python. Even some Windows computers (notably those from HP) now come with Python already installed. If you do need to install Python and aren't confident about the task you can find a few notes on the BeginnersGuide/Download wiki page, but installation is unremarkable on most platforms.\\n\\nLearning\\nBefore getting started, you may want to find out which IDEs and text editors are tailored to make Python editing easy, browse the list of introductory books, or look at code samples that you might find helpful.\\n\\nThere is a list of tutorials suitable for experienced programmers on the BeginnersGuide/Tutorials page. There is also a list of resources in other languages which might be useful if English is not your first language.\\n\\nThe online documentation is your first port of call for definitive information. There is a fairly brief tutorial that gives you basic information about the language and gets you started. You can follow this by looking at the library reference for a full description of Python's many libraries and the language reference for a complete (though somewhat dry) explanation of Python's syntax. If you are looking for common Python recipes and patterns, you can browse the ActiveState Python Cookbook\\n\\nLooking for Something Specific?\\nIf you want to know whether a particular application, or a library with particular functionality, is available in Python there are a number of possible sources of information. The Python web site provides a Python Package Index (also known as the Cheese Shop, a reference to the Monty Python script of that name). There is also a search page for a number of sources of Python-related information. Failing that, just Google for a phrase including the word ''python'' and you may well get the result you need. If all else fails, ask on the python newsgroup and there's a good chance someone will put you on the right track.\\n\\nFrequently Asked Questions\\nIf you have a question, it's a good idea to try the FAQ, which answers the most commonly asked questions about Python.\\n\\nLooking to Help?\\nIf you want to help to develop Python, take a look at the developer area for further information. Please note that you don't have to be an expert programmer to help. The documentation is just as important as the compiler, and still needs plenty of work!\\n\\n\\n    \")]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory loader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader = DirectoryLoader(\"../data/text_files\",\n",
    "                             glob = '**/*.txt',   #Pattern to match files,\n",
    "                             loader_cls = TextLoader,     ##loader class to use\n",
    "                             loader_kwargs = {'encoding': 'utf-8'},\n",
    "                             show_progress = False)\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62678cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='CS570 Spring 2025: Analysis of Algorithms         Exam II\\u200b\\n \\n \\nPoints \\n \\nPoints \\nProblem 1 \\n16 \\nProblem 4 \\n17 \\nProblem 2 \\n9 \\nProblem 5 \\n18 \\nProblem 3 \\n20 \\nProblem 6 \\n18 \\nTotal 98 \\n\\u200b\\n \\n \\nFirst name \\n \\nLast Name \\n \\nStudent ID \\n \\n \\n \\n \\nInstructions: \\n1.\\u200b\\nThis is a 2-hr exam. Closed book and notes. No electronic devices or internet access. \\n2.\\u200b\\nA single double sided 8.5in x 11in cheat sheet is allowed. \\n3.\\u200b\\nIf a description to an algorithm or a proof is required, please limit your description or \\nproof to within 150 words, preferably not exceeding the space allotted for that question. \\n4.\\u200b\\nNo space other than the pages in the exam booklet will be scanned for grading. \\n5.\\u200b\\nIf you require an additional page for a question, you can use the extra page provided \\nwithin this booklet. However please indicate clearly that you are continuing the solution \\non the additional page. \\n6.\\u200b\\nDo not detach any sheets from the booklet. \\n7.\\u200b\\nIf using a pencil to write the answers, make sure you apply enough pressure, so your \\nanswers are readable in the scanned copy of your exam. \\n8.\\u200b\\nDo not write your answers in cursive scripts. \\n9.\\u200b\\nThis exam is printed double sided. Check and use the back of each page.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Student ID: \\n1)\\u200b\\n16 pts (2 pts each)\\u200b\\nMark the following statements as TRUE or FALSE by circling the correct answer. No need to \\nprovide any justification. \\n \\n[ TRUE/FALSE ] \\nBellman-Ford is asymptotically faster than Dijkstraâ€™s algorithm when dealing with graphs \\nthat only have non-negative edge weights. \\n[ TRUE/FALSE ]\\u200b\\nIn a maximum flow problem in a flow network with non-zero integer capacities, there \\nmust exist at least one integer-valued maximum flow f with non-zero v(f). \\n[ TRUE/FALSE ] \\nGiven a feasible circulation in a network with unlimited capacities, if the flow on each \\nedge is increased by one unit, we will get another feasible circulation.  \\n \\n[ TRUE/FALSE ] \\nSuppose an edge e is not saturated due to max flow f in a Flow Network. Then increasing \\neâ€™s capacity will not increase the max flow value of the network. \\n\\u200b\\n[ TRUE/FALSE ] \\nThe worst-case time complexity of any dynamic programming algorithm with n3 unique \\nsubproblems is â„¦(n3). \\n \\n[ TRUE/FALSE ] \\nConsider the following recurrence formula  \\u200b\\n \\n \\n\\u200b\\nwhere A and B are fixed input arrays and subproblems OPT(j) are defined for j = 1, â€¦, n. \\nThen, this will lead to an O(n) dynamic programming algorithm. \\n \\n[ TRUE/FALSE ] \\nBy definition, all dynamic programming algorithms must run in polynomial time with \\nrespect to the input size (either in terms of the number of integers, or in terms of the \\nnumber of bits in the input). \\n \\n[ TRUE/FALSE ] \\nGiven a flow network G and its max flow f, we can always find an s-t  cut (A,B) in G \\nwhere all edges e crossing the cut either have f(e) = C(e) or f(e) = 0.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Student ID: \\n2)\\u200b\\n9 pts  -   3 pts each. No partial credit \\n \\nI. What is the capacity of the minimum s-t cut in the following flow network? \\n \\n \\nA) 5 \\nB) 6 \\nC) 7 \\nD) 8 \\nSolution: C \\n \\nII.  \\xa0Which of the following statements is/are False regarding dynamic programming? \\nSelect all correct answers! \\nA) It solves problems by breaking them into overlapping subproblems. \\nB) It can be implemented using a bottom-up approach known as tabulation. \\nC) A problem of size n must have subproblems of size n/b where b>1 is an integer\\u200b\\n     constant  \\nD) It can utilize memoization to store values of optimal solutions for unique subproblems \\nto avoid redundant calculations. \\nSolution: C \\n \\nIII. Which of the following statements is/are True about the Ford-Fulkerson algorithm? \\nSelect all correct answers! \\nA)\\u200b For integer edge capacities, it terminates in at most v(f) iterations, where v(f) is the \\nmaximum flow value. \\nB)\\u200b For integer edge capacities, it terminates in at most C iterations, where C is the total \\ncapacity of edges out of source. \\nC)\\u200b It may converge to an incorrect value of max flow for non-integer edge capacities. \\nD)\\u200b Assuming it terminates, it will always find the same maximum flow f independent of \\nthe choice of augmenting paths. \\nSolution: A,B,C'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Student ID: \\n3)\\u200b 20 pts  \\n \\nConsider the flow network below with demands and lower bounds. \\u200b\\nNotation: (le,Ce) show lower bound and capacity on each edge. Numbers assigned to \\nnodes show the demand value at that node. \\n \\nFollow the steps as described in class to determine if a feasible circulation exists \\na)\\u200b Convert the given network to an equivalent one with no lower bounds. Show all your \\nwork. (6 pts) \\n \\n \\nNo lower bounds'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Student ID: \\n \\nRUBRIC: \\n(3 points) Draw the imbalance graph correctly \\n(3 points) Draw the final graph with no lower bounds correctly \\n-1 point for every wrong number \\n \\n \\n \\n \\n \\nb) Convert the network obtained in a) to an equivalent one with no demands (i.e., a flow \\nnetwork) (2 pts) \\n \\nFlow Network (i.e., with no demands) \\n \\nRUBRIC:\\u200b\\n(1 point) Create a super source and super sink \\n(1 point) Connect nodes correctly to either source/sink \\n0 points if no super source or super sink was created \\n \\n \\n \\n \\n \\nc) In the network obtained in b), compute max flow using Scaled version of Ford \\nFulkerson algorithm. Only show the different scaling iterations marked with the \\ncorresponding Delta value, and for each scaling iteration, show the augmenting path(s) \\nfound with the respective augmenting flow value. No need to show residual graphs. \\u200b\\n(10 pts)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Student ID: \\nMax flow with scaled version of Ford Fulkerson algorithm \\n \\n \\nRUBRIC: \\n(- 5 points):  Did not use scaled version of Ford Fulkerson algorithm \\n(- 2 points): Incorrect max flow value \\n(- 2 points): Missed delta 8, 4 and 1 in calculation \\n(- 1 point): For each minor calculation error'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Student ID: \\n \\nd) Based on your solution in part c, explain whether the originally given network has a \\nfeasible circulation or not. (2 pts) \\nSince max flow not equal to total demand, the circulation is not feasible \\n \\n(2 points) correctly mention that circulation is not feasible because max flow not equal to \\ntotal demand \\n0 points if there is no mention of max flow being not equal to demand AND circulation \\nnot being feasible'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Student ID: \\n4)\\u200b 17 pts \\nConsider the directed graph below.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\na)\\u200b Use the Bellman-Ford algorithm to find the negative cycle in this graph. In other \\nwords, you must solve this problem numerically with the given edge weights and  \\n     Explain how the destination node t is determined (3 pts) \\ncreate new node t* and connect all nodes to t* with distance of 0. \\n \\n1 - point for creating new node  \\n2 - for connecting all the nodes distance 0 (or any number as long as it is the same for \\nall nodes) \\n0 - no creation of a new node \\n \\n \\nb)\\u200b  State how many iterations of Bellman-Ford are required (2 pts) \\nn=7 iterations \\n2 - Point for stating the correct number of iterations  \\n0 - Incorrect answer'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Student ID: \\nc)\\u200b  Show each iteration of Bellman-Ford by showing subproblem values in a table \\nwith columns representing nodes and rows representing iterations (6 pts) \\n \\n \\na \\nâˆ \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\nb \\nâˆ \\n0 \\n0 \\n0 \\n-1 \\n-2 \\n-2 \\n-2 \\nc \\nâˆ \\n0 \\n0 \\n-5 \\n-6 \\n-6 \\n-6 \\n-7 \\nd \\nâˆ \\n0 \\n-7 \\n-8 \\n-8 \\n-8 \\n-9 \\n-10 \\ne \\nâˆ \\n0 \\n-1 \\n-1 \\n-1 \\n-2 \\n-3 \\n-3 \\nf \\nâˆ \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\nt* \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n \\nw/ up \\nto 0 \\nedges \\nw/ up \\nto 1 \\nedges \\nw/ up \\nto 2 \\nedges \\nw/ up \\nto 3 \\nedges \\nw/ up \\nto 4 \\nedges \\nw/ up \\nto 5 \\nedges \\nw/ up \\nto 6 \\nedges \\nw/ up \\nto 7 \\nedges \\n \\n1 - Point for each correct iteration. Whole iteration must be correct to get 1 full point. \\n \\n \\nd)\\u200b Show how it is determined that a negative cycle exists using your results (2 pts) \\nSome of the numbers in iteration 7 still go down. This is an indication that a shorter \\npath using (n=7 edges) to the destination exists in the graph. Since the graph only has \\nn=7 nodes, this path must contain a negative cycle.  \\n \\n2 - Points for correctly stating the iteration number: 7 and the logic behind selecting \\niter 7. \\n \\ne)\\u200b Show how the negative cycle is found using your calculated results (4 pts) \\nWe start from any of the nodes where the distance goes down at iteration 7 and find \\nthe shortest path (containing negative cycle) to t*.  \\ne.g. start at d. \\n- dâ€™s shortest distance to t* comes from e \\n- eâ€™s shortest distance to t* comes from b \\n- bâ€™s shortest distance to t* comes from c \\n- câ€™s shortest distance to t* comes from d \\nWe have come back to the starting point of the cycle. Therefore debcd forms a \\nnegative cycle. \\n \\n1 - Point for stating that distance goes down at 7th Iteration  \\n2 - Points for explaining the shortest distance method and the loop. \\n1- Point for giving the correct cycle : debcd \\n0 - If does not start from iteration 7.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Student ID: \\n5)  18 pts \\nSuppose USC wants to send m teams of students into a coding competition, each team \\nconsisting of one freshman and one sophomore (a student cannot be on multiple teams). \\nSelected students are a set of freshmen {s1 â€¦ sm} and sophomores {sm+1 â€¦ s2m} who need \\nto be matched to form teams. Further, each team formed needs to be assigned a professor \\nto mentor them. For this, a set of professors {p1 â€¦ pn} are available, but to ensure some \\nload-balancing, each professor can mentor up to 5 teams each and must mentor at least \\none team. Each student si (i = 1, â€¦, 2m) is asked for a subset of professors Xi theyâ€™d like \\nto be mentored by, and a professor pk can be assigned to mentor a team (si, sj) if pk is in \\nboth Xi and Xj. \\n \\na)\\u200b Design a network flow algorithm to determine if it is feasible to form m teams and \\nassign mentors given all the constraints above. (12 points) \\n \\nWe will have 3 layers - freshmen, sophomores, and professors. The idea would be to have \\na â€˜unit of flowâ€™ represent a team with its mentor. Professor layer has to be in the middle \\nso that it can connect to both the other layers, which is required to capture the \\nconnectivity constraints posed by the Xiâ€™s. \\n \\nNetwork construction: \\n1) Node S with demand -m, node T with +m. \\n2) First layer {s1 â€¦ sm} as nodes, with edges from S to all with cap 1 (optional: lb 1),\\u200b\\n3) Second layer pjâ€™s as EDGES with lb 1, cap 5,\\u200b\\n4) 3rd layer {sm+1 â€¦ s2m} as nodes, with edges to T from all with cap 1 (optional: lb 1).\\u200b\\n5) Add edge between a professor pk and a student si (directed along S to T) if pk is in Xi. \\n \\nAlternative with no S/T: \\nNo need to add S,T (and their edges) - Directly add demand -1 to freshmen nodes, and +1 \\nto sophomore nodes. \\n \\nthe freshmen and sophomore nodes have symmetric design and their constructions can be \\nflipped (i.e. all arrow directions and demands) \\n \\nRubric: \\n2 pts - representing freshmen and sophomore as nodes \\n3 pts - Nodes S, T (with edges to corresponding student layers) and demands -m, +m\\u200b\\nOR simply adding demands -1, +1 to all the freshmen, sophomores resp. \\n4 pts -  Points for professors represented as edges (2) with correct LB and cap (2)  \\n3 pts - Points for connecting EACH of the student layers to the professor layer (2) and \\nmentioning si is connected to pk if it is contained in Xi (1)..'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Student ID: \\nb)\\u200b Prove correctness of your algorithm. (6 points) \\n \\nForming m teams and assigning mentors given all the constraints is feasible if and only if \\nthe constructed network has feasible circulation. \\n \\n=> ) Suppose m teams with mentors can be formed to satisfy all the constraints.\\u200b\\nFor each team (si, sj) and their mentor pk , assign flow of 1 on the path S - si - edge pk \\n- sj - T (if using the â€˜no S/T approachâ€™, simply the path si - edge pk - sj ).\\u200b\\nThis path exists as per the step 5) of construction since pk must be in both Xi and Xj.\\u200b\\nEach student being in exactly 1 team (thus, having m total teams) satisfies the demands \\non S and T and the capacities of the corresponding edges (or demands on all the student \\nnodes if using â€˜no S/T approachâ€™).\\u200b\\nLoad-balancing of mentors ensures that LB and cap on each edge pk is satisfied. \\nThus, we have feasible circulation in the network. \\n \\n<= ) Suppose the network has feasible circulation. \\nFor every unit flow on an edge pk , we can trace it to either side using flow \\nconservation (since no demands on endpoints of edge pk and the student nodes) to \\nget a path S - si - edge pk - sj - T with unit flow (if using the â€˜no S/T approachâ€™, simply \\nthe path si - edge pk - sj using flow conservation on endpoints of edge pk ). For each \\nsuch path, form the team (si, sj) and assign pk as their mentor.\\u200b\\nstep 5) of construction ensures pk must be in both Xi and Xj. \\nDemands on S, T ensure there are m teams. Capacities 1 on edges from S to freshmen \\nand sophomores to T ensure each student is in exactly one team. (ensured by demands on \\nall the student nodes if using â€˜no S/T approachâ€™). \\nLoad-balancing of mentors is ensured by the LB and cap on each edge pk. \\n \\n \\n2 - Point for claim thatâ€™s clear and precise (for a nearly correct network) \\n2 - Points for proof in forward direction  \\n2 - Points for proof in backward direction'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Student ID: \\n6)  18 pts \\nA film screening festival is being hosted in your neighborhood which will last for D days, \\nwith one movie screened each day. The list of movies consists of English, French and \\nKorean movies.\\u200b\\nThe movie screened on day d \\nâ—\\u200b is in language Ld âˆˆ {En, Fr, Ko}, \\nâ—\\u200b has a runtime of td minutes (integer) and \\nâ—\\u200b has a rating of Rd (may not be integer). \\nGiven your busy schedule, you have decided you can spend at most T time for the film \\nfestival (in minutes, integer). You want to watch the best movies as much as possible, so \\nyou set the objective of maximizing the total rating of movies you watch. Your only other \\nconstraint is that you do NOT want to watch two movies in the same language back to \\nback. For instance, if you watch a movie on day 10 and day 14 (skipping days 11-13), \\nthen L10 and L14 must be different languages. \\nUse dynamic programming to compute the maximum total rating of movies you can \\nwatch given the constraints above. \\nDefine (in plain English) subproblems to be solved. (4 pts) \\n \\nOPT_en(d,t) = Max total rating possible from watching movies from days 1, â€¦, d \\nwith t minutes available, with the last movie watched being in English. \\nSimilarly, Opt_fr(d,t) and Opt_ko(d,t). \\n \\nEquivalently, OPT(d, t, l) = Max total rating possible from watching movies from \\ndays 1, â€¦, d with t minutes available, with the last movie watched being in \\nLanguage l.  \\n \\n4pt - correct subproblem \\n2pt - (if not correct) close enough subproblem, including the following cases: \\n1.\\u200b Miss to separate different languages for 2-D definition. \\n2.\\u200b Define \\n with the last parameter specified to the exact \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘,  ğ‘¡, ğ¿ğ‘‘]\\nlanguage on day d.  \\n \\nNote that the two solutions above only differ in notation, not the actual subproblems.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Student ID: \\nb) Write the recurrence relation(s) for the subproblems (6 pts) \\n \\nThere should be a way to decide if you can watch the movie on day d by controlling \\nwhich language is permissible.  \\n \\nIf \\n: \\nğ¿ğ‘‘==  ğ¸ğ‘›\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘,  ğ‘¡] =  ğ‘šğ‘ğ‘¥ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘âˆ’1,  ğ‘¡],  ğ‘…ğ‘‘+ ğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘]}\\n{\\n}\\nIf \\n: \\nğ¿ğ‘‘ ! = ğ¸ğ‘›\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘âˆ’1, ğ‘¡]\\nSimilar recurrence for \\n and \\n (Condition checks for respective \\nğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡]\\nğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡]\\nlanguage L to include the 2nd term inside â€˜maxâ€™ which calls subproblems for Lâ€™ and \\nLâ€™â€™, ignores the 2nd term if condition not true) \\n \\nRubrics breakdown: \\n \\n3pt - Correct for watching film on day d. \\nOnly when \\n,  \\nğ¿ğ‘‘! = ğ¿ğ‘‘âˆ’1\\n-- 2-Dimension \\n similar for \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘…ğ‘‘+ ğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘]}\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡], ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡]\\n-- 3-Dimension \\n \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] = ğ‘…ğ‘‘+\\nğ‘™ğ‘™âˆˆ{ğ¸ğ‘›, ğ¹ğ‘Ÿ,ğ¾ğ‘œ} && ğ‘™ğ‘™!=ğ‘™\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘  , ğ‘™ğ‘™]} \\n \\n \\n3pt - Correct for skipping case.  \\n-- 2-Dimension \\n for each language \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡]\\n-- 3-Dimension \\n \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] = ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡, ğ‘™]\\n \\n0pt - if movies are skipped without checking the conditions. \\n \\n \\nc) Using the recurrence formula in part b, write pseudocode using iteration to compute \\nthe minimum number of packages to meet the objective. (6 pts) \\n \\n2pt - Correct pseudocode (order of computation (loop) increasing in d, t) \\nPrecondition: OPT definition in a) and recurrence formula in b) have to be close to \\ncorrect! - 0pt otherwise \\n1pt - each minor error (if more than one, 0pt)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Student ID: \\n \\nSpecify base cases and their values (3 pts) \\nOPT_en[0,t] = OPT_fr[0,t] = OPT_ko[0,t] = 0 \\nOPT_en[i,t] = -inf for t<0 // okay to skip this base case if otherwise accounted for by \\na condition in the recurrence, that ignores a term when t_d > t etc. \\n \\n1pt - Correct base case with zero available day. (no partial credit) \\n-- 2-Dimension \\n \\nâˆ€ğ‘¡âˆˆ[0, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[0, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[0, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[0, ğ‘¡] = 0\\n-- 3-Dimension \\n \\nâˆ€ğ‘¡âˆˆ[0, ğ‘‡], ğ‘™âˆˆ{ğ¸ğ‘›, ğ¹ğ‘Ÿ, ğ¾ğ‘œ},  ğ‘‚ğ‘ƒğ‘‡[0, ğ‘¡, ğ‘™] = 0\\n \\n2pt - Correct base case with negative available time.  \\n-- 2-Dimension \\n \\nâˆ€ğ‘¡< 0, ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡] =  âˆ’âˆ\\n-- 3-Dimension \\n \\nâˆ€ğ‘¡< 0,  ğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] =  âˆ’âˆ\\n \\n \\n \\nSpecify where the final answer can be found? (1 pt) \\n \\n1pt - Final answer location. (no partial credit) \\n-- 2-Dimension \\n \\nğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ·, ğ‘‡]}\\n-- 3-Dimension \\n \\nğ‘™âˆˆ{ğ¸ğ‘›,ğ¹ğ‘Ÿ,ğ¾ğ‘œ}\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ·, ğ‘‡, ğ‘™]}\\n \\nd) What is the complexity of your solution? (1 pt) \\nIs this an efficient solution? (1 pt) \\nO(DT).\\u200b\\nPseudo-polynomial run time (due to non-polynomial dependence on the value T), i.e., \\nthis is not an efficient solution. \\n \\n \\n1pt - Correct Time Complexity O(D*T). \\n1pt - Stating not efficient'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 0}, page_content='1 \\nHomework: Web Crawling \\n \\n \\n1. Objective \\n \\nIn this assignment, you will work with a simple web crawler to measure aspects of a crawl, study the \\ncharacteristics of the crawl, download web pages from the crawl and gather webpage metadata, all \\nfrom pre-selected news websites. \\n \\n2. Preliminaries \\n \\nTo begin we will make use of an existing open source Java web crawler called crawler4j. This crawler \\nis built upon the open source crawler4j library which is located on github. For complete details on \\ndownloading and compiling see  \\n \\nAlso see the document â€œInstructions for Installing Eclipse and Crawler4jâ€ located on the Assignments \\nweb page for help. \\nNote:  You can use any IDE of your choice. But we have provided installation instructions for Eclipse \\nIDE only \\n \\n3. Crawling \\n \\nYour task is to configure and compile the crawler and then have it crawl a news website. In the interest \\nof distributing the load evenly and not overloading the news servers, we have pre-assigned the news \\nsites to be crawled according to your USC ID number, given in the table below. \\n \\nThe maximum pages to fetch can be set in crawler4j and it should be set to 20,000 to ensure a \\nreasonable execution time for this exercise. Also, maximum depth should be set to 16 to ensure that \\nwe limit the crawling. \\n \\nYou should crawl only the news websites assigned to you, and your crawler should be configured so \\nthat it does not visit pages outside of the given news website!  \\n \\nUSC ID ends with News Sites to Crawl \\nNe\\nwsS\\nite \\nNa\\nme \\nRoot URL \\n01~20 \\nNY Times \\nnytimes \\nhttps://www.nytimes.com \\n21~40 \\nWall Street Journal \\nwsj \\nhttps://www.wsj.com \\n41~60 \\nFox News \\nfoxnews \\nhttps://www.foxnews.com \\n61~80 \\nUSA Today \\nusatoday \\nhttps://www.usatoday.com \\n81~00 \\nLos Angeles Times \\nlatimes \\nhttps://www.latimes.com \\n \\n \\n \\n \\nLimit your crawler so it only visits HTML, doc, pdf and different image format URLs and record the \\nmeta data for those file types'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='2 \\n4. Collecting Statistics \\n \\nYour primary task is to enhance the crawler so it collects information about: \\n1. the URLs it attempts to fetch, a two column spreadsheet, column 1 containing the URL and \\ncolumn 2 containing the HTTP/HTTPS status code received; name the file fetch_NewsSite.csv \\n(where the name â€œNewsSiteâ€ is replaced by the news website name in the table above that you \\nare crawling). The number of rows should be no more than 20,000 as that is our pre-set limit. \\nColumn names for this file can be URL and Status \\n2. the files it successfully downloads, a four column spreadsheet, column 1 containing the \\nURLs successfully downloaded, column 2 containing the size of the downloaded file (in \\nBytes, or you can choose your own preferred unit (bytes,kb,mb)), column 3 containing the \\n# of outlinks found, and column 4 containing the resulting content-type; name the file \\nvisit_NewsSite.csv; clearly the number of rows will be less than the number of rows in \\nfetch_NewsSite.csv \\n3. all of the URLs (including repeats) that were discovered and processed in some way; a two \\ncolumn spreadsheet where column 1 contains the encountered URL and column two an \\nindicator of whether the URL a. resides in the website (OK), or b. points outside of the website \\n(N_OK). (A file points out of the website if its URL does not start with the initial host/domain \\nname, e.g. when crawling USA Today news website all inside URLs must start with \\n.) Name the file urls_NewsSite.csv. This file will be much larger than \\nfetch_*.csv and visit_*.csv.   \\nFor example for New York Times- the URL \\n and the \\nURL \\n are both considered as residing in the same website \\nwhereas the following URL is not considered to be in the same website, \\nhttp://store.nytimes.com/ \\n \\nNote1: you should modify the crawler so it outputs the above data into three separate csv files; \\nyou will use them for processing later; \\nNote2: all uses of NewsSite above should be replaced by the name given in the column labeled \\nNewsSite Name in the table on page 1. \\nNote 3: You should denote the units in size column of visit.csv. The best way would be to write \\nthe units that you are using in column header name and let the rest of the size data be in numbers \\nfor easier statistical analysis. The hard requirement is only to show the units clearly and \\ncorrectly. \\n \\n \\nBased on the information recorded by the crawler in the output files above, you are to collate the \\nfollowing statistics for a crawl of your designated news website: \\n \\nâ— Fetch statistics: \\no # fetches attempted: \\nThe total number of URLs that the crawler attempted to fetch. This is usually equal to the \\nMAXPAGES setting if the crawler reached that limit; less if the website is smaller than that. \\no # fetches succeeded: \\nThe number of URLs that were successfully downloaded in their entirety, i.e. returning a \\nHTTP status code of 2XX. \\no # fetches failed or aborted: \\nThe number of fetches that failed for whatever reason, including, but not limited to: HTTP'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='3 \\nredirections (3XX), client errors (4XX), server errors (5XX) and other network-related \\nerrors.1 \\nâ— Outgoing URLs: statistics about URLs extracted from visited HTML pages \\no Total URLs extracted: \\nThe grand total number of URLs extracted (including repeats) from all visited pages \\no # unique URLs extracted: \\nThe number of unique URLs encountered by the crawler \\no # unique URLs within your news website: \\nThe number of unique URLs encountered that are associated with the news website, \\ni.e. the URL begins with the given root URL of the news website, but the remainder of the \\nURL is distinct \\no # unique URLs outside the news website: \\nThe number of unique URLs encountered that were not from the news website. \\n \\nâ— Status codes: number of times various HTTP status codes were encountered during crawling, \\nincluding (but not limited to): 200, 301, 401, 402, 404, etc.  \\nâ— File sizes: statistics about file sizes of visited URLs â€“ the number of files in each size range \\n(See Appendix A). \\no 1KB = 1024B; 1MB = 1024KB \\nâ—  Content Type: a list of the different content-types encountered \\n \\nThese statistics should be collated and submitted as a plain text file whose name is \\nCrawlReport_NewsSite.txt, following the format given in Appendix A at the end of this document. \\nMake sure you understand the crawler code and required output before you commence collating \\nthese statistics.  \\n \\nFor efficient crawling it is a good idea to have multiple crawling threads. You are required to use \\nmultiple threads in this exercise. crawler4j supports multi-threading and our examples show \\nsetting the number of crawlers to seven (see the line in the code int numberOfCrawlers = \\n7;). However, if you do a naive implementation the threads will trample on each other when \\noutputting to your statistics collection files. Therefore you need to be a bit smarter about how to \\ncollect the statistics, and crawler4j documentation has a good example of how to do this. See both \\nof the following links for details: \\n  \\n \\nand \\nhttps://github.com/yasserg/crawler4j/blob/master/crawler4j-examples/crawler4j-examples-\\nbase/src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorCrawler.java \\n \\n All the information that you are required to collect can be derived by processing the crawler \\noutput. \\n5. FAQ \\n \\nQ: For the purposes of counting unique URLs, how to handle URLs that differ only in the query \\nstring? For example: https://www.nytimes.com/page?q=0 and \\nhttps://www.nytimes.com/page?q=1 \\n \\n1 Based purely on the success/failure of the fetching process. Do not include errors caused by difficulty in parsing \\ncontent after it has already been successfully downloaded.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 3}, page_content=\"4 \\nA:  These can be treated as different URLs. \\n \\n \\nQ: URL case sensitivity: are these the same, or different URLs? \\n \\nhttps://www.nytimes.com/foo and https://www.nytimes.com/FOO \\nA:  The path component of a URL is considered to be case-sensitive, so the crawler behavior is \\ncorrect according to RFC3986. Therefore, these are different URLs. \\nThe page served may be the same because: \\nâ— that particular web server implementation treats path as case-insensitive (some server \\nimplementations do this, especially windows-based implementations) \\nâ— the web server implementation treats path as case-sensitive, but aliasing or redirect is being \\nused. \\nThis is one of the reasons why deduplication is necessary in practice. \\n \\n \\nQ: Attempting to compile the crawler results in syntax errors. \\nA:  Make sure that you have included crawler4j as well as all its dependencies. \\nAlso check your Java version; the code includes more recent Java constructs such as the typed \\ncollection List<String> which requires at least Java 1.5.0. \\n \\nQ: I get the following warnings when trying to run the crawler: \\n \\nlog4j: WARN No appenders could be found for logger \\nlog4j: WARN Please initialize the log4j system properly. \\n \\n \\n \\nA:  You failed to include the log4j.properties file that comes with crawler4j. \\n \\nQ:  On Windows, I am encountering the error: Exception_Access_Violation \\nA:  This is a Java issue. See: \\n \\n \\n \\nQ: I am encountering multiple instances of this info message: \\n \\nINFO [Crawler 1] I/O exception (org.apache.http.NoHttpResponseException) \\ncaught when processing request: The target server failed to respond \\nINFO [Crawler 1] Retrying request \\n \\n \\n \\nA:  If you're working off an unsteady wireless link, you may be battling network issues such as packet \\nlosses â€“ try to use a better connection. If not, the web server may be struggling to keep up with the \\nfrequency of your requests. \\nAs indicated by the info message, the crawler will retry the fetch, so a few isolated occurrences of \\nthis message are not an issue. However, if the problem repeats persistently, the situation is not \\nlikely to improve if you continue hammering the server at the same frequency. Try giving the \\nserver more room to breathe:\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 4}, page_content='5 \\n/* \\n * Be polite: Make sure that we don\\'t send more than \\n * 1 request per second (1000 milliseconds between requests). \\n */ \\nconfig.setPolitenessDelay(2500);         \\n/* \\n * READ ROBOTS.TXT of the website - Crawl-Delay: 10 \\n * Multiply that value by 1000 for millisecond value \\n */ \\n \\n \\n \\n \\n \\nQ: The crawler seems to choke on some of the downloaded files, for example: \\n \\njava.lang.StringIndexOutOfBoundsException: String index out of range: -2 \\n \\n \\n \\n \\njava.lang.NullPointerException: charsetName \\n \\n \\n \\n \\nA:  Safely ignore those. We are using a fairly simple, rudimentary crawler and it is not necessarily \\nrobust enough to handle all the possible quirks of heavy-duty crawling and parsing. These \\nproblems are few in number (compared to the entire crawl size), and for this exercise we\\'re \\nokay with it as long as it skips the few problem cases and keeps crawling everything else, and \\nterminates properly â€“ as opposed to exiting with fatal errors. \\n \\nQ: While running the crawler, you may get the following error:  \\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".  \\nSLF4J: Defaulting to no-operation (NOP) logger implementation  \\nSLF4J: See \\n for further details.   \\n \\nA. Download slf4j-simple-1.7.25.jar from \\n  add this as an external JAR to the project in the same way as the crawler-4j JAR \\nwill make the crawler display logs now. \\n \\nQ: What should we do with URL if it contains comma ? \\nA: Replace the comma with \"-\" or \"_\", so that it doesn\\'t throw an error. \\n \\nQ: Should the number of 200 codes in the fetch.csv file have to exactly match with the number of \\nrecords in the visit.csv? \\nA:  No, but it should be close, like within 2,000 of 20,000. If not then you may be filtering too \\nmuch. \\n \\nQ: \"CrawlConfig cannot be resolved to a type\" ? \\nA: import edu.uci.ics.crawler4j.crawler.CrawlConfig; make sure the external jars are added to \\nClassPath. ModulePath only contains the JRE. If it doesnâ€™t work, check if standard JRE imports \\nare working. Or using an alternative way: Using maven. Initialize a new project using maven and'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content='6 \\nthen just add a crawler4j dependency in the pom.xml file (auto-generated by maven). The \\ndependency is given in Crawler4j github page. \\n \\nQ: What\\'s the difference between aborted fetches and failed fetches? \\nA: failed:  Can be due to HTTP errors and other network related errors \\n    aborted:  Client decided to stop the fetching. (ex: Taking too much time to fetch)  \\nYou may sum up both the values and provide the combined result in the write up. \\n \\nQ: For some reason my crawler attempts 19,999 fetches, even though max pages is set to 20,000, \\ndoes this matter? \\nA: No, it doesnâ€™t matter. It can occur because 20,000 is the limit that you will try to fetch (it may \\ncontain successful status code like 200 and other like 301). But the visit.csv will contain only the \\nURL\\'s for which you are able to successfully download the files. \\n \\nQ: How to differentiate fetched pages and downloaded pages? \\nA: In this assignment we do not ask you to save any of the downloaded files to the disk. Visiting \\na page means crawler4j processing a page (it will parse the page and extract relevant information \\nlike outgoing URLs ). That means all visited pages are downloaded. \\nYou must make sure that your crawler crawls both http and https pages of the given domain \\n \\nQ: How much time should it approximately take to crawl a website using n crawlers? \\nA: (i) Depends on your parameters set for the crawler \\n     (ii) Depends on the politeness you set in the crawler program \\nYour crawl time in hours = maxPagesToFetch / 3600 * politeness delay in seconds \\nExample: a 20,000 page fetch with a politeness delay of 2 seconds will take 11.11 hours. That is \\nassuming you are running enough threads to ensure a page fetch every 2 seconds. Therefore, it \\ncan vary for everyone. \\n \\nQ: For the third CSV file, urls_NewSite.csv, should the discovered URLs include redirect \\nURLs? \\nA: YES, if the redirect URL is the one that gets status code 300, then the URL that redirects the \\nURL to point to will be added to the scheduler of the crawler and waits to be visited. \\n \\nQ: When the URL ends with \"/\", what needs to be done? \\nA: You should filter using content type. Please have a peek into Crawler 4j code located at \\n \\n You will get a hint on how to know the content type of the page, even if the extension is not \\nexplicitly mentioned in the URL \\n \\nQ: Eclipse keeps crashing after a few minutes of running my code. But when I reduce the no of pages to \\nfetch, it works fine.  \\nA: Increase heap size for eclipse using this. \\n \\n  \\n \\nQ: What if a URL has an unknown extension? \\nA: Please check the content type of the page if it has an unknown extension \\n \\nQ: Why do some links return True in shouldVisit() but cannot be visited by Visit()? \\nA: shouldVisit() function is used to calculate whether the page should be visited or not. It may or \\nmay not be a visitable page.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 6}, page_content='7 \\nFor example - If you are crawling the site http://viterbi.usc.edu/, the page \\nhttp://viterbi.usc.edu/mySamplePage.html should be visited. but this page may return a 404 Not \\nFound Error or it may be redirected to some other site like http://mysamplesite.com. In this case, \\nshouldVisit() function would return true because the page should be visited but visit() will not be \\ncalled because the page cannot be visited. \\nComment:  \\n \\n \\n \\nhas details on regular expressions that you need to take care.  \\n \\nComment: Since many newspaper websites dump images and other types of media on CDN,  \\nyour crawl may only encounter html files. That is fine. \\n \\nComment: File types css,js,json and others should not be visited. E.g. you can add .json to your \\npattern filter. If the extension does not appear, use \\n!page.getContentType().contains(â€œapplication.jsonâ€) \\n \\nComment: Some sites may have less than the 20,000 pages, but as long as the formula matches. \\ni,e \\n# fetches attempted = # fetches succeeded + # fetches aborted + # fetches failed \\nyour homework is ok. However, the variation should not be more than 10% away from the limit \\nas it is an indication that something is wrong. \\nScenario: \\nMy visit.csv file has about 15 URLs lesser than the number of URLs with status code \\n200. It is fine if the difference is less than 10%. \\n \\nComment: the homework description states that you only need to consider HTML, doc, pdf  \\nand different image format URLs . But you should also consider URL\\'s with no extension \\nas they may return a file of one of the above types. \\n \\nComment: The distinction between failed and aborted web pages. \\nfailed: Can be due to content not found, HTTP errors or other network related errors \\n     \\naborted:  the client (the crawler) decided to stop the fetching. (ex: Taking too much time to \\nfetch).  \\nYou may sum up both the values and provide the combined result in the write up. \\n \\nQ: In the visit_NewsSite.csv, do we also need to chop \"charset=utf-8\" from content-type?  Or \\njust chop \"charset=urf-8\" in the report? \\nA: You can chop Encoding part(charset=urf-8) in all places. \\n \\n \\nQ: REGARDING STATISTICS \\nA: #unique URLs extracted = #unique URLs within + #unique URLs outside  \\n     #total urls extracted is the sum of #outgoing links. \\n     #total urls extracted is the sum of all values in column 3 of visit.csv \\nFor text/html files, find the number of out links. For non-text/html files, the number should be 0. \\n \\n \\nQ: How to handle pages with NO Extension'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='8 \\nA: Use getContentType() in visit() and donâ€™t rely just on extension. If the content type \\nreturned is not one of the required content types for the assignment, you should ignore it for any \\ncalculation of the statistics. This will probably result in more rows in visit.csv, but itâ€™s acceptable \\naccording to the grading guidelines. \\n \\nQ: Clarification on \"the URLs it attempts to fetch\" \\nA: \"The URLs it attempts to fetch\" means all the URLs crawled from start seed which reside in \\nthe news website and has the required media types. \\n \\n \\n \\n \\nNote #1: Extracted urls do not have to be added to visit queue. Some of them which satisfy a \\nrequirement (e.g : content type, domain, not duplicate) will be added to visit queue. But others \\nwill be dumped by the crawler.  \\nHowever, as long as the grading guideline is satisfied, we will not deduct points. \\n \\nNote#2: : 303 could be considered aborted. 404 could be considered failed.  \\nTo summarize: we consider a request to be aborted if the crawler decides to terminate that \\nrequest. Client-side timeout is an example.  Requests can fail due to reasons like content not \\nfound, server errors, etc. \\n \\nNote#3: Fetch statistics: \\n# fetches attempted: The total number of URLs that the crawler attempted to fetch. This is \\nusually equal to the MAXPAGES setting if the crawler reached that limit; less if the website is \\nsmaller than that. \\n# fetches succeeded: The number of URLs that were successfully downloaded in their entirety, \\ni.e. returning a HTTP status code of 2XX. \\n# fetches failed or aborted: The number of fetches that failed for whatever reason, including, but \\nnot limited to: HTTP redirections (3XX), client errors (4XX), server errors (5XX) and other \\nnetwork-related errors. \\nNote#4:  Consider fetches failed and aborted as same similar to as mentioned in Note#3 \\n \\nNote#5: Hint on crawling pages other than html \\nLook for how to turn ON the Binary Content in Crawling in crawler4j. Make sure you are not \\njust crawling the html parsed data and not the binary data which includes file types other than \\nhtml. \\nSearch on the internet on how to crawl binary data and I am sure you will get something on how \\nto parse pages other than html types. \\nThere will be pages other than html in almost every news site so please make sure you crawl \\nthem properly. \\n \\nQ: Regarding the content type in visit_NewsSite.csv, should we display \"text/html;charset=UTF-\\n8\" or chop out the encoding and write \"text/html\" in the Excel sheet ?  \\nA: ONLY TEXT/HTML, ignore rest. \\n \\nQ: Should we limit the URLs that the crawler attempted to fetch within the news domain? e.g. if \\nwe encounter \\n we should skip fetching by adding constraints in \\n\"shouldVisit()\"? But do we need to include it in urls_NewsSite.csv? \\nA: Yes, you need to include every encountered url in urls_NewsSite.csv.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 8}, page_content=\"9 \\nQ: All 3xx,4xx, 5xx should be considered as aborted? \\nA: YES \\n \\nQ: Are â€œcookieâ€ domains considered as an original newsite domain ? \\nA: NO, they should not be included as part of the newsite you are crawling. For details see  \\nhttps://web.archive.org/web/20200418163316/https://www.mxsasha.eu/blog/2014/03/04/definiti\\nve-guide-to-cookie-domains/ \\n \\nQ. More about statistics \\nA: visit.csv will contain the urls which are succeeded i.e. 200 status code with known/ allowed \\ncontent types. Fetch.csv will include all the urls which are been attempted to fetch i.e. with all \\nthe status codes. \\nfetch.csv entries will be = visit.csv entries (with 2xx status codes) + entries with status codes \\nother than 2XX \\nvisit.csv =  entries with 2XX status codes. \\nAlso, you should not and it is not necessary to use customized status code. Just use the status \\ncode what the webpage returns to you. \\n \\n(Note:-> fetch.csv should have urls from news site domain only) \\n \\nQ: do we need to check content-type for all the extracted URLs, i.e. url.csv or just for visited \\nURLs, e.g. those in visit.csv? \\nA: only those in visit_NewsSite.csv \\n \\nQ: How to get the size of the downloaded file? \\nA: It will be the size of the page. Ex - for an image or pdf, it will be the size of the image or the \\npdf, for the html files, it will be the size of the file. The size should be in bytes (or kb, mb \\netc.).  (page.getContentData().length) \\n \\nQ: Change logging level in crawler4j? \\nA:  If you are using the latest version of Crawler4j, logging can be controlled through \\nlogback.xml. You can view the github issue thread for knowing more about the logback \\nconfigurations \\n- \\n \\n. \\n \\nQ: Crawling urls only yield text/html. I have only filtered out css|js|mp3|zip|gz, But all the \\nvisited urls have return type text/html is it fine? Or is there a problem? \\n \\nA: It is fine. Some websites host their asset files (images/pdfs) on another CDN, and the URL for \\nthe same would be different from www.newssite.com, so you might only get html files for that \\nnews site. \\n \\nQ: Eclipse Error: Provider class org.apache.tika.parser.external.CompositeExternalParser not in \\nmodule \\n \\nI'm trying to follow the guide and run the boiler plate code, but eclipse gives this error when I'm \\ntrying to run the copy pasted code from the installation guide \\n \\nA: Please import crawler4j jars in ClassPath and not ModulePath, while configuring the build in \\nEclipse. \\n \\nQ: Illegal State Exception Error\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 9}, page_content='10 \\nA: 1) if you are using a newest java version, I would downgrade to 8 to use, there are some sort \\nof a similar issue with the newest java version. \\n2) carefully follow the instructions on Crawler4jinstallation.pdf \\n3) make sure to add the jar file to the CLASS PATH  \\n4) if any module is missing, download from google and add to the prj class path.  \\n \\n \\nQ: /data/crawl error \\nException in thread \"main\" java.lang.Exception: couldn\\'t create the storage folder: /data/crawl \\ndoes it already exist ? \\n at edu.uci.ics.crawler4j.crawler.CrawlController.<init>(CrawlController.java:84) \\n at Controller.main(Controller.java:20) \\n \\nA: Replace the path /data/crawl in the Controller class code with a location on your machine  \\n \\nQ: Do we need to remove duplicate urls in fetch.csv (if exists)? \\n \\nA: Crawler4j already handles duplication checks so you don\\'t have to handle it. It doesn\\'t crawl \\npages that have already been visited. \\n \\nQ: Error in Controller.java- \"Unhandled exception type Exception\" \\nA: Make sure Exception Handling is taken care of in the code. Since CrawlController class \\nthrows exception, so it needs to be handled inside a try-catch block. \\n \\nQ: Crawler cannot stop - when I set maxFetchPage to 20000, my script cannot stop and keeps \\nrunning forever. I have to kill it by myself. However, it looks like that my crawler has crawled \\nall the 20000 pages but just cannot end. \\n \\nA:Set a reasonable maxDepthofCrawling, Politeness Delay, setSocketTimeout(), and Number of \\ncrawlers in the Controller class, and retry. Also ensure there are no System.out.print() statements \\nrunning inside the Crawler code. \\n \\nQ: If you are in countries that have connection problems. \\nA: We would suggest you to visit https://itservices.usc.edu/vpn/ for more information. \\nEnable the VPN, clear the cache, restart the computer should help solve the problem. \\n \\n \\n \\n \\n6. Submission Instructions \\n \\n \\nâ— Save your statistics report as a plain text file and name it based on the news website \\ndomain names assigned below: \\n \\nUSC ID ends with Site  \\n01~20 \\nCrawlReport_nytimes.txt \\n21~40 \\nCrawlReport_wsj.txt \\n41~60 \\nCrawlReport_foxnews.txt \\n61~80 \\nCrawlReport_usatoday.txt'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 10}, page_content='11 \\n81~00 \\nCrawlReport_latimes.txt \\n \\n \\n \\n \\nâ— Also include the output files generated from your crawler run, using the extensions as \\nshown above: \\no fetch_NewsSite.csv \\no visit_NewsSite.csv \\nâ— \\nDo NOT include the output files  \\no urls_NewsSite.csv \\nwhere _NewSite should be replaced by the name from the table above. \\nâ— Do not submit Java code or compiled programs; it is not required. \\nâ— Compress all of the above into a single zip archive and name it: \\ncrawl.zip \\nUse only standard zip format. Do NOT use other formats such as zipx, rar, ace, etc. For \\nexample the zip file might contain the following three files: \\n \\n1. CrawlReport_nytimes.txt, (the statistics file)         \\n2. fetch_nytimes.csv          \\n3. visit_nytimes.csv \\n \\nâ— Please upload your homework to your Google Drive CSCI572 folder, in the subfolder named \\nhw2 \\n \\n \\n \\nAppendix A \\n \\nUse the following format to tabulate the statistics that you collated based on the crawler outputs. \\n \\nNote: The status codes and content types shown are only a sample. The status codes and content types \\nthat you encounter may vary, and should all be listed and reflected in your report. Do NOT lump \\neverything else that is not in this sample under an â€œOtherâ€ heading. You may, however, exclude status \\ncodes and types for which you have a count of zero. Also, note the use of multiple threads. You are \\nrequired to use multiple threads in this exercise. \\n \\nCrawlReport_NewsSite.txt'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 11}, page_content='12 \\nName: Tommy Trojan \\nUSC ID: 1234567890 \\nNews site crawled: nytimes.com \\nNumber of threads: 7 \\n \\nFetch Statistics \\n================ \\n# fetches attempted: \\n# fetches succeeded: \\n# fetches failed or aborted: \\n \\nOutgoing URLs: \\n============== \\nTotal URLs extracted: \\n# unique URLs extracted: \\n# unique URLs within News Site: \\n# unique URLs outside News Site: \\n \\nStatus Codes: \\n============= \\n200 OK: \\n301 Moved Permanently: \\n401 Unauthorized: \\n403 Forbidden: \\n404 Not Found: \\n \\nFile Sizes: \\n=========== \\n< 1KB: \\n1KB ~ <10KB: \\n10KB ~ <100KB: \\n100KB ~ <1MB: \\n>= 1MB: \\n \\nContent Types: \\n============== \\ntext/html: \\nimage/gif: \\nimage/jpeg: \\nimage/png: \\napplication/pdf:'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 0}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nName:\\nUSC ID:\\nRead the following rules carefully:\\nâ€¢ Write your name and ID number in the solution you submit.\\nâ€¢ Please sign and submit the code of honor in the exam with your solutions. The exam\\ncannot be graded without a signed code of honor. You are supposed to do all of the\\nproblems on your own without receiving help from others.\\nâ€¢ Cheating in the exam will not be tolerated. If you are caught cheating, you will be\\nreported to the authorities. The recommendation of the instructor will be at least an\\nF in the course in such cases.\\nâ€¢ Do not post any questions on Piazza about the exam. The TAs have been instructed to\\nrefrain from answering questions about the midterm. In case of ambiguity or problems\\nin the questions, just do your best.\\nâ€¢ The use of generative AI is prohibited in answering the questions.\\nâ€¢ Problems are not sorted in terms of di!culty. Please avoid guess work and long and\\nirrelevant answers.\\nâ€¢ Instructions on submitting the solutions to paper and pencil and coding problems will\\nbe provided shortly by the TAs. You can handwrite or typeset your solutions to paper\\nand pencil problems.\\nâ€¢ Show all your work and your ï¬nal answer. Showing only the ï¬nal answer of a question\\nmay not receive full credit and you must show your solution and reasoning behind the\\nanswer. Simplify your answer as much as you can.\\nâ€¢ The exam has 8 questions, 16 pages, and a total of 100 points.\\nâ€¢ The submission deadline for this midterm is 11:59 PM, Friday, October 24, 2025.\\nâ€¢ As this is a take home exam that extends over several days, OSAS students DO NOT\\nreceive extra time, per OSAS guidelines.\\nâ€¢ Any change in the midterm (paper and pencil or coding) after the deadline is considered\\nlate submission. One second late is late. The midterm is graded based on when it was\\nsubmitted, not when it was ï¬nished. The midterm can be submitted up to three days\\nlate, with 10% penalty per late day.\\nHomework late days cannot be used for the\\nmidterm.\\nâ€¢ Submission after the grace period will receive a zero. One second late is late.\\n1'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 1}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nGrading Breakdown\\nProblem\\nScore\\nEarned\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n5\\n10\\n6\\n10\\n7\\n20\\n8\\n20\\nTotal\\n100\\n2'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 2}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nHonor Code\\nI pledge on my honor that I have not given or received any unauthorized assistance on\\nthis examination.\\nName:\\nSignature:\\n3'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 3}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n1. You are working with a dataset where you train ï¬ve polynomial regression models\\n(Model A to E) of increasing complexity (degrees 1, 3, 5, 7, and 9 respectively) on\\nthe same training data of size n = 80. The performance of these models has been\\nevaluated using 5-fold cross-validation. The table below shows the average training\\nand validation Mean Squared Errors (MSE) and Mean Absolute Errors (MAE) for\\neach model:\\nModel\\nDegree\\nTraining MSE\\nValidation MSE\\nA\\n1\\n21.4\\n24.7\\nB\\n3\\n12.8\\n14.6\\nC\\n5\\n6.9\\n8.2\\nD\\n7\\n3.0\\n9.9\\nE\\n9\\n1.2\\n23.3\\nInstructions:\\n(a) Deï¬ne and explain the mathematical concepts of bias and variance. How do they\\nrelate to model complexity in supervised learning?\\n(b) Using the above table, calculate the bias2, variance, and Expected Prediction\\nError (EPE) for each model. Assume:\\nâ€¢ Training MSE â†’Variance\\nâ€¢ Validation MSE â†’Total EPE\\nâ€¢ Irreducible Error = 1.0\\n(c) For x = 2, 4, 6, 8, use the following table of true function values f(x) and model\\npredictions Ë†f(x) from Model C. Compute the absolute prediction error and squared\\nerror for each x. Then compute the average squared error (MSE):\\nx\\n2\\n4\\n6\\n8\\nf(x) (True)\\n5.0\\n9.5\\n13.0\\n16.5\\nË†f(x) (Predicted)\\n4.8\\n10.0\\n12.2\\n17.0\\n|f(x) â†‘Ë†f(x)|\\n(f(x) â†‘Ë†f(x))2\\n(d) Interpret the trends observed in the table. Which model provides the best tradeo!\\nbetween bias and variance? Justify your choice using comparisons across at least\\nthree models, and classify which models su!er from underï¬tting or overï¬tting.\\n4'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 4}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 5}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 6}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 7}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 8}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n2. Consider a logistic regression problem in which there are no features, which means\\nthat:\\nPr(Y = 1) =\\neÏ‰0\\n1 + eÏ‰0\\nAssume that we have m data points with label Y = 1 and n data points with label\\nY = 0 (remember that features are irrelevant).\\n(a) Write down the likelihood function l(Ï‰0).\\n(b) Find the Maximum Likelihood estimate Ë†Ï‰0 for this data set. [Hint: maximize\\nloge l(Ï‰0)].\\n(c) Determine conditions under which this simple classiï¬er classiï¬es data points into\\nY = 1 or Y = 0.\\n5'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 9}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 10}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 11}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 12}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n3. For the following data set for classiï¬cation:\\nIndex\\nX\\nY\\n1\\n-1\\n1\\n2\\n0\\n0\\n3\\n3\\n0\\n4\\n1\\n1\\n5\\n-2\\n0\\nAssume that we want to construct a regularized logistic regression model for this\\ndataset.\\n(a) Write down the L1-regularized loss function J(Ï‰0, Ï‰1) for this dataset with regu-\\nlarization parameter Îµ = 2.\\n(b) Compare the bias variance of the regularized model with the unregularized model\\n(Îµ = 0).\\n6'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 13}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 14}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 15}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 16}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n4. Consider the following data set for classiï¬cation:\\nIndex\\nX\\nY\\n1\\n1\\n1\\n2\\n-1\\n0\\n3\\n-2\\n0\\n(a) Show all possible bootstrap samples of the dataset that have the same size as this\\ndataset. Note that permutations of the same data set are considered the same\\ndataset, for example {1, 2, 3} and {2, 3, 1} are the same dataset.\\n(b) Construct a KNN classiï¬cation model for all bootstrap samples in part 4a with\\nK = 2 and predict the label for the test point xâ†’= 0 by majority votes of the\\npredictions of those bootstrap models. Break ties in favor of class 1.\\n7'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 17}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 18}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 19}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 20}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 21}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 22}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n5. You are given the following dataset containing short text sequences and their associated\\nlabels:\\nTask: Explain and demonstrate how this text data can be processed and classiï¬ed\\ninto the correct sentiment class (Positive or Negative). Your answer should cover the\\nfollowing parts:\\n(a) Preprocessing\\ni. Tokenize and lowercase the sentence:\\nâ€œI Loved the movie, it was fantastic!â€\\nShow the processed output.\\n(b) Feature Representation\\ni. Explain the di!erence between:\\nâ€¢ Bag-of-Words (BoW) representation\\nâ€¢ TFâ€“IDF (Term Frequencyâ€“Inverse Document Frequency) representation\\nii. For the word â€œmovieâ€, calculate its TFâ€“IDF value.\\nUse the tokenized\\nversion of the ï¬rst sentence (â€œI loved the movie, it was fantastic!â€) to\\ncompute TF, and use the entire dataset above to compute IDF.\\nShow the formula and calculation steps.\\n(c) Model Building Use the NaÂ¨Ä±ve Bayes classiï¬er and binary TF (TF=1 if the\\nterm exists in the document and TF=0 if it does not) to classify the sentence\\nâ€œWhat a great movie.â€ Use histograms for your density estimates.\\n8'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 23}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 24}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 25}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 26}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 27}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 28}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n6. A researcher studies the relationship between weekly exercise hours (X) and stress level\\nscore (Y ) in graduate students. The sample size is n = 26 and the sample Pearson\\ncorrelation is r = â†’0.39.\\n(a) Test at signiï¬cance level Ï‰ = 0.05 the null hypothesis H0 : Îµ1 = 0 versus the\\ntwo-sided alternative H1 : Îµ1 â†‘= 0. Show the test statistic, decision rule, and\\nconclusion in context.\\n(b) Report and interpret the coe!cient of determination R2.\\n(c) Brieï¬‚y explain what the negative sign of r indicates in this scenario.\\nNote: Everything needed to solve this question is contained in the exam; do not\\nconsult external tables.\\n9'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 29}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 30}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 31}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n7. You are given a binary dataset with two real-valued features (x, y) and a label in {0, 1}.\\nClass 0 points are denoted by â†’and class 1 points by â†‘.\\nImportant rules for all parts.\\nâ€¢ A point may not be its own neighbor.\\nâ€¢ Break ties in neighbor votes in favor of class 0.\\nâ€¢ Show work to justify your neighbor choices (distances and votes) whenever asked.\\nDataset\\nidx\\nx\\ny\\nlabel\\n1\\n1.0\\n6.0\\n0\\n2\\n2.2\\n7.0\\n0\\n3\\n3.1\\n8.2\\n0\\n4\\n4.0\\n6.1\\n0\\n5\\n5.2\\n6.0\\n1\\n6\\n6.0\\n6.2\\n0\\n7\\n6.2\\n4.8\\n1\\n8\\n7.0\\n3.8\\n1\\n9\\n8.2\\n4.6\\n1\\n10\\n8.8\\n6.0\\n0\\n11\\n3.2\\n5.4\\n1\\n12\\n2.8\\n6.2\\n0\\n13\\n7.6\\n7.6\\n1\\n14\\n4.8\\n7.4\\n1\\nDistance metrics. For p = (x1, y1) and q = (x2, y2):\\nd1(p, q) = |x1 â†“x2| + |y1 â†“y2|\\n(Manhattan),\\nd2(p, q) =\\n!\\n(x1 â†“x2)2 + (y1 â†“y2)2\\n(Euclidean),\\ndâ†’(p, q) = max{|x1 â†“x2|, |y1 â†“y2|}\\n(Chebyshev).\\nTasks.\\n(a)\\nLOOCV of 1-NN across metrics.\\nConsider 1-nearest-neighbor classiï¬cation on (x, y). The distance function will be\\nselected from the options provided. For each option, calculate the LOOCV error\\nof 1-NN and indicate which distance gives superior performance. The options are:\\nManhattan d1, Euclidean d2, and Chebyshev dâ†’.\\n(b) E!ect of k.\\nFor each metric d1, d2, and dâ†’, compute the LOOCV error for k â†”{1, 3, 5}.\\nReport the error for each (metric, k) and list the misclassiï¬ed indices. For the best\\nLOOCV setting, show the full 5-NN calculation (neighbor IDs, labels, distances,\\nvote) for every misclassiï¬ed point.\\n10'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 32}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nFigure 1: Dataset with indices and class markers. Use this for geometric intuition; grading\\nis based on your calculations.\\n(c) Fixed 5-fold CV (Euclidean).\\nUsing Euclidean d2 and k â†’{1, 3, 5}, evaluate the ï¬xed folds (do not reshu!e):\\nFold 1 = {1, 7, 11},\\nFold 2 = {2, 8, 12},\\nFold 3 = {3, 9, 5},\\nFold 4 = {4, 6, 10},\\nFold 5 = {13, 14}.\\nFor each k, report the error on each fold, the mean error across folds, and the\\nmisclassiï¬ed index set per fold. Select the k recommended by this split and justify\\nbrieï¬‚y.\\n(d) 1-NN decision boundary (Euclidean).\\nSketch the qualitative 1-NN (k=1) decision regions for d2. Indicate at least two\\nplaces where the boundary is clearly non-linear due to local class interleaving It\\nis ï¬ne if you use software to plot it and you do not need to submit the code.\\n11'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 33}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 34}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 35}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 36}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 37}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 38}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 39}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 40}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 41}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 42}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 43}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 44}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 45}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 46}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 47}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n8. Programming Question: Predicting Housing Prices with Linear Regression\\nDataset: student-mat.csv\\nLink: https://archive.ics.uci.edu/dataset/320/student+performance\\nIn this problem, you will build a linear regression model to predict housing prices in\\nthe USA based on various features. You will use Python and scikit-learn for this\\ntask.\\n(a) Data Exploration and Pre-processing\\ni. Import the student-mat.csv dataset as a pandas DataFrame.\\nii. Select the following features: age, studytime, schoolsup, goout, Dalc,\\nWalc, health, absences, G3 (target/dependent variable). Encode the bi-\\nnary variable (schoolsup) values as 0s (no) and 1s (yes). Combine Dalc and\\nWalc into one variable alc by taking averge, then remove Dalc and Walc.\\nDisplay the ï¬rst ï¬ve rows of the pre-processed dataset.\\niii. Find the number of outliers for each independent variable using the IQR\\nmethod.\\niv. Standardize and run PCA on the dataset. Create a scatterplot of PC1 vs PC2,\\ncoloring the dots by their ï¬nal grade G3. Inspect the component loadings and\\ndetermine which features contribute the most to PC1 and PC2. Keep and\\nuse standardized data for remaining problems.\\n(b) Linear Regression\\ni. Split data into training set and testing set with an 80:20 ratio. Use random\\nseed 552 for reproducibility.\\nii. Build three models using the training set: A. Linear Regression Model, B.\\nLinear Regression Model with Ridge Regularization, and C. Linear Regression\\nModel with Lasso Regularization. Set Ï‰=0.1.\\niii. Test all three models on the test set. Find out the best performing model\\nwith repsect to each of the metrics: Mean Absolute Error (MAE), Root Mean\\nSquared Error (RMSE), and RÂ².\\niv. How do you interpret RÂ² values from the three models?\\nv. Print coe!cients of independent variables from the three models in one table.\\nvi. Whatâ€™s the relationship between each independent variable and the dependent\\nvariable?\\nvii. How do the regularization methods diâ€er? What can you conclude about the\\ndataset and features given the results?\\nviii. If the regularization strength (Ï‰) is increased, what would happen to perfor-\\nmance metrics?\\nix. What are some feature engineering methods you would suggest to improve\\nmodel performance?\\nExpected Output: Your submission should include:\\n12'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 48}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 49}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nâ€¢ Jupyter Notebook .ipynb with all the steps clearly commented.\\nâ€¢ The output of each step as speciï¬ed above (e.g., head of DataFrame, info, describe,\\nmissing value counts, evaluation metrics for all models, coe!cients, and intercept).\\nâ€¢ Visualizations for outlier detection and residual analysis.\\nâ€¢ A brief discussion answering the interpretation questions.\\n13'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 50}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nScratch paper\\nName:\\nUSC ID:\\n14'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 51}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nScratch paper\\nName:\\nUSC ID:\\n15'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 52}, page_content='')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader = DirectoryLoader(\"../data/pdf\",\n",
    "                             glob = '**/*.pdf',   #Pattern to match files,\n",
    "                             loader_cls = PyMuPDFLoader,     ##loader class to use\n",
    "                             show_progress = False)\n",
    "\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "926d29c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540d270",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50132110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for better RAG performance.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ed7ec9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 79 documents into 84 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: CS570 Spring 2025: Analysis of Algorithms         Exam IIâ€‹\n",
      " \n",
      " \n",
      "Points \n",
      " \n",
      "Points \n",
      "Problem 1 \n",
      "16 \n",
      "Problem 4 \n",
      "17 \n",
      "Problem 2 \n",
      "9 \n",
      "Problem 5 \n",
      "18 \n",
      "Problem 3 \n",
      "20 \n",
      "Problem 6 \n",
      "18 \n",
      "Total 98 \n",
      "â€‹\n",
      " \n",
      " \n",
      "First name \n",
      " \n",
      "...\n",
      "Metadata: {'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='CS570 Spring 2025: Analysis of Algorithms         Exam II\\u200b\\n \\n \\nPoints \\n \\nPoints \\nProblem 1 \\n16 \\nProblem 4 \\n17 \\nProblem 2 \\n9 \\nProblem 5 \\n18 \\nProblem 3 \\n20 \\nProblem 6 \\n18 \\nTotal 98 \\n\\u200b\\n \\n \\nFirst name \\n \\nLast Name \\n \\nStudent ID \\n \\n \\n \\n \\nInstructions: \\n1.\\u200b\\nThis is a 2-hr exam. Closed book and notes. No electronic devices or internet access. \\n2.\\u200b\\nA single double sided 8.5in x 11in cheat sheet is allowed. \\n3.\\u200b\\nIf a description to an algorithm or a proof is required, please limit your description or \\nproof to within 150 words, preferably not exceeding the space allotted for that question. \\n4.\\u200b\\nNo space other than the pages in the exam booklet will be scanned for grading. \\n5.\\u200b\\nIf you require an additional page for a question, you can use the extra page provided \\nwithin this booklet. However please indicate clearly that you are continuing the solution \\non the additional page. \\n6.\\u200b\\nDo not detach any sheets from the booklet. \\n7.\\u200b'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='within this booklet. However please indicate clearly that you are continuing the solution \\non the additional page. \\n6.\\u200b\\nDo not detach any sheets from the booklet. \\n7.\\u200b\\nIf using a pencil to write the answers, make sure you apply enough pressure, so your \\nanswers are readable in the scanned copy of your exam. \\n8.\\u200b\\nDo not write your answers in cursive scripts. \\n9.\\u200b\\nThis exam is printed double sided. Check and use the back of each page.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Student ID: \\n1)\\u200b\\n16 pts (2 pts each)\\u200b\\nMark the following statements as TRUE or FALSE by circling the correct answer. No need to \\nprovide any justification. \\n \\n[ TRUE/FALSE ] \\nBellman-Ford is asymptotically faster than Dijkstraâ€™s algorithm when dealing with graphs \\nthat only have non-negative edge weights. \\n[ TRUE/FALSE ]\\u200b\\nIn a maximum flow problem in a flow network with non-zero integer capacities, there \\nmust exist at least one integer-valued maximum flow f with non-zero v(f). \\n[ TRUE/FALSE ] \\nGiven a feasible circulation in a network with unlimited capacities, if the flow on each \\nedge is increased by one unit, we will get another feasible circulation.  \\n \\n[ TRUE/FALSE ] \\nSuppose an edge e is not saturated due to max flow f in a Flow Network. Then increasing \\neâ€™s capacity will not increase the max flow value of the network. \\n\\u200b\\n[ TRUE/FALSE ] \\nThe worst-case time complexity of any dynamic programming algorithm with n3 unique \\nsubproblems is â„¦(n3). \\n \\n[ TRUE/FALSE ]'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='\\u200b\\n[ TRUE/FALSE ] \\nThe worst-case time complexity of any dynamic programming algorithm with n3 unique \\nsubproblems is â„¦(n3). \\n \\n[ TRUE/FALSE ] \\nConsider the following recurrence formula  \\u200b\\n \\n \\n\\u200b\\nwhere A and B are fixed input arrays and subproblems OPT(j) are defined for j = 1, â€¦, n. \\nThen, this will lead to an O(n) dynamic programming algorithm. \\n \\n[ TRUE/FALSE ] \\nBy definition, all dynamic programming algorithms must run in polynomial time with \\nrespect to the input size (either in terms of the number of integers, or in terms of the \\nnumber of bits in the input). \\n \\n[ TRUE/FALSE ] \\nGiven a flow network G and its max flow f, we can always find an s-t  cut (A,B) in G \\nwhere all edges e crossing the cut either have f(e) = C(e) or f(e) = 0.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Student ID: \\n2)\\u200b\\n9 pts  -   3 pts each. No partial credit \\n \\nI. What is the capacity of the minimum s-t cut in the following flow network? \\n \\n \\nA) 5 \\nB) 6 \\nC) 7 \\nD) 8 \\nSolution: C \\n \\nII.  \\xa0Which of the following statements is/are False regarding dynamic programming? \\nSelect all correct answers! \\nA) It solves problems by breaking them into overlapping subproblems. \\nB) It can be implemented using a bottom-up approach known as tabulation. \\nC) A problem of size n must have subproblems of size n/b where b>1 is an integer\\u200b\\n     constant  \\nD) It can utilize memoization to store values of optimal solutions for unique subproblems \\nto avoid redundant calculations. \\nSolution: C \\n \\nIII. Which of the following statements is/are True about the Ford-Fulkerson algorithm? \\nSelect all correct answers! \\nA)\\u200b For integer edge capacities, it terminates in at most v(f) iterations, where v(f) is the \\nmaximum flow value.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Select all correct answers! \\nA)\\u200b For integer edge capacities, it terminates in at most v(f) iterations, where v(f) is the \\nmaximum flow value. \\nB)\\u200b For integer edge capacities, it terminates in at most C iterations, where C is the total \\ncapacity of edges out of source. \\nC)\\u200b It may converge to an incorrect value of max flow for non-integer edge capacities. \\nD)\\u200b Assuming it terminates, it will always find the same maximum flow f independent of \\nthe choice of augmenting paths. \\nSolution: A,B,C'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Student ID: \\n3)\\u200b 20 pts  \\n \\nConsider the flow network below with demands and lower bounds. \\u200b\\nNotation: (le,Ce) show lower bound and capacity on each edge. Numbers assigned to \\nnodes show the demand value at that node. \\n \\nFollow the steps as described in class to determine if a feasible circulation exists \\na)\\u200b Convert the given network to an equivalent one with no lower bounds. Show all your \\nwork. (6 pts) \\n \\n \\nNo lower bounds'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Student ID: \\n \\nRUBRIC: \\n(3 points) Draw the imbalance graph correctly \\n(3 points) Draw the final graph with no lower bounds correctly \\n-1 point for every wrong number \\n \\n \\n \\n \\n \\nb) Convert the network obtained in a) to an equivalent one with no demands (i.e., a flow \\nnetwork) (2 pts) \\n \\nFlow Network (i.e., with no demands) \\n \\nRUBRIC:\\u200b\\n(1 point) Create a super source and super sink \\n(1 point) Connect nodes correctly to either source/sink \\n0 points if no super source or super sink was created \\n \\n \\n \\n \\n \\nc) In the network obtained in b), compute max flow using Scaled version of Ford \\nFulkerson algorithm. Only show the different scaling iterations marked with the \\ncorresponding Delta value, and for each scaling iteration, show the augmenting path(s) \\nfound with the respective augmenting flow value. No need to show residual graphs. \\u200b\\n(10 pts)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Student ID: \\nMax flow with scaled version of Ford Fulkerson algorithm \\n \\n \\nRUBRIC: \\n(- 5 points):  Did not use scaled version of Ford Fulkerson algorithm \\n(- 2 points): Incorrect max flow value \\n(- 2 points): Missed delta 8, 4 and 1 in calculation \\n(- 1 point): For each minor calculation error'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Student ID: \\n \\nd) Based on your solution in part c, explain whether the originally given network has a \\nfeasible circulation or not. (2 pts) \\nSince max flow not equal to total demand, the circulation is not feasible \\n \\n(2 points) correctly mention that circulation is not feasible because max flow not equal to \\ntotal demand \\n0 points if there is no mention of max flow being not equal to demand AND circulation \\nnot being feasible'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Student ID: \\n4)\\u200b 17 pts \\nConsider the directed graph below.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\na)\\u200b Use the Bellman-Ford algorithm to find the negative cycle in this graph. In other \\nwords, you must solve this problem numerically with the given edge weights and  \\n     Explain how the destination node t is determined (3 pts) \\ncreate new node t* and connect all nodes to t* with distance of 0. \\n \\n1 - point for creating new node  \\n2 - for connecting all the nodes distance 0 (or any number as long as it is the same for \\nall nodes) \\n0 - no creation of a new node \\n \\n \\nb)\\u200b  State how many iterations of Bellman-Ford are required (2 pts) \\nn=7 iterations \\n2 - Point for stating the correct number of iterations  \\n0 - Incorrect answer'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Student ID: \\nc)\\u200b  Show each iteration of Bellman-Ford by showing subproblem values in a table \\nwith columns representing nodes and rows representing iterations (6 pts) \\n \\n \\na \\nâˆ \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\nb \\nâˆ \\n0 \\n0 \\n0 \\n-1 \\n-2 \\n-2 \\n-2 \\nc \\nâˆ \\n0 \\n0 \\n-5 \\n-6 \\n-6 \\n-6 \\n-7 \\nd \\nâˆ \\n0 \\n-7 \\n-8 \\n-8 \\n-8 \\n-9 \\n-10 \\ne \\nâˆ \\n0 \\n-1 \\n-1 \\n-1 \\n-2 \\n-3 \\n-3 \\nf \\nâˆ \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\nt* \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n \\nw/ up \\nto 0 \\nedges \\nw/ up \\nto 1 \\nedges \\nw/ up \\nto 2 \\nedges \\nw/ up \\nto 3 \\nedges \\nw/ up \\nto 4 \\nedges \\nw/ up \\nto 5 \\nedges \\nw/ up \\nto 6 \\nedges \\nw/ up \\nto 7 \\nedges \\n \\n1 - Point for each correct iteration. Whole iteration must be correct to get 1 full point. \\n \\n \\nd)\\u200b Show how it is determined that a negative cycle exists using your results (2 pts) \\nSome of the numbers in iteration 7 still go down. This is an indication that a shorter \\npath using (n=7 edges) to the destination exists in the graph. Since the graph only has \\nn=7 nodes, this path must contain a negative cycle.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='path using (n=7 edges) to the destination exists in the graph. Since the graph only has \\nn=7 nodes, this path must contain a negative cycle.  \\n \\n2 - Points for correctly stating the iteration number: 7 and the logic behind selecting \\niter 7. \\n \\ne)\\u200b Show how the negative cycle is found using your calculated results (4 pts) \\nWe start from any of the nodes where the distance goes down at iteration 7 and find \\nthe shortest path (containing negative cycle) to t*.  \\ne.g. start at d. \\n- dâ€™s shortest distance to t* comes from e \\n- eâ€™s shortest distance to t* comes from b \\n- bâ€™s shortest distance to t* comes from c \\n- câ€™s shortest distance to t* comes from d \\nWe have come back to the starting point of the cycle. Therefore debcd forms a \\nnegative cycle. \\n \\n1 - Point for stating that distance goes down at 7th Iteration  \\n2 - Points for explaining the shortest distance method and the loop. \\n1- Point for giving the correct cycle : debcd \\n0 - If does not start from iteration 7.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Student ID: \\n5)  18 pts \\nSuppose USC wants to send m teams of students into a coding competition, each team \\nconsisting of one freshman and one sophomore (a student cannot be on multiple teams). \\nSelected students are a set of freshmen {s1 â€¦ sm} and sophomores {sm+1 â€¦ s2m} who need \\nto be matched to form teams. Further, each team formed needs to be assigned a professor \\nto mentor them. For this, a set of professors {p1 â€¦ pn} are available, but to ensure some \\nload-balancing, each professor can mentor up to 5 teams each and must mentor at least \\none team. Each student si (i = 1, â€¦, 2m) is asked for a subset of professors Xi theyâ€™d like \\nto be mentored by, and a professor pk can be assigned to mentor a team (si, sj) if pk is in \\nboth Xi and Xj. \\n \\na)\\u200b Design a network flow algorithm to determine if it is feasible to form m teams and \\nassign mentors given all the constraints above. (12 points) \\n \\nWe will have 3 layers - freshmen, sophomores, and professors. The idea would be to have'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='assign mentors given all the constraints above. (12 points) \\n \\nWe will have 3 layers - freshmen, sophomores, and professors. The idea would be to have \\na â€˜unit of flowâ€™ represent a team with its mentor. Professor layer has to be in the middle \\nso that it can connect to both the other layers, which is required to capture the \\nconnectivity constraints posed by the Xiâ€™s. \\n \\nNetwork construction: \\n1) Node S with demand -m, node T with +m. \\n2) First layer {s1 â€¦ sm} as nodes, with edges from S to all with cap 1 (optional: lb 1),\\u200b\\n3) Second layer pjâ€™s as EDGES with lb 1, cap 5,\\u200b\\n4) 3rd layer {sm+1 â€¦ s2m} as nodes, with edges to T from all with cap 1 (optional: lb 1).\\u200b\\n5) Add edge between a professor pk and a student si (directed along S to T) if pk is in Xi. \\n \\nAlternative with no S/T: \\nNo need to add S,T (and their edges) - Directly add demand -1 to freshmen nodes, and +1 \\nto sophomore nodes. \\n \\nthe freshmen and sophomore nodes have symmetric design and their constructions can be'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='No need to add S,T (and their edges) - Directly add demand -1 to freshmen nodes, and +1 \\nto sophomore nodes. \\n \\nthe freshmen and sophomore nodes have symmetric design and their constructions can be \\nflipped (i.e. all arrow directions and demands) \\n \\nRubric: \\n2 pts - representing freshmen and sophomore as nodes \\n3 pts - Nodes S, T (with edges to corresponding student layers) and demands -m, +m\\u200b\\nOR simply adding demands -1, +1 to all the freshmen, sophomores resp. \\n4 pts -  Points for professors represented as edges (2) with correct LB and cap (2)  \\n3 pts - Points for connecting EACH of the student layers to the professor layer (2) and \\nmentioning si is connected to pk if it is contained in Xi (1)..'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Student ID: \\nb)\\u200b Prove correctness of your algorithm. (6 points) \\n \\nForming m teams and assigning mentors given all the constraints is feasible if and only if \\nthe constructed network has feasible circulation. \\n \\n=> ) Suppose m teams with mentors can be formed to satisfy all the constraints.\\u200b\\nFor each team (si, sj) and their mentor pk , assign flow of 1 on the path S - si - edge pk \\n- sj - T (if using the â€˜no S/T approachâ€™, simply the path si - edge pk - sj ).\\u200b\\nThis path exists as per the step 5) of construction since pk must be in both Xi and Xj.\\u200b\\nEach student being in exactly 1 team (thus, having m total teams) satisfies the demands \\non S and T and the capacities of the corresponding edges (or demands on all the student \\nnodes if using â€˜no S/T approachâ€™).\\u200b\\nLoad-balancing of mentors ensures that LB and cap on each edge pk is satisfied. \\nThus, we have feasible circulation in the network. \\n \\n<= ) Suppose the network has feasible circulation.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Load-balancing of mentors ensures that LB and cap on each edge pk is satisfied. \\nThus, we have feasible circulation in the network. \\n \\n<= ) Suppose the network has feasible circulation. \\nFor every unit flow on an edge pk , we can trace it to either side using flow \\nconservation (since no demands on endpoints of edge pk and the student nodes) to \\nget a path S - si - edge pk - sj - T with unit flow (if using the â€˜no S/T approachâ€™, simply \\nthe path si - edge pk - sj using flow conservation on endpoints of edge pk ). For each \\nsuch path, form the team (si, sj) and assign pk as their mentor.\\u200b\\nstep 5) of construction ensures pk must be in both Xi and Xj. \\nDemands on S, T ensure there are m teams. Capacities 1 on edges from S to freshmen \\nand sophomores to T ensure each student is in exactly one team. (ensured by demands on \\nall the student nodes if using â€˜no S/T approachâ€™). \\nLoad-balancing of mentors is ensured by the LB and cap on each edge pk.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='all the student nodes if using â€˜no S/T approachâ€™). \\nLoad-balancing of mentors is ensured by the LB and cap on each edge pk. \\n \\n \\n2 - Point for claim thatâ€™s clear and precise (for a nearly correct network) \\n2 - Points for proof in forward direction  \\n2 - Points for proof in backward direction'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Student ID: \\n6)  18 pts \\nA film screening festival is being hosted in your neighborhood which will last for D days, \\nwith one movie screened each day. The list of movies consists of English, French and \\nKorean movies.\\u200b\\nThe movie screened on day d \\nâ—\\u200b is in language Ld âˆˆ {En, Fr, Ko}, \\nâ—\\u200b has a runtime of td minutes (integer) and \\nâ—\\u200b has a rating of Rd (may not be integer). \\nGiven your busy schedule, you have decided you can spend at most T time for the film \\nfestival (in minutes, integer). You want to watch the best movies as much as possible, so \\nyou set the objective of maximizing the total rating of movies you watch. Your only other \\nconstraint is that you do NOT want to watch two movies in the same language back to \\nback. For instance, if you watch a movie on day 10 and day 14 (skipping days 11-13), \\nthen L10 and L14 must be different languages. \\nUse dynamic programming to compute the maximum total rating of movies you can \\nwatch given the constraints above.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='then L10 and L14 must be different languages. \\nUse dynamic programming to compute the maximum total rating of movies you can \\nwatch given the constraints above. \\nDefine (in plain English) subproblems to be solved. (4 pts) \\n \\nOPT_en(d,t) = Max total rating possible from watching movies from days 1, â€¦, d \\nwith t minutes available, with the last movie watched being in English. \\nSimilarly, Opt_fr(d,t) and Opt_ko(d,t). \\n \\nEquivalently, OPT(d, t, l) = Max total rating possible from watching movies from \\ndays 1, â€¦, d with t minutes available, with the last movie watched being in \\nLanguage l.  \\n \\n4pt - correct subproblem \\n2pt - (if not correct) close enough subproblem, including the following cases: \\n1.\\u200b Miss to separate different languages for 2-D definition. \\n2.\\u200b Define \\n with the last parameter specified to the exact \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘,  ğ‘¡, ğ¿ğ‘‘]\\nlanguage on day d.  \\n \\nNote that the two solutions above only differ in notation, not the actual subproblems.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Student ID: \\nb) Write the recurrence relation(s) for the subproblems (6 pts) \\n \\nThere should be a way to decide if you can watch the movie on day d by controlling \\nwhich language is permissible.  \\n \\nIf \\n: \\nğ¿ğ‘‘==  ğ¸ğ‘›\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘,  ğ‘¡] =  ğ‘šğ‘ğ‘¥ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘âˆ’1,  ğ‘¡],  ğ‘…ğ‘‘+ ğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘]}\\n{\\n}\\nIf \\n: \\nğ¿ğ‘‘ ! = ğ¸ğ‘›\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘âˆ’1, ğ‘¡]\\nSimilar recurrence for \\n and \\n (Condition checks for respective \\nğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡]\\nğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡]\\nlanguage L to include the 2nd term inside â€˜maxâ€™ which calls subproblems for Lâ€™ and \\nLâ€™â€™, ignores the 2nd term if condition not true) \\n \\nRubrics breakdown: \\n \\n3pt - Correct for watching film on day d. \\nOnly when \\n,  \\nğ¿ğ‘‘! = ğ¿ğ‘‘âˆ’1\\n-- 2-Dimension \\n similar for \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘…ğ‘‘+ ğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘]}\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡], ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡]\\n-- 3-Dimension \\n \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] = ğ‘…ğ‘‘+\\nğ‘™ğ‘™âˆˆ{ğ¸ğ‘›, ğ¹ğ‘Ÿ,ğ¾ğ‘œ} && ğ‘™ğ‘™!=ğ‘™\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘  , ğ‘™ğ‘™]} \\n \\n \\n3pt - Correct for skipping case.  \\n-- 2-Dimension \\n for each language \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡]\\n-- 3-Dimension'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='ğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] = ğ‘…ğ‘‘+\\nğ‘™ğ‘™âˆˆ{ğ¸ğ‘›, ğ¹ğ‘Ÿ,ğ¾ğ‘œ} && ğ‘™ğ‘™!=ğ‘™\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘  , ğ‘™ğ‘™]} \\n \\n \\n3pt - Correct for skipping case.  \\n-- 2-Dimension \\n for each language \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡]\\n-- 3-Dimension \\n \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] = ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡, ğ‘™]\\n \\n0pt - if movies are skipped without checking the conditions. \\n \\n \\nc) Using the recurrence formula in part b, write pseudocode using iteration to compute \\nthe minimum number of packages to meet the objective. (6 pts) \\n \\n2pt - Correct pseudocode (order of computation (loop) increasing in d, t) \\nPrecondition: OPT definition in a) and recurrence formula in b) have to be close to \\ncorrect! - 0pt otherwise \\n1pt - each minor error (if more than one, 0pt)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Student ID: \\n \\nSpecify base cases and their values (3 pts) \\nOPT_en[0,t] = OPT_fr[0,t] = OPT_ko[0,t] = 0 \\nOPT_en[i,t] = -inf for t<0 // okay to skip this base case if otherwise accounted for by \\na condition in the recurrence, that ignores a term when t_d > t etc. \\n \\n1pt - Correct base case with zero available day. (no partial credit) \\n-- 2-Dimension \\n \\nâˆ€ğ‘¡âˆˆ[0, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[0, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[0, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[0, ğ‘¡] = 0\\n-- 3-Dimension \\n \\nâˆ€ğ‘¡âˆˆ[0, ğ‘‡], ğ‘™âˆˆ{ğ¸ğ‘›, ğ¹ğ‘Ÿ, ğ¾ğ‘œ},  ğ‘‚ğ‘ƒğ‘‡[0, ğ‘¡, ğ‘™] = 0\\n \\n2pt - Correct base case with negative available time.  \\n-- 2-Dimension \\n \\nâˆ€ğ‘¡< 0, ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡] =  âˆ’âˆ\\n-- 3-Dimension \\n \\nâˆ€ğ‘¡< 0,  ğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] =  âˆ’âˆ\\n \\n \\n \\nSpecify where the final answer can be found? (1 pt) \\n \\n1pt - Final answer location. (no partial credit) \\n-- 2-Dimension \\n \\nğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ·, ğ‘‡]}\\n-- 3-Dimension \\n \\nğ‘™âˆˆ{ğ¸ğ‘›,ğ¹ğ‘Ÿ,ğ¾ğ‘œ}\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ·, ğ‘‡, ğ‘™]}\\n \\nd) What is the complexity of your solution? (1 pt) \\nIs this an efficient solution? (1 pt) \\nO(DT).\\u200b'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='ğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ·, ğ‘‡]}\\n-- 3-Dimension \\n \\nğ‘™âˆˆ{ğ¸ğ‘›,ğ¹ğ‘Ÿ,ğ¾ğ‘œ}\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ·, ğ‘‡, ğ‘™]}\\n \\nd) What is the complexity of your solution? (1 pt) \\nIs this an efficient solution? (1 pt) \\nO(DT).\\u200b\\nPseudo-polynomial run time (due to non-polynomial dependence on the value T), i.e., \\nthis is not an efficient solution. \\n \\n \\n1pt - Correct Time Complexity O(D*T). \\n1pt - Stating not efficient'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 0}, page_content='1 \\nHomework: Web Crawling \\n \\n \\n1. Objective \\n \\nIn this assignment, you will work with a simple web crawler to measure aspects of a crawl, study the \\ncharacteristics of the crawl, download web pages from the crawl and gather webpage metadata, all \\nfrom pre-selected news websites. \\n \\n2. Preliminaries \\n \\nTo begin we will make use of an existing open source Java web crawler called crawler4j. This crawler \\nis built upon the open source crawler4j library which is located on github. For complete details on \\ndownloading and compiling see  \\n \\nAlso see the document â€œInstructions for Installing Eclipse and Crawler4jâ€ located on the Assignments \\nweb page for help. \\nNote:  You can use any IDE of your choice. But we have provided installation instructions for Eclipse \\nIDE only \\n \\n3. Crawling \\n \\nYour task is to configure and compile the crawler and then have it crawl a news website. In the interest \\nof distributing the load evenly and not overloading the news servers, we have pre-assigned the news'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 0}, page_content='of distributing the load evenly and not overloading the news servers, we have pre-assigned the news \\nsites to be crawled according to your USC ID number, given in the table below. \\n \\nThe maximum pages to fetch can be set in crawler4j and it should be set to 20,000 to ensure a \\nreasonable execution time for this exercise. Also, maximum depth should be set to 16 to ensure that \\nwe limit the crawling. \\n \\nYou should crawl only the news websites assigned to you, and your crawler should be configured so \\nthat it does not visit pages outside of the given news website!  \\n \\nUSC ID ends with News Sites to Crawl \\nNe\\nwsS\\nite \\nNa\\nme \\nRoot URL \\n01~20 \\nNY Times \\nnytimes \\nhttps://www.nytimes.com \\n21~40 \\nWall Street Journal \\nwsj \\nhttps://www.wsj.com \\n41~60 \\nFox News \\nfoxnews \\nhttps://www.foxnews.com \\n61~80 \\nUSA Today \\nusatoday \\nhttps://www.usatoday.com \\n81~00 \\nLos Angeles Times \\nlatimes \\nhttps://www.latimes.com'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 0}, page_content='wsj \\nhttps://www.wsj.com \\n41~60 \\nFox News \\nfoxnews \\nhttps://www.foxnews.com \\n61~80 \\nUSA Today \\nusatoday \\nhttps://www.usatoday.com \\n81~00 \\nLos Angeles Times \\nlatimes \\nhttps://www.latimes.com \\n \\n \\n \\n \\nLimit your crawler so it only visits HTML, doc, pdf and different image format URLs and record the \\nmeta data for those file types'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='2 \\n4. Collecting Statistics \\n \\nYour primary task is to enhance the crawler so it collects information about: \\n1. the URLs it attempts to fetch, a two column spreadsheet, column 1 containing the URL and \\ncolumn 2 containing the HTTP/HTTPS status code received; name the file fetch_NewsSite.csv \\n(where the name â€œNewsSiteâ€ is replaced by the news website name in the table above that you \\nare crawling). The number of rows should be no more than 20,000 as that is our pre-set limit. \\nColumn names for this file can be URL and Status \\n2. the files it successfully downloads, a four column spreadsheet, column 1 containing the \\nURLs successfully downloaded, column 2 containing the size of the downloaded file (in \\nBytes, or you can choose your own preferred unit (bytes,kb,mb)), column 3 containing the \\n# of outlinks found, and column 4 containing the resulting content-type; name the file \\nvisit_NewsSite.csv; clearly the number of rows will be less than the number of rows in \\nfetch_NewsSite.csv'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='# of outlinks found, and column 4 containing the resulting content-type; name the file \\nvisit_NewsSite.csv; clearly the number of rows will be less than the number of rows in \\nfetch_NewsSite.csv \\n3. all of the URLs (including repeats) that were discovered and processed in some way; a two \\ncolumn spreadsheet where column 1 contains the encountered URL and column two an \\nindicator of whether the URL a. resides in the website (OK), or b. points outside of the website \\n(N_OK). (A file points out of the website if its URL does not start with the initial host/domain \\nname, e.g. when crawling USA Today news website all inside URLs must start with \\n.) Name the file urls_NewsSite.csv. This file will be much larger than \\nfetch_*.csv and visit_*.csv.   \\nFor example for New York Times- the URL \\n and the \\nURL \\n are both considered as residing in the same website \\nwhereas the following URL is not considered to be in the same website, \\nhttp://store.nytimes.com/'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='and the \\nURL \\n are both considered as residing in the same website \\nwhereas the following URL is not considered to be in the same website, \\nhttp://store.nytimes.com/ \\n \\nNote1: you should modify the crawler so it outputs the above data into three separate csv files; \\nyou will use them for processing later; \\nNote2: all uses of NewsSite above should be replaced by the name given in the column labeled \\nNewsSite Name in the table on page 1. \\nNote 3: You should denote the units in size column of visit.csv. The best way would be to write \\nthe units that you are using in column header name and let the rest of the size data be in numbers \\nfor easier statistical analysis. The hard requirement is only to show the units clearly and \\ncorrectly. \\n \\n \\nBased on the information recorded by the crawler in the output files above, you are to collate the \\nfollowing statistics for a crawl of your designated news website: \\n \\nâ— Fetch statistics: \\no # fetches attempted:'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='following statistics for a crawl of your designated news website: \\n \\nâ— Fetch statistics: \\no # fetches attempted: \\nThe total number of URLs that the crawler attempted to fetch. This is usually equal to the \\nMAXPAGES setting if the crawler reached that limit; less if the website is smaller than that. \\no # fetches succeeded: \\nThe number of URLs that were successfully downloaded in their entirety, i.e. returning a \\nHTTP status code of 2XX. \\no # fetches failed or aborted: \\nThe number of fetches that failed for whatever reason, including, but not limited to: HTTP'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='3 \\nredirections (3XX), client errors (4XX), server errors (5XX) and other network-related \\nerrors.1 \\nâ— Outgoing URLs: statistics about URLs extracted from visited HTML pages \\no Total URLs extracted: \\nThe grand total number of URLs extracted (including repeats) from all visited pages \\no # unique URLs extracted: \\nThe number of unique URLs encountered by the crawler \\no # unique URLs within your news website: \\nThe number of unique URLs encountered that are associated with the news website, \\ni.e. the URL begins with the given root URL of the news website, but the remainder of the \\nURL is distinct \\no # unique URLs outside the news website: \\nThe number of unique URLs encountered that were not from the news website. \\n \\nâ— Status codes: number of times various HTTP status codes were encountered during crawling, \\nincluding (but not limited to): 200, 301, 401, 402, 404, etc.  \\nâ— File sizes: statistics about file sizes of visited URLs â€“ the number of files in each size range \\n(See Appendix A).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='including (but not limited to): 200, 301, 401, 402, 404, etc.  \\nâ— File sizes: statistics about file sizes of visited URLs â€“ the number of files in each size range \\n(See Appendix A). \\no 1KB = 1024B; 1MB = 1024KB \\nâ—  Content Type: a list of the different content-types encountered \\n \\nThese statistics should be collated and submitted as a plain text file whose name is \\nCrawlReport_NewsSite.txt, following the format given in Appendix A at the end of this document. \\nMake sure you understand the crawler code and required output before you commence collating \\nthese statistics.  \\n \\nFor efficient crawling it is a good idea to have multiple crawling threads. You are required to use \\nmultiple threads in this exercise. crawler4j supports multi-threading and our examples show \\nsetting the number of crawlers to seven (see the line in the code int numberOfCrawlers = \\n7;). However, if you do a naive implementation the threads will trample on each other when'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='setting the number of crawlers to seven (see the line in the code int numberOfCrawlers = \\n7;). However, if you do a naive implementation the threads will trample on each other when \\noutputting to your statistics collection files. Therefore you need to be a bit smarter about how to \\ncollect the statistics, and crawler4j documentation has a good example of how to do this. See both \\nof the following links for details: \\n  \\n \\nand \\nhttps://github.com/yasserg/crawler4j/blob/master/crawler4j-examples/crawler4j-examples-\\nbase/src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorCrawler.java \\n \\n All the information that you are required to collect can be derived by processing the crawler \\noutput. \\n5. FAQ \\n \\nQ: For the purposes of counting unique URLs, how to handle URLs that differ only in the query \\nstring? For example: https://www.nytimes.com/page?q=0 and \\nhttps://www.nytimes.com/page?q=1'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='5. FAQ \\n \\nQ: For the purposes of counting unique URLs, how to handle URLs that differ only in the query \\nstring? For example: https://www.nytimes.com/page?q=0 and \\nhttps://www.nytimes.com/page?q=1 \\n \\n1 Based purely on the success/failure of the fetching process. Do not include errors caused by difficulty in parsing \\ncontent after it has already been successfully downloaded.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 3}, page_content='4 \\nA:  These can be treated as different URLs. \\n \\n \\nQ: URL case sensitivity: are these the same, or different URLs? \\n \\nhttps://www.nytimes.com/foo and https://www.nytimes.com/FOO \\nA:  The path component of a URL is considered to be case-sensitive, so the crawler behavior is \\ncorrect according to RFC3986. Therefore, these are different URLs. \\nThe page served may be the same because: \\nâ— that particular web server implementation treats path as case-insensitive (some server \\nimplementations do this, especially windows-based implementations) \\nâ— the web server implementation treats path as case-sensitive, but aliasing or redirect is being \\nused. \\nThis is one of the reasons why deduplication is necessary in practice. \\n \\n \\nQ: Attempting to compile the crawler results in syntax errors. \\nA:  Make sure that you have included crawler4j as well as all its dependencies. \\nAlso check your Java version; the code includes more recent Java constructs such as the typed'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 3}, page_content=\"A:  Make sure that you have included crawler4j as well as all its dependencies. \\nAlso check your Java version; the code includes more recent Java constructs such as the typed \\ncollection List<String> which requires at least Java 1.5.0. \\n \\nQ: I get the following warnings when trying to run the crawler: \\n \\nlog4j: WARN No appenders could be found for logger \\nlog4j: WARN Please initialize the log4j system properly. \\n \\n \\n \\nA:  You failed to include the log4j.properties file that comes with crawler4j. \\n \\nQ:  On Windows, I am encountering the error: Exception_Access_Violation \\nA:  This is a Java issue. See: \\n \\n \\n \\nQ: I am encountering multiple instances of this info message: \\n \\nINFO [Crawler 1] I/O exception (org.apache.http.NoHttpResponseException) \\ncaught when processing request: The target server failed to respond \\nINFO [Crawler 1] Retrying request \\n \\n \\n \\nA:  If you're working off an unsteady wireless link, you may be battling network issues such as packet\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 3}, page_content=\"INFO [Crawler 1] Retrying request \\n \\n \\n \\nA:  If you're working off an unsteady wireless link, you may be battling network issues such as packet \\nlosses â€“ try to use a better connection. If not, the web server may be struggling to keep up with the \\nfrequency of your requests. \\nAs indicated by the info message, the crawler will retry the fetch, so a few isolated occurrences of \\nthis message are not an issue. However, if the problem repeats persistently, the situation is not \\nlikely to improve if you continue hammering the server at the same frequency. Try giving the \\nserver more room to breathe:\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 4}, page_content=\"5 \\n/* \\n * Be polite: Make sure that we don't send more than \\n * 1 request per second (1000 milliseconds between requests). \\n */ \\nconfig.setPolitenessDelay(2500);         \\n/* \\n * READ ROBOTS.TXT of the website - Crawl-Delay: 10 \\n * Multiply that value by 1000 for millisecond value \\n */ \\n \\n \\n \\n \\n \\nQ: The crawler seems to choke on some of the downloaded files, for example: \\n \\njava.lang.StringIndexOutOfBoundsException: String index out of range: -2 \\n \\n \\n \\n \\njava.lang.NullPointerException: charsetName \\n \\n \\n \\n \\nA:  Safely ignore those. We are using a fairly simple, rudimentary crawler and it is not necessarily \\nrobust enough to handle all the possible quirks of heavy-duty crawling and parsing. These \\nproblems are few in number (compared to the entire crawl size), and for this exercise we're \\nokay with it as long as it skips the few problem cases and keeps crawling everything else, and \\nterminates properly â€“ as opposed to exiting with fatal errors.\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 4}, page_content='okay with it as long as it skips the few problem cases and keeps crawling everything else, and \\nterminates properly â€“ as opposed to exiting with fatal errors. \\n \\nQ: While running the crawler, you may get the following error:  \\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".  \\nSLF4J: Defaulting to no-operation (NOP) logger implementation  \\nSLF4J: See \\n for further details.   \\n \\nA. Download slf4j-simple-1.7.25.jar from \\n  add this as an external JAR to the project in the same way as the crawler-4j JAR \\nwill make the crawler display logs now. \\n \\nQ: What should we do with URL if it contains comma ? \\nA: Replace the comma with \"-\" or \"_\", so that it doesn\\'t throw an error. \\n \\nQ: Should the number of 200 codes in the fetch.csv file have to exactly match with the number of \\nrecords in the visit.csv? \\nA:  No, but it should be close, like within 2,000 of 20,000. If not then you may be filtering too \\nmuch. \\n \\nQ: \"CrawlConfig cannot be resolved to a type\" ?'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 4}, page_content='records in the visit.csv? \\nA:  No, but it should be close, like within 2,000 of 20,000. If not then you may be filtering too \\nmuch. \\n \\nQ: \"CrawlConfig cannot be resolved to a type\" ? \\nA: import edu.uci.ics.crawler4j.crawler.CrawlConfig; make sure the external jars are added to \\nClassPath. ModulePath only contains the JRE. If it doesnâ€™t work, check if standard JRE imports \\nare working. Or using an alternative way: Using maven. Initialize a new project using maven and'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content=\"6 \\nthen just add a crawler4j dependency in the pom.xml file (auto-generated by maven). The \\ndependency is given in Crawler4j github page. \\n \\nQ: What's the difference between aborted fetches and failed fetches? \\nA: failed:  Can be due to HTTP errors and other network related errors \\n    aborted:  Client decided to stop the fetching. (ex: Taking too much time to fetch)  \\nYou may sum up both the values and provide the combined result in the write up. \\n \\nQ: For some reason my crawler attempts 19,999 fetches, even though max pages is set to 20,000, \\ndoes this matter? \\nA: No, it doesnâ€™t matter. It can occur because 20,000 is the limit that you will try to fetch (it may \\ncontain successful status code like 200 and other like 301). But the visit.csv will contain only the \\nURL's for which you are able to successfully download the files. \\n \\nQ: How to differentiate fetched pages and downloaded pages?\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content=\"URL's for which you are able to successfully download the files. \\n \\nQ: How to differentiate fetched pages and downloaded pages? \\nA: In this assignment we do not ask you to save any of the downloaded files to the disk. Visiting \\na page means crawler4j processing a page (it will parse the page and extract relevant information \\nlike outgoing URLs ). That means all visited pages are downloaded. \\nYou must make sure that your crawler crawls both http and https pages of the given domain \\n \\nQ: How much time should it approximately take to crawl a website using n crawlers? \\nA: (i) Depends on your parameters set for the crawler \\n     (ii) Depends on the politeness you set in the crawler program \\nYour crawl time in hours = maxPagesToFetch / 3600 * politeness delay in seconds \\nExample: a 20,000 page fetch with a politeness delay of 2 seconds will take 11.11 hours. That is \\nassuming you are running enough threads to ensure a page fetch every 2 seconds. Therefore, it \\ncan vary for everyone.\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content='assuming you are running enough threads to ensure a page fetch every 2 seconds. Therefore, it \\ncan vary for everyone. \\n \\nQ: For the third CSV file, urls_NewSite.csv, should the discovered URLs include redirect \\nURLs? \\nA: YES, if the redirect URL is the one that gets status code 300, then the URL that redirects the \\nURL to point to will be added to the scheduler of the crawler and waits to be visited. \\n \\nQ: When the URL ends with \"/\", what needs to be done? \\nA: You should filter using content type. Please have a peek into Crawler 4j code located at \\n \\n You will get a hint on how to know the content type of the page, even if the extension is not \\nexplicitly mentioned in the URL \\n \\nQ: Eclipse keeps crashing after a few minutes of running my code. But when I reduce the no of pages to \\nfetch, it works fine.  \\nA: Increase heap size for eclipse using this. \\n \\n  \\n \\nQ: What if a URL has an unknown extension? \\nA: Please check the content type of the page if it has an unknown extension'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content='A: Increase heap size for eclipse using this. \\n \\n  \\n \\nQ: What if a URL has an unknown extension? \\nA: Please check the content type of the page if it has an unknown extension \\n \\nQ: Why do some links return True in shouldVisit() but cannot be visited by Visit()? \\nA: shouldVisit() function is used to calculate whether the page should be visited or not. It may or \\nmay not be a visitable page.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 6}, page_content='7 \\nFor example - If you are crawling the site http://viterbi.usc.edu/, the page \\nhttp://viterbi.usc.edu/mySamplePage.html should be visited. but this page may return a 404 Not \\nFound Error or it may be redirected to some other site like http://mysamplesite.com. In this case, \\nshouldVisit() function would return true because the page should be visited but visit() will not be \\ncalled because the page cannot be visited. \\nComment:  \\n \\n \\n \\nhas details on regular expressions that you need to take care.  \\n \\nComment: Since many newspaper websites dump images and other types of media on CDN,  \\nyour crawl may only encounter html files. That is fine. \\n \\nComment: File types css,js,json and others should not be visited. E.g. you can add .json to your \\npattern filter. If the extension does not appear, use \\n!page.getContentType().contains(â€œapplication.jsonâ€) \\n \\nComment: Some sites may have less than the 20,000 pages, but as long as the formula matches. \\ni,e'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 6}, page_content=\"!page.getContentType().contains(â€œapplication.jsonâ€) \\n \\nComment: Some sites may have less than the 20,000 pages, but as long as the formula matches. \\ni,e \\n# fetches attempted = # fetches succeeded + # fetches aborted + # fetches failed \\nyour homework is ok. However, the variation should not be more than 10% away from the limit \\nas it is an indication that something is wrong. \\nScenario: \\nMy visit.csv file has about 15 URLs lesser than the number of URLs with status code \\n200. It is fine if the difference is less than 10%. \\n \\nComment: the homework description states that you only need to consider HTML, doc, pdf  \\nand different image format URLs . But you should also consider URL's with no extension \\nas they may return a file of one of the above types. \\n \\nComment: The distinction between failed and aborted web pages. \\nfailed: Can be due to content not found, HTTP errors or other network related errors\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 6}, page_content='Comment: The distinction between failed and aborted web pages. \\nfailed: Can be due to content not found, HTTP errors or other network related errors \\n     \\naborted:  the client (the crawler) decided to stop the fetching. (ex: Taking too much time to \\nfetch).  \\nYou may sum up both the values and provide the combined result in the write up. \\n \\nQ: In the visit_NewsSite.csv, do we also need to chop \"charset=utf-8\" from content-type?  Or \\njust chop \"charset=urf-8\" in the report? \\nA: You can chop Encoding part(charset=urf-8) in all places. \\n \\n \\nQ: REGARDING STATISTICS \\nA: #unique URLs extracted = #unique URLs within + #unique URLs outside  \\n     #total urls extracted is the sum of #outgoing links. \\n     #total urls extracted is the sum of all values in column 3 of visit.csv \\nFor text/html files, find the number of out links. For non-text/html files, the number should be 0. \\n \\n \\nQ: How to handle pages with NO Extension'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='8 \\nA: Use getContentType() in visit() and donâ€™t rely just on extension. If the content type \\nreturned is not one of the required content types for the assignment, you should ignore it for any \\ncalculation of the statistics. This will probably result in more rows in visit.csv, but itâ€™s acceptable \\naccording to the grading guidelines. \\n \\nQ: Clarification on \"the URLs it attempts to fetch\" \\nA: \"The URLs it attempts to fetch\" means all the URLs crawled from start seed which reside in \\nthe news website and has the required media types. \\n \\n \\n \\n \\nNote #1: Extracted urls do not have to be added to visit queue. Some of them which satisfy a \\nrequirement (e.g : content type, domain, not duplicate) will be added to visit queue. But others \\nwill be dumped by the crawler.  \\nHowever, as long as the grading guideline is satisfied, we will not deduct points. \\n \\nNote#2: : 303 could be considered aborted. 404 could be considered failed.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='will be dumped by the crawler.  \\nHowever, as long as the grading guideline is satisfied, we will not deduct points. \\n \\nNote#2: : 303 could be considered aborted. 404 could be considered failed.  \\nTo summarize: we consider a request to be aborted if the crawler decides to terminate that \\nrequest. Client-side timeout is an example.  Requests can fail due to reasons like content not \\nfound, server errors, etc. \\n \\nNote#3: Fetch statistics: \\n# fetches attempted: The total number of URLs that the crawler attempted to fetch. This is \\nusually equal to the MAXPAGES setting if the crawler reached that limit; less if the website is \\nsmaller than that. \\n# fetches succeeded: The number of URLs that were successfully downloaded in their entirety, \\ni.e. returning a HTTP status code of 2XX. \\n# fetches failed or aborted: The number of fetches that failed for whatever reason, including, but \\nnot limited to: HTTP redirections (3XX), client errors (4XX), server errors (5XX) and other'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='# fetches failed or aborted: The number of fetches that failed for whatever reason, including, but \\nnot limited to: HTTP redirections (3XX), client errors (4XX), server errors (5XX) and other \\nnetwork-related errors. \\nNote#4:  Consider fetches failed and aborted as same similar to as mentioned in Note#3 \\n \\nNote#5: Hint on crawling pages other than html \\nLook for how to turn ON the Binary Content in Crawling in crawler4j. Make sure you are not \\njust crawling the html parsed data and not the binary data which includes file types other than \\nhtml. \\nSearch on the internet on how to crawl binary data and I am sure you will get something on how \\nto parse pages other than html types. \\nThere will be pages other than html in almost every news site so please make sure you crawl \\nthem properly. \\n \\nQ: Regarding the content type in visit_NewsSite.csv, should we display \"text/html;charset=UTF-\\n8\" or chop out the encoding and write \"text/html\" in the Excel sheet ?  \\nA: ONLY TEXT/HTML, ignore rest.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='8\" or chop out the encoding and write \"text/html\" in the Excel sheet ?  \\nA: ONLY TEXT/HTML, ignore rest. \\n \\nQ: Should we limit the URLs that the crawler attempted to fetch within the news domain? e.g. if \\nwe encounter \\n we should skip fetching by adding constraints in \\n\"shouldVisit()\"? But do we need to include it in urls_NewsSite.csv? \\nA: Yes, you need to include every encountered url in urls_NewsSite.csv.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 8}, page_content='9 \\nQ: All 3xx,4xx, 5xx should be considered as aborted? \\nA: YES \\n \\nQ: Are â€œcookieâ€ domains considered as an original newsite domain ? \\nA: NO, they should not be included as part of the newsite you are crawling. For details see  \\nhttps://web.archive.org/web/20200418163316/https://www.mxsasha.eu/blog/2014/03/04/definiti\\nve-guide-to-cookie-domains/ \\n \\nQ. More about statistics \\nA: visit.csv will contain the urls which are succeeded i.e. 200 status code with known/ allowed \\ncontent types. Fetch.csv will include all the urls which are been attempted to fetch i.e. with all \\nthe status codes. \\nfetch.csv entries will be = visit.csv entries (with 2xx status codes) + entries with status codes \\nother than 2XX \\nvisit.csv =  entries with 2XX status codes. \\nAlso, you should not and it is not necessary to use customized status code. Just use the status \\ncode what the webpage returns to you. \\n \\n(Note:-> fetch.csv should have urls from news site domain only)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 8}, page_content='code what the webpage returns to you. \\n \\n(Note:-> fetch.csv should have urls from news site domain only) \\n \\nQ: do we need to check content-type for all the extracted URLs, i.e. url.csv or just for visited \\nURLs, e.g. those in visit.csv? \\nA: only those in visit_NewsSite.csv \\n \\nQ: How to get the size of the downloaded file? \\nA: It will be the size of the page. Ex - for an image or pdf, it will be the size of the image or the \\npdf, for the html files, it will be the size of the file. The size should be in bytes (or kb, mb \\netc.).  (page.getContentData().length) \\n \\nQ: Change logging level in crawler4j? \\nA:  If you are using the latest version of Crawler4j, logging can be controlled through \\nlogback.xml. You can view the github issue thread for knowing more about the logback \\nconfigurations \\n- \\n \\n. \\n \\nQ: Crawling urls only yield text/html. I have only filtered out css|js|mp3|zip|gz, But all the \\nvisited urls have return type text/html is it fine? Or is there a problem?'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 8}, page_content=\"configurations \\n- \\n \\n. \\n \\nQ: Crawling urls only yield text/html. I have only filtered out css|js|mp3|zip|gz, But all the \\nvisited urls have return type text/html is it fine? Or is there a problem? \\n \\nA: It is fine. Some websites host their asset files (images/pdfs) on another CDN, and the URL for \\nthe same would be different from www.newssite.com, so you might only get html files for that \\nnews site. \\n \\nQ: Eclipse Error: Provider class org.apache.tika.parser.external.CompositeExternalParser not in \\nmodule \\n \\nI'm trying to follow the guide and run the boiler plate code, but eclipse gives this error when I'm \\ntrying to run the copy pasted code from the installation guide \\n \\nA: Please import crawler4j jars in ClassPath and not ModulePath, while configuring the build in \\nEclipse. \\n \\nQ: Illegal State Exception Error\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 9}, page_content='10 \\nA: 1) if you are using a newest java version, I would downgrade to 8 to use, there are some sort \\nof a similar issue with the newest java version. \\n2) carefully follow the instructions on Crawler4jinstallation.pdf \\n3) make sure to add the jar file to the CLASS PATH  \\n4) if any module is missing, download from google and add to the prj class path.  \\n \\n \\nQ: /data/crawl error \\nException in thread \"main\" java.lang.Exception: couldn\\'t create the storage folder: /data/crawl \\ndoes it already exist ? \\n at edu.uci.ics.crawler4j.crawler.CrawlController.<init>(CrawlController.java:84) \\n at Controller.main(Controller.java:20) \\n \\nA: Replace the path /data/crawl in the Controller class code with a location on your machine  \\n \\nQ: Do we need to remove duplicate urls in fetch.csv (if exists)? \\n \\nA: Crawler4j already handles duplication checks so you don\\'t have to handle it. It doesn\\'t crawl \\npages that have already been visited. \\n \\nQ: Error in Controller.java- \"Unhandled exception type Exception\"'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 9}, page_content='pages that have already been visited. \\n \\nQ: Error in Controller.java- \"Unhandled exception type Exception\" \\nA: Make sure Exception Handling is taken care of in the code. Since CrawlController class \\nthrows exception, so it needs to be handled inside a try-catch block. \\n \\nQ: Crawler cannot stop - when I set maxFetchPage to 20000, my script cannot stop and keeps \\nrunning forever. I have to kill it by myself. However, it looks like that my crawler has crawled \\nall the 20000 pages but just cannot end. \\n \\nA:Set a reasonable maxDepthofCrawling, Politeness Delay, setSocketTimeout(), and Number of \\ncrawlers in the Controller class, and retry. Also ensure there are no System.out.print() statements \\nrunning inside the Crawler code. \\n \\nQ: If you are in countries that have connection problems. \\nA: We would suggest you to visit https://itservices.usc.edu/vpn/ for more information. \\nEnable the VPN, clear the cache, restart the computer should help solve the problem.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 9}, page_content='A: We would suggest you to visit https://itservices.usc.edu/vpn/ for more information. \\nEnable the VPN, clear the cache, restart the computer should help solve the problem. \\n \\n \\n \\n \\n6. Submission Instructions \\n \\n \\nâ— Save your statistics report as a plain text file and name it based on the news website \\ndomain names assigned below: \\n \\nUSC ID ends with Site  \\n01~20 \\nCrawlReport_nytimes.txt \\n21~40 \\nCrawlReport_wsj.txt \\n41~60 \\nCrawlReport_foxnews.txt \\n61~80 \\nCrawlReport_usatoday.txt'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 10}, page_content='11 \\n81~00 \\nCrawlReport_latimes.txt \\n \\n \\n \\n \\nâ— Also include the output files generated from your crawler run, using the extensions as \\nshown above: \\no fetch_NewsSite.csv \\no visit_NewsSite.csv \\nâ— \\nDo NOT include the output files  \\no urls_NewsSite.csv \\nwhere _NewSite should be replaced by the name from the table above. \\nâ— Do not submit Java code or compiled programs; it is not required. \\nâ— Compress all of the above into a single zip archive and name it: \\ncrawl.zip \\nUse only standard zip format. Do NOT use other formats such as zipx, rar, ace, etc. For \\nexample the zip file might contain the following three files: \\n \\n1. CrawlReport_nytimes.txt, (the statistics file)         \\n2. fetch_nytimes.csv          \\n3. visit_nytimes.csv \\n \\nâ— Please upload your homework to your Google Drive CSCI572 folder, in the subfolder named \\nhw2 \\n \\n \\n \\nAppendix A \\n \\nUse the following format to tabulate the statistics that you collated based on the crawler outputs.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 10}, page_content='hw2 \\n \\n \\n \\nAppendix A \\n \\nUse the following format to tabulate the statistics that you collated based on the crawler outputs. \\n \\nNote: The status codes and content types shown are only a sample. The status codes and content types \\nthat you encounter may vary, and should all be listed and reflected in your report. Do NOT lump \\neverything else that is not in this sample under an â€œOtherâ€ heading. You may, however, exclude status \\ncodes and types for which you have a count of zero. Also, note the use of multiple threads. You are \\nrequired to use multiple threads in this exercise. \\n \\nCrawlReport_NewsSite.txt'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 11}, page_content='12 \\nName: Tommy Trojan \\nUSC ID: 1234567890 \\nNews site crawled: nytimes.com \\nNumber of threads: 7 \\n \\nFetch Statistics \\n================ \\n# fetches attempted: \\n# fetches succeeded: \\n# fetches failed or aborted: \\n \\nOutgoing URLs: \\n============== \\nTotal URLs extracted: \\n# unique URLs extracted: \\n# unique URLs within News Site: \\n# unique URLs outside News Site: \\n \\nStatus Codes: \\n============= \\n200 OK: \\n301 Moved Permanently: \\n401 Unauthorized: \\n403 Forbidden: \\n404 Not Found: \\n \\nFile Sizes: \\n=========== \\n< 1KB: \\n1KB ~ <10KB: \\n10KB ~ <100KB: \\n100KB ~ <1MB: \\n>= 1MB: \\n \\nContent Types: \\n============== \\ntext/html: \\nimage/gif: \\nimage/jpeg: \\nimage/png: \\napplication/pdf:'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 0}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nName:\\nUSC ID:\\nRead the following rules carefully:\\nâ€¢ Write your name and ID number in the solution you submit.\\nâ€¢ Please sign and submit the code of honor in the exam with your solutions. The exam\\ncannot be graded without a signed code of honor. You are supposed to do all of the\\nproblems on your own without receiving help from others.\\nâ€¢ Cheating in the exam will not be tolerated. If you are caught cheating, you will be\\nreported to the authorities. The recommendation of the instructor will be at least an\\nF in the course in such cases.\\nâ€¢ Do not post any questions on Piazza about the exam. The TAs have been instructed to\\nrefrain from answering questions about the midterm. In case of ambiguity or problems\\nin the questions, just do your best.\\nâ€¢ The use of generative AI is prohibited in answering the questions.\\nâ€¢ Problems are not sorted in terms of di!culty. Please avoid guess work and long and\\nirrelevant answers.'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 0}, page_content='â€¢ The use of generative AI is prohibited in answering the questions.\\nâ€¢ Problems are not sorted in terms of di!culty. Please avoid guess work and long and\\nirrelevant answers.\\nâ€¢ Instructions on submitting the solutions to paper and pencil and coding problems will\\nbe provided shortly by the TAs. You can handwrite or typeset your solutions to paper\\nand pencil problems.\\nâ€¢ Show all your work and your ï¬nal answer. Showing only the ï¬nal answer of a question\\nmay not receive full credit and you must show your solution and reasoning behind the\\nanswer. Simplify your answer as much as you can.\\nâ€¢ The exam has 8 questions, 16 pages, and a total of 100 points.\\nâ€¢ The submission deadline for this midterm is 11:59 PM, Friday, October 24, 2025.\\nâ€¢ As this is a take home exam that extends over several days, OSAS students DO NOT\\nreceive extra time, per OSAS guidelines.\\nâ€¢ Any change in the midterm (paper and pencil or coding) after the deadline is considered'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 0}, page_content='receive extra time, per OSAS guidelines.\\nâ€¢ Any change in the midterm (paper and pencil or coding) after the deadline is considered\\nlate submission. One second late is late. The midterm is graded based on when it was\\nsubmitted, not when it was ï¬nished. The midterm can be submitted up to three days\\nlate, with 10% penalty per late day.\\nHomework late days cannot be used for the\\nmidterm.\\nâ€¢ Submission after the grace period will receive a zero. One second late is late.\\n1'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 1}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nGrading Breakdown\\nProblem\\nScore\\nEarned\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n5\\n10\\n6\\n10\\n7\\n20\\n8\\n20\\nTotal\\n100\\n2'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 2}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nHonor Code\\nI pledge on my honor that I have not given or received any unauthorized assistance on\\nthis examination.\\nName:\\nSignature:\\n3'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 3}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n1. You are working with a dataset where you train ï¬ve polynomial regression models\\n(Model A to E) of increasing complexity (degrees 1, 3, 5, 7, and 9 respectively) on\\nthe same training data of size n = 80. The performance of these models has been\\nevaluated using 5-fold cross-validation. The table below shows the average training\\nand validation Mean Squared Errors (MSE) and Mean Absolute Errors (MAE) for\\neach model:\\nModel\\nDegree\\nTraining MSE\\nValidation MSE\\nA\\n1\\n21.4\\n24.7\\nB\\n3\\n12.8\\n14.6\\nC\\n5\\n6.9\\n8.2\\nD\\n7\\n3.0\\n9.9\\nE\\n9\\n1.2\\n23.3\\nInstructions:\\n(a) Deï¬ne and explain the mathematical concepts of bias and variance. How do they\\nrelate to model complexity in supervised learning?\\n(b) Using the above table, calculate the bias2, variance, and Expected Prediction\\nError (EPE) for each model. Assume:\\nâ€¢ Training MSE â†’Variance\\nâ€¢ Validation MSE â†’Total EPE\\nâ€¢ Irreducible Error = 1.0'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 3}, page_content='(b) Using the above table, calculate the bias2, variance, and Expected Prediction\\nError (EPE) for each model. Assume:\\nâ€¢ Training MSE â†’Variance\\nâ€¢ Validation MSE â†’Total EPE\\nâ€¢ Irreducible Error = 1.0\\n(c) For x = 2, 4, 6, 8, use the following table of true function values f(x) and model\\npredictions Ë†f(x) from Model C. Compute the absolute prediction error and squared\\nerror for each x. Then compute the average squared error (MSE):\\nx\\n2\\n4\\n6\\n8\\nf(x) (True)\\n5.0\\n9.5\\n13.0\\n16.5\\nË†f(x) (Predicted)\\n4.8\\n10.0\\n12.2\\n17.0\\n|f(x) â†‘Ë†f(x)|\\n(f(x) â†‘Ë†f(x))2\\n(d) Interpret the trends observed in the table. Which model provides the best tradeo!\\nbetween bias and variance? Justify your choice using comparisons across at least\\nthree models, and classify which models su!er from underï¬tting or overï¬tting.\\n4'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 8}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n2. Consider a logistic regression problem in which there are no features, which means\\nthat:\\nPr(Y = 1) =\\neÏ‰0\\n1 + eÏ‰0\\nAssume that we have m data points with label Y = 1 and n data points with label\\nY = 0 (remember that features are irrelevant).\\n(a) Write down the likelihood function l(Ï‰0).\\n(b) Find the Maximum Likelihood estimate Ë†Ï‰0 for this data set. [Hint: maximize\\nloge l(Ï‰0)].\\n(c) Determine conditions under which this simple classiï¬er classiï¬es data points into\\nY = 1 or Y = 0.\\n5'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 12}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n3. For the following data set for classiï¬cation:\\nIndex\\nX\\nY\\n1\\n-1\\n1\\n2\\n0\\n0\\n3\\n3\\n0\\n4\\n1\\n1\\n5\\n-2\\n0\\nAssume that we want to construct a regularized logistic regression model for this\\ndataset.\\n(a) Write down the L1-regularized loss function J(Ï‰0, Ï‰1) for this dataset with regu-\\nlarization parameter Îµ = 2.\\n(b) Compare the bias variance of the regularized model with the unregularized model\\n(Îµ = 0).\\n6'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 16}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n4. Consider the following data set for classiï¬cation:\\nIndex\\nX\\nY\\n1\\n1\\n1\\n2\\n-1\\n0\\n3\\n-2\\n0\\n(a) Show all possible bootstrap samples of the dataset that have the same size as this\\ndataset. Note that permutations of the same data set are considered the same\\ndataset, for example {1, 2, 3} and {2, 3, 1} are the same dataset.\\n(b) Construct a KNN classiï¬cation model for all bootstrap samples in part 4a with\\nK = 2 and predict the label for the test point xâ†’= 0 by majority votes of the\\npredictions of those bootstrap models. Break ties in favor of class 1.\\n7'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 22}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n5. You are given the following dataset containing short text sequences and their associated\\nlabels:\\nTask: Explain and demonstrate how this text data can be processed and classiï¬ed\\ninto the correct sentiment class (Positive or Negative). Your answer should cover the\\nfollowing parts:\\n(a) Preprocessing\\ni. Tokenize and lowercase the sentence:\\nâ€œI Loved the movie, it was fantastic!â€\\nShow the processed output.\\n(b) Feature Representation\\ni. Explain the di!erence between:\\nâ€¢ Bag-of-Words (BoW) representation\\nâ€¢ TFâ€“IDF (Term Frequencyâ€“Inverse Document Frequency) representation\\nii. For the word â€œmovieâ€, calculate its TFâ€“IDF value.\\nUse the tokenized\\nversion of the ï¬rst sentence (â€œI loved the movie, it was fantastic!â€) to\\ncompute TF, and use the entire dataset above to compute IDF.\\nShow the formula and calculation steps.\\n(c) Model Building Use the NaÂ¨Ä±ve Bayes classiï¬er and binary TF (TF=1 if the'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 22}, page_content='compute TF, and use the entire dataset above to compute IDF.\\nShow the formula and calculation steps.\\n(c) Model Building Use the NaÂ¨Ä±ve Bayes classiï¬er and binary TF (TF=1 if the\\nterm exists in the document and TF=0 if it does not) to classify the sentence\\nâ€œWhat a great movie.â€ Use histograms for your density estimates.\\n8'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 28}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n6. A researcher studies the relationship between weekly exercise hours (X) and stress level\\nscore (Y ) in graduate students. The sample size is n = 26 and the sample Pearson\\ncorrelation is r = â†’0.39.\\n(a) Test at signiï¬cance level Ï‰ = 0.05 the null hypothesis H0 : Îµ1 = 0 versus the\\ntwo-sided alternative H1 : Îµ1 â†‘= 0. Show the test statistic, decision rule, and\\nconclusion in context.\\n(b) Report and interpret the coe!cient of determination R2.\\n(c) Brieï¬‚y explain what the negative sign of r indicates in this scenario.\\nNote: Everything needed to solve this question is contained in the exam; do not\\nconsult external tables.\\n9'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 31}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n7. You are given a binary dataset with two real-valued features (x, y) and a label in {0, 1}.\\nClass 0 points are denoted by â†’and class 1 points by â†‘.\\nImportant rules for all parts.\\nâ€¢ A point may not be its own neighbor.\\nâ€¢ Break ties in neighbor votes in favor of class 0.\\nâ€¢ Show work to justify your neighbor choices (distances and votes) whenever asked.\\nDataset\\nidx\\nx\\ny\\nlabel\\n1\\n1.0\\n6.0\\n0\\n2\\n2.2\\n7.0\\n0\\n3\\n3.1\\n8.2\\n0\\n4\\n4.0\\n6.1\\n0\\n5\\n5.2\\n6.0\\n1\\n6\\n6.0\\n6.2\\n0\\n7\\n6.2\\n4.8\\n1\\n8\\n7.0\\n3.8\\n1\\n9\\n8.2\\n4.6\\n1\\n10\\n8.8\\n6.0\\n0\\n11\\n3.2\\n5.4\\n1\\n12\\n2.8\\n6.2\\n0\\n13\\n7.6\\n7.6\\n1\\n14\\n4.8\\n7.4\\n1\\nDistance metrics. For p = (x1, y1) and q = (x2, y2):\\nd1(p, q) = |x1 â†“x2| + |y1 â†“y2|\\n(Manhattan),\\nd2(p, q) =\\n!\\n(x1 â†“x2)2 + (y1 â†“y2)2\\n(Euclidean),\\ndâ†’(p, q) = max{|x1 â†“x2|, |y1 â†“y2|}\\n(Chebyshev).\\nTasks.\\n(a)\\nLOOCV of 1-NN across metrics.\\nConsider 1-nearest-neighbor classiï¬cation on (x, y). The distance function will be'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 31}, page_content='(Euclidean),\\ndâ†’(p, q) = max{|x1 â†“x2|, |y1 â†“y2|}\\n(Chebyshev).\\nTasks.\\n(a)\\nLOOCV of 1-NN across metrics.\\nConsider 1-nearest-neighbor classiï¬cation on (x, y). The distance function will be\\nselected from the options provided. For each option, calculate the LOOCV error\\nof 1-NN and indicate which distance gives superior performance. The options are:\\nManhattan d1, Euclidean d2, and Chebyshev dâ†’.\\n(b) E!ect of k.\\nFor each metric d1, d2, and dâ†’, compute the LOOCV error for k â†”{1, 3, 5}.\\nReport the error for each (metric, k) and list the misclassiï¬ed indices. For the best\\nLOOCV setting, show the full 5-NN calculation (neighbor IDs, labels, distances,\\nvote) for every misclassiï¬ed point.\\n10'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 32}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nFigure 1: Dataset with indices and class markers. Use this for geometric intuition; grading\\nis based on your calculations.\\n(c) Fixed 5-fold CV (Euclidean).\\nUsing Euclidean d2 and k â†’{1, 3, 5}, evaluate the ï¬xed folds (do not reshu!e):\\nFold 1 = {1, 7, 11},\\nFold 2 = {2, 8, 12},\\nFold 3 = {3, 9, 5},\\nFold 4 = {4, 6, 10},\\nFold 5 = {13, 14}.\\nFor each k, report the error on each fold, the mean error across folds, and the\\nmisclassiï¬ed index set per fold. Select the k recommended by this split and justify\\nbrieï¬‚y.\\n(d) 1-NN decision boundary (Euclidean).\\nSketch the qualitative 1-NN (k=1) decision regions for d2. Indicate at least two\\nplaces where the boundary is clearly non-linear due to local class interleaving It\\nis ï¬ne if you use software to plot it and you do not need to submit the code.\\n11'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 47}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n8. Programming Question: Predicting Housing Prices with Linear Regression\\nDataset: student-mat.csv\\nLink: https://archive.ics.uci.edu/dataset/320/student+performance\\nIn this problem, you will build a linear regression model to predict housing prices in\\nthe USA based on various features. You will use Python and scikit-learn for this\\ntask.\\n(a) Data Exploration and Pre-processing\\ni. Import the student-mat.csv dataset as a pandas DataFrame.\\nii. Select the following features: age, studytime, schoolsup, goout, Dalc,\\nWalc, health, absences, G3 (target/dependent variable). Encode the bi-\\nnary variable (schoolsup) values as 0s (no) and 1s (yes). Combine Dalc and\\nWalc into one variable alc by taking averge, then remove Dalc and Walc.\\nDisplay the ï¬rst ï¬ve rows of the pre-processed dataset.\\niii. Find the number of outliers for each independent variable using the IQR\\nmethod.'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 47}, page_content='Display the ï¬rst ï¬ve rows of the pre-processed dataset.\\niii. Find the number of outliers for each independent variable using the IQR\\nmethod.\\niv. Standardize and run PCA on the dataset. Create a scatterplot of PC1 vs PC2,\\ncoloring the dots by their ï¬nal grade G3. Inspect the component loadings and\\ndetermine which features contribute the most to PC1 and PC2. Keep and\\nuse standardized data for remaining problems.\\n(b) Linear Regression\\ni. Split data into training set and testing set with an 80:20 ratio. Use random\\nseed 552 for reproducibility.\\nii. Build three models using the training set: A. Linear Regression Model, B.\\nLinear Regression Model with Ridge Regularization, and C. Linear Regression\\nModel with Lasso Regularization. Set Ï‰=0.1.\\niii. Test all three models on the test set. Find out the best performing model\\nwith repsect to each of the metrics: Mean Absolute Error (MAE), Root Mean\\nSquared Error (RMSE), and RÂ².\\niv. How do you interpret RÂ² values from the three models?'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 47}, page_content='with repsect to each of the metrics: Mean Absolute Error (MAE), Root Mean\\nSquared Error (RMSE), and RÂ².\\niv. How do you interpret RÂ² values from the three models?\\nv. Print coe!cients of independent variables from the three models in one table.\\nvi. Whatâ€™s the relationship between each independent variable and the dependent\\nvariable?\\nvii. How do the regularization methods diâ€er? What can you conclude about the\\ndataset and features given the results?\\nviii. If the regularization strength (Ï‰) is increased, what would happen to perfor-\\nmance metrics?\\nix. What are some feature engineering methods you would suggest to improve\\nmodel performance?\\nExpected Output: Your submission should include:\\n12'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 49}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nâ€¢ Jupyter Notebook .ipynb with all the steps clearly commented.\\nâ€¢ The output of each step as speciï¬ed above (e.g., head of DataFrame, info, describe,\\nmissing value counts, evaluation metrics for all models, coe!cients, and intercept).\\nâ€¢ Visualizations for outlier detection and residual analysis.\\nâ€¢ A brief discussion answering the interpretation questions.\\n13'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 50}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nScratch paper\\nName:\\nUSC ID:\\n14'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 51}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nScratch paper\\nName:\\nUSC ID:\\n15')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bce5e",
   "metadata": {},
   "source": [
    "### embedding and vector store db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82df0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00737a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Projects\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\viswa\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x21fce8f3e00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialise the embedding manager\n",
    "\n",
    "        Args:\n",
    "        model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\" Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "         Generate embeddings for a list of texts\n",
    "\n",
    "         Args:\n",
    "          tests: List of text strings to embed\n",
    "\n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embedding for {len(texts)} texts ...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "    # def get_embedding_dimension(self) -> int:\n",
    "    #     \"\"\"Get the embedding dimension of the model\"\"\"\n",
    "    #     if not self.model:\n",
    "    #         raise ValueError(\"model not loaded\")\n",
    "    #     return self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    ## initialize the embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2943d00",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1588489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. COllection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x21fceaff380>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "            Initialise the vector store\n",
    "\n",
    "            Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persistent_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent CHromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. COllection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            documents: List of Langchain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store\")\n",
    "\n",
    "        # Prepare data for chromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        document_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            document_text.append(doc.page_content)\n",
    "\n",
    "            #Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=document_text\n",
    "            )\n",
    "            print(f\"Successfully add {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcce4e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='CS570 Spring 2025: Analysis of Algorithms         Exam II\\u200b\\n \\n \\nPoints \\n \\nPoints \\nProblem 1 \\n16 \\nProblem 4 \\n17 \\nProblem 2 \\n9 \\nProblem 5 \\n18 \\nProblem 3 \\n20 \\nProblem 6 \\n18 \\nTotal 98 \\n\\u200b\\n \\n \\nFirst name \\n \\nLast Name \\n \\nStudent ID \\n \\n \\n \\n \\nInstructions: \\n1.\\u200b\\nThis is a 2-hr exam. Closed book and notes. No electronic devices or internet access. \\n2.\\u200b\\nA single double sided 8.5in x 11in cheat sheet is allowed. \\n3.\\u200b\\nIf a description to an algorithm or a proof is required, please limit your description or \\nproof to within 150 words, preferably not exceeding the space allotted for that question. \\n4.\\u200b\\nNo space other than the pages in the exam booklet will be scanned for grading. \\n5.\\u200b\\nIf you require an additional page for a question, you can use the extra page provided \\nwithin this booklet. However please indicate clearly that you are continuing the solution \\non the additional page. \\n6.\\u200b\\nDo not detach any sheets from the booklet. \\n7.\\u200b'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='within this booklet. However please indicate clearly that you are continuing the solution \\non the additional page. \\n6.\\u200b\\nDo not detach any sheets from the booklet. \\n7.\\u200b\\nIf using a pencil to write the answers, make sure you apply enough pressure, so your \\nanswers are readable in the scanned copy of your exam. \\n8.\\u200b\\nDo not write your answers in cursive scripts. \\n9.\\u200b\\nThis exam is printed double sided. Check and use the back of each page.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Student ID: \\n1)\\u200b\\n16 pts (2 pts each)\\u200b\\nMark the following statements as TRUE or FALSE by circling the correct answer. No need to \\nprovide any justification. \\n \\n[ TRUE/FALSE ] \\nBellman-Ford is asymptotically faster than Dijkstraâ€™s algorithm when dealing with graphs \\nthat only have non-negative edge weights. \\n[ TRUE/FALSE ]\\u200b\\nIn a maximum flow problem in a flow network with non-zero integer capacities, there \\nmust exist at least one integer-valued maximum flow f with non-zero v(f). \\n[ TRUE/FALSE ] \\nGiven a feasible circulation in a network with unlimited capacities, if the flow on each \\nedge is increased by one unit, we will get another feasible circulation.  \\n \\n[ TRUE/FALSE ] \\nSuppose an edge e is not saturated due to max flow f in a Flow Network. Then increasing \\neâ€™s capacity will not increase the max flow value of the network. \\n\\u200b\\n[ TRUE/FALSE ] \\nThe worst-case time complexity of any dynamic programming algorithm with n3 unique \\nsubproblems is â„¦(n3). \\n \\n[ TRUE/FALSE ]'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='\\u200b\\n[ TRUE/FALSE ] \\nThe worst-case time complexity of any dynamic programming algorithm with n3 unique \\nsubproblems is â„¦(n3). \\n \\n[ TRUE/FALSE ] \\nConsider the following recurrence formula  \\u200b\\n \\n \\n\\u200b\\nwhere A and B are fixed input arrays and subproblems OPT(j) are defined for j = 1, â€¦, n. \\nThen, this will lead to an O(n) dynamic programming algorithm. \\n \\n[ TRUE/FALSE ] \\nBy definition, all dynamic programming algorithms must run in polynomial time with \\nrespect to the input size (either in terms of the number of integers, or in terms of the \\nnumber of bits in the input). \\n \\n[ TRUE/FALSE ] \\nGiven a flow network G and its max flow f, we can always find an s-t  cut (A,B) in G \\nwhere all edges e crossing the cut either have f(e) = C(e) or f(e) = 0.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Student ID: \\n2)\\u200b\\n9 pts  -   3 pts each. No partial credit \\n \\nI. What is the capacity of the minimum s-t cut in the following flow network? \\n \\n \\nA) 5 \\nB) 6 \\nC) 7 \\nD) 8 \\nSolution: C \\n \\nII.  \\xa0Which of the following statements is/are False regarding dynamic programming? \\nSelect all correct answers! \\nA) It solves problems by breaking them into overlapping subproblems. \\nB) It can be implemented using a bottom-up approach known as tabulation. \\nC) A problem of size n must have subproblems of size n/b where b>1 is an integer\\u200b\\n     constant  \\nD) It can utilize memoization to store values of optimal solutions for unique subproblems \\nto avoid redundant calculations. \\nSolution: C \\n \\nIII. Which of the following statements is/are True about the Ford-Fulkerson algorithm? \\nSelect all correct answers! \\nA)\\u200b For integer edge capacities, it terminates in at most v(f) iterations, where v(f) is the \\nmaximum flow value.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Select all correct answers! \\nA)\\u200b For integer edge capacities, it terminates in at most v(f) iterations, where v(f) is the \\nmaximum flow value. \\nB)\\u200b For integer edge capacities, it terminates in at most C iterations, where C is the total \\ncapacity of edges out of source. \\nC)\\u200b It may converge to an incorrect value of max flow for non-integer edge capacities. \\nD)\\u200b Assuming it terminates, it will always find the same maximum flow f independent of \\nthe choice of augmenting paths. \\nSolution: A,B,C'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Student ID: \\n3)\\u200b 20 pts  \\n \\nConsider the flow network below with demands and lower bounds. \\u200b\\nNotation: (le,Ce) show lower bound and capacity on each edge. Numbers assigned to \\nnodes show the demand value at that node. \\n \\nFollow the steps as described in class to determine if a feasible circulation exists \\na)\\u200b Convert the given network to an equivalent one with no lower bounds. Show all your \\nwork. (6 pts) \\n \\n \\nNo lower bounds'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Student ID: \\n \\nRUBRIC: \\n(3 points) Draw the imbalance graph correctly \\n(3 points) Draw the final graph with no lower bounds correctly \\n-1 point for every wrong number \\n \\n \\n \\n \\n \\nb) Convert the network obtained in a) to an equivalent one with no demands (i.e., a flow \\nnetwork) (2 pts) \\n \\nFlow Network (i.e., with no demands) \\n \\nRUBRIC:\\u200b\\n(1 point) Create a super source and super sink \\n(1 point) Connect nodes correctly to either source/sink \\n0 points if no super source or super sink was created \\n \\n \\n \\n \\n \\nc) In the network obtained in b), compute max flow using Scaled version of Ford \\nFulkerson algorithm. Only show the different scaling iterations marked with the \\ncorresponding Delta value, and for each scaling iteration, show the augmenting path(s) \\nfound with the respective augmenting flow value. No need to show residual graphs. \\u200b\\n(10 pts)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Student ID: \\nMax flow with scaled version of Ford Fulkerson algorithm \\n \\n \\nRUBRIC: \\n(- 5 points):  Did not use scaled version of Ford Fulkerson algorithm \\n(- 2 points): Incorrect max flow value \\n(- 2 points): Missed delta 8, 4 and 1 in calculation \\n(- 1 point): For each minor calculation error'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Student ID: \\n \\nd) Based on your solution in part c, explain whether the originally given network has a \\nfeasible circulation or not. (2 pts) \\nSince max flow not equal to total demand, the circulation is not feasible \\n \\n(2 points) correctly mention that circulation is not feasible because max flow not equal to \\ntotal demand \\n0 points if there is no mention of max flow being not equal to demand AND circulation \\nnot being feasible'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Student ID: \\n4)\\u200b 17 pts \\nConsider the directed graph below.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\na)\\u200b Use the Bellman-Ford algorithm to find the negative cycle in this graph. In other \\nwords, you must solve this problem numerically with the given edge weights and  \\n     Explain how the destination node t is determined (3 pts) \\ncreate new node t* and connect all nodes to t* with distance of 0. \\n \\n1 - point for creating new node  \\n2 - for connecting all the nodes distance 0 (or any number as long as it is the same for \\nall nodes) \\n0 - no creation of a new node \\n \\n \\nb)\\u200b  State how many iterations of Bellman-Ford are required (2 pts) \\nn=7 iterations \\n2 - Point for stating the correct number of iterations  \\n0 - Incorrect answer'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Student ID: \\nc)\\u200b  Show each iteration of Bellman-Ford by showing subproblem values in a table \\nwith columns representing nodes and rows representing iterations (6 pts) \\n \\n \\na \\nâˆ \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\nb \\nâˆ \\n0 \\n0 \\n0 \\n-1 \\n-2 \\n-2 \\n-2 \\nc \\nâˆ \\n0 \\n0 \\n-5 \\n-6 \\n-6 \\n-6 \\n-7 \\nd \\nâˆ \\n0 \\n-7 \\n-8 \\n-8 \\n-8 \\n-9 \\n-10 \\ne \\nâˆ \\n0 \\n-1 \\n-1 \\n-1 \\n-2 \\n-3 \\n-3 \\nf \\nâˆ \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\nt* \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n \\nw/ up \\nto 0 \\nedges \\nw/ up \\nto 1 \\nedges \\nw/ up \\nto 2 \\nedges \\nw/ up \\nto 3 \\nedges \\nw/ up \\nto 4 \\nedges \\nw/ up \\nto 5 \\nedges \\nw/ up \\nto 6 \\nedges \\nw/ up \\nto 7 \\nedges \\n \\n1 - Point for each correct iteration. Whole iteration must be correct to get 1 full point. \\n \\n \\nd)\\u200b Show how it is determined that a negative cycle exists using your results (2 pts) \\nSome of the numbers in iteration 7 still go down. This is an indication that a shorter \\npath using (n=7 edges) to the destination exists in the graph. Since the graph only has \\nn=7 nodes, this path must contain a negative cycle.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='path using (n=7 edges) to the destination exists in the graph. Since the graph only has \\nn=7 nodes, this path must contain a negative cycle.  \\n \\n2 - Points for correctly stating the iteration number: 7 and the logic behind selecting \\niter 7. \\n \\ne)\\u200b Show how the negative cycle is found using your calculated results (4 pts) \\nWe start from any of the nodes where the distance goes down at iteration 7 and find \\nthe shortest path (containing negative cycle) to t*.  \\ne.g. start at d. \\n- dâ€™s shortest distance to t* comes from e \\n- eâ€™s shortest distance to t* comes from b \\n- bâ€™s shortest distance to t* comes from c \\n- câ€™s shortest distance to t* comes from d \\nWe have come back to the starting point of the cycle. Therefore debcd forms a \\nnegative cycle. \\n \\n1 - Point for stating that distance goes down at 7th Iteration  \\n2 - Points for explaining the shortest distance method and the loop. \\n1- Point for giving the correct cycle : debcd \\n0 - If does not start from iteration 7.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Student ID: \\n5)  18 pts \\nSuppose USC wants to send m teams of students into a coding competition, each team \\nconsisting of one freshman and one sophomore (a student cannot be on multiple teams). \\nSelected students are a set of freshmen {s1 â€¦ sm} and sophomores {sm+1 â€¦ s2m} who need \\nto be matched to form teams. Further, each team formed needs to be assigned a professor \\nto mentor them. For this, a set of professors {p1 â€¦ pn} are available, but to ensure some \\nload-balancing, each professor can mentor up to 5 teams each and must mentor at least \\none team. Each student si (i = 1, â€¦, 2m) is asked for a subset of professors Xi theyâ€™d like \\nto be mentored by, and a professor pk can be assigned to mentor a team (si, sj) if pk is in \\nboth Xi and Xj. \\n \\na)\\u200b Design a network flow algorithm to determine if it is feasible to form m teams and \\nassign mentors given all the constraints above. (12 points) \\n \\nWe will have 3 layers - freshmen, sophomores, and professors. The idea would be to have'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='assign mentors given all the constraints above. (12 points) \\n \\nWe will have 3 layers - freshmen, sophomores, and professors. The idea would be to have \\na â€˜unit of flowâ€™ represent a team with its mentor. Professor layer has to be in the middle \\nso that it can connect to both the other layers, which is required to capture the \\nconnectivity constraints posed by the Xiâ€™s. \\n \\nNetwork construction: \\n1) Node S with demand -m, node T with +m. \\n2) First layer {s1 â€¦ sm} as nodes, with edges from S to all with cap 1 (optional: lb 1),\\u200b\\n3) Second layer pjâ€™s as EDGES with lb 1, cap 5,\\u200b\\n4) 3rd layer {sm+1 â€¦ s2m} as nodes, with edges to T from all with cap 1 (optional: lb 1).\\u200b\\n5) Add edge between a professor pk and a student si (directed along S to T) if pk is in Xi. \\n \\nAlternative with no S/T: \\nNo need to add S,T (and their edges) - Directly add demand -1 to freshmen nodes, and +1 \\nto sophomore nodes. \\n \\nthe freshmen and sophomore nodes have symmetric design and their constructions can be'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='No need to add S,T (and their edges) - Directly add demand -1 to freshmen nodes, and +1 \\nto sophomore nodes. \\n \\nthe freshmen and sophomore nodes have symmetric design and their constructions can be \\nflipped (i.e. all arrow directions and demands) \\n \\nRubric: \\n2 pts - representing freshmen and sophomore as nodes \\n3 pts - Nodes S, T (with edges to corresponding student layers) and demands -m, +m\\u200b\\nOR simply adding demands -1, +1 to all the freshmen, sophomores resp. \\n4 pts -  Points for professors represented as edges (2) with correct LB and cap (2)  \\n3 pts - Points for connecting EACH of the student layers to the professor layer (2) and \\nmentioning si is connected to pk if it is contained in Xi (1)..'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Student ID: \\nb)\\u200b Prove correctness of your algorithm. (6 points) \\n \\nForming m teams and assigning mentors given all the constraints is feasible if and only if \\nthe constructed network has feasible circulation. \\n \\n=> ) Suppose m teams with mentors can be formed to satisfy all the constraints.\\u200b\\nFor each team (si, sj) and their mentor pk , assign flow of 1 on the path S - si - edge pk \\n- sj - T (if using the â€˜no S/T approachâ€™, simply the path si - edge pk - sj ).\\u200b\\nThis path exists as per the step 5) of construction since pk must be in both Xi and Xj.\\u200b\\nEach student being in exactly 1 team (thus, having m total teams) satisfies the demands \\non S and T and the capacities of the corresponding edges (or demands on all the student \\nnodes if using â€˜no S/T approachâ€™).\\u200b\\nLoad-balancing of mentors ensures that LB and cap on each edge pk is satisfied. \\nThus, we have feasible circulation in the network. \\n \\n<= ) Suppose the network has feasible circulation.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Load-balancing of mentors ensures that LB and cap on each edge pk is satisfied. \\nThus, we have feasible circulation in the network. \\n \\n<= ) Suppose the network has feasible circulation. \\nFor every unit flow on an edge pk , we can trace it to either side using flow \\nconservation (since no demands on endpoints of edge pk and the student nodes) to \\nget a path S - si - edge pk - sj - T with unit flow (if using the â€˜no S/T approachâ€™, simply \\nthe path si - edge pk - sj using flow conservation on endpoints of edge pk ). For each \\nsuch path, form the team (si, sj) and assign pk as their mentor.\\u200b\\nstep 5) of construction ensures pk must be in both Xi and Xj. \\nDemands on S, T ensure there are m teams. Capacities 1 on edges from S to freshmen \\nand sophomores to T ensure each student is in exactly one team. (ensured by demands on \\nall the student nodes if using â€˜no S/T approachâ€™). \\nLoad-balancing of mentors is ensured by the LB and cap on each edge pk.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='all the student nodes if using â€˜no S/T approachâ€™). \\nLoad-balancing of mentors is ensured by the LB and cap on each edge pk. \\n \\n \\n2 - Point for claim thatâ€™s clear and precise (for a nearly correct network) \\n2 - Points for proof in forward direction  \\n2 - Points for proof in backward direction'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Student ID: \\n6)  18 pts \\nA film screening festival is being hosted in your neighborhood which will last for D days, \\nwith one movie screened each day. The list of movies consists of English, French and \\nKorean movies.\\u200b\\nThe movie screened on day d \\nâ—\\u200b is in language Ld âˆˆ {En, Fr, Ko}, \\nâ—\\u200b has a runtime of td minutes (integer) and \\nâ—\\u200b has a rating of Rd (may not be integer). \\nGiven your busy schedule, you have decided you can spend at most T time for the film \\nfestival (in minutes, integer). You want to watch the best movies as much as possible, so \\nyou set the objective of maximizing the total rating of movies you watch. Your only other \\nconstraint is that you do NOT want to watch two movies in the same language back to \\nback. For instance, if you watch a movie on day 10 and day 14 (skipping days 11-13), \\nthen L10 and L14 must be different languages. \\nUse dynamic programming to compute the maximum total rating of movies you can \\nwatch given the constraints above.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='then L10 and L14 must be different languages. \\nUse dynamic programming to compute the maximum total rating of movies you can \\nwatch given the constraints above. \\nDefine (in plain English) subproblems to be solved. (4 pts) \\n \\nOPT_en(d,t) = Max total rating possible from watching movies from days 1, â€¦, d \\nwith t minutes available, with the last movie watched being in English. \\nSimilarly, Opt_fr(d,t) and Opt_ko(d,t). \\n \\nEquivalently, OPT(d, t, l) = Max total rating possible from watching movies from \\ndays 1, â€¦, d with t minutes available, with the last movie watched being in \\nLanguage l.  \\n \\n4pt - correct subproblem \\n2pt - (if not correct) close enough subproblem, including the following cases: \\n1.\\u200b Miss to separate different languages for 2-D definition. \\n2.\\u200b Define \\n with the last parameter specified to the exact \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘,  ğ‘¡, ğ¿ğ‘‘]\\nlanguage on day d.  \\n \\nNote that the two solutions above only differ in notation, not the actual subproblems.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Student ID: \\nb) Write the recurrence relation(s) for the subproblems (6 pts) \\n \\nThere should be a way to decide if you can watch the movie on day d by controlling \\nwhich language is permissible.  \\n \\nIf \\n: \\nğ¿ğ‘‘==  ğ¸ğ‘›\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘,  ğ‘¡] =  ğ‘šğ‘ğ‘¥ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘âˆ’1,  ğ‘¡],  ğ‘…ğ‘‘+ ğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘]}\\n{\\n}\\nIf \\n: \\nğ¿ğ‘‘ ! = ğ¸ğ‘›\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘âˆ’1, ğ‘¡]\\nSimilar recurrence for \\n and \\n (Condition checks for respective \\nğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡]\\nğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡]\\nlanguage L to include the 2nd term inside â€˜maxâ€™ which calls subproblems for Lâ€™ and \\nLâ€™â€™, ignores the 2nd term if condition not true) \\n \\nRubrics breakdown: \\n \\n3pt - Correct for watching film on day d. \\nOnly when \\n,  \\nğ¿ğ‘‘! = ğ¿ğ‘‘âˆ’1\\n-- 2-Dimension \\n similar for \\nğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘…ğ‘‘+ ğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘]}\\n \\nğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡], ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡]\\n-- 3-Dimension \\n \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] = ğ‘…ğ‘‘+\\nğ‘™ğ‘™âˆˆ{ğ¸ğ‘›, ğ¹ğ‘Ÿ,ğ¾ğ‘œ} && ğ‘™ğ‘™!=ğ‘™\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘  , ğ‘™ğ‘™]} \\n \\n \\n3pt - Correct for skipping case.  \\n-- 2-Dimension \\n for each language \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡]\\n-- 3-Dimension'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='ğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] = ğ‘…ğ‘‘+\\nğ‘™ğ‘™âˆˆ{ğ¸ğ‘›, ğ¹ğ‘Ÿ,ğ¾ğ‘œ} && ğ‘™ğ‘™!=ğ‘™\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡âˆ’ğ‘¡ğ‘‘  , ğ‘™ğ‘™]} \\n \\n \\n3pt - Correct for skipping case.  \\n-- 2-Dimension \\n for each language \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡]\\n-- 3-Dimension \\n \\nğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] = ğ‘‚ğ‘ƒğ‘‡[ğ‘‘âˆ’1, ğ‘¡, ğ‘™]\\n \\n0pt - if movies are skipped without checking the conditions. \\n \\n \\nc) Using the recurrence formula in part b, write pseudocode using iteration to compute \\nthe minimum number of packages to meet the objective. (6 pts) \\n \\n2pt - Correct pseudocode (order of computation (loop) increasing in d, t) \\nPrecondition: OPT definition in a) and recurrence formula in b) have to be close to \\ncorrect! - 0pt otherwise \\n1pt - each minor error (if more than one, 0pt)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Student ID: \\n \\nSpecify base cases and their values (3 pts) \\nOPT_en[0,t] = OPT_fr[0,t] = OPT_ko[0,t] = 0 \\nOPT_en[i,t] = -inf for t<0 // okay to skip this base case if otherwise accounted for by \\na condition in the recurrence, that ignores a term when t_d > t etc. \\n \\n1pt - Correct base case with zero available day. (no partial credit) \\n-- 2-Dimension \\n \\nâˆ€ğ‘¡âˆˆ[0, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[0, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[0, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[0, ğ‘¡] = 0\\n-- 3-Dimension \\n \\nâˆ€ğ‘¡âˆˆ[0, ğ‘‡], ğ‘™âˆˆ{ğ¸ğ‘›, ğ¹ğ‘Ÿ, ğ¾ğ‘œ},  ğ‘‚ğ‘ƒğ‘‡[0, ğ‘¡, ğ‘™] = 0\\n \\n2pt - Correct base case with negative available time.  \\n-- 2-Dimension \\n \\nâˆ€ğ‘¡< 0, ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ‘‘, ğ‘¡] = ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ‘‘, ğ‘¡] =  âˆ’âˆ\\n-- 3-Dimension \\n \\nâˆ€ğ‘¡< 0,  ğ‘‚ğ‘ƒğ‘‡[ğ‘‘, ğ‘¡, ğ‘™] =  âˆ’âˆ\\n \\n \\n \\nSpecify where the final answer can be found? (1 pt) \\n \\n1pt - Final answer location. (no partial credit) \\n-- 2-Dimension \\n \\nğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ·, ğ‘‡]}\\n-- 3-Dimension \\n \\nğ‘™âˆˆ{ğ¸ğ‘›,ğ¹ğ‘Ÿ,ğ¾ğ‘œ}\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ·, ğ‘‡, ğ‘™]}\\n \\nd) What is the complexity of your solution? (1 pt) \\nIs this an efficient solution? (1 pt) \\nO(DT).\\u200b'),\n",
       " Document(metadata={'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CS570 Exam 2 Spring 2025 solutions and rubrics.pdf', 'total_pages': 14, 'format': 'PDF 1.4', 'title': 'CS570 Exam 2 Spring 2025 solutions and rubrics.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='ğ‘šğ‘ğ‘¥{ğ‘‚ğ‘ƒğ‘‡ğ‘’ğ‘›[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘“ğ‘Ÿ[ğ·, ğ‘‡],  ğ‘‚ğ‘ƒğ‘‡ğ‘˜ğ‘œ[ğ·, ğ‘‡]}\\n-- 3-Dimension \\n \\nğ‘™âˆˆ{ğ¸ğ‘›,ğ¹ğ‘Ÿ,ğ¾ğ‘œ}\\nmax\\n{ğ‘‚ğ‘ƒğ‘‡[ğ·, ğ‘‡, ğ‘™]}\\n \\nd) What is the complexity of your solution? (1 pt) \\nIs this an efficient solution? (1 pt) \\nO(DT).\\u200b\\nPseudo-polynomial run time (due to non-polynomial dependence on the value T), i.e., \\nthis is not an efficient solution. \\n \\n \\n1pt - Correct Time Complexity O(D*T). \\n1pt - Stating not efficient'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 0}, page_content='1 \\nHomework: Web Crawling \\n \\n \\n1. Objective \\n \\nIn this assignment, you will work with a simple web crawler to measure aspects of a crawl, study the \\ncharacteristics of the crawl, download web pages from the crawl and gather webpage metadata, all \\nfrom pre-selected news websites. \\n \\n2. Preliminaries \\n \\nTo begin we will make use of an existing open source Java web crawler called crawler4j. This crawler \\nis built upon the open source crawler4j library which is located on github. For complete details on \\ndownloading and compiling see  \\n \\nAlso see the document â€œInstructions for Installing Eclipse and Crawler4jâ€ located on the Assignments \\nweb page for help. \\nNote:  You can use any IDE of your choice. But we have provided installation instructions for Eclipse \\nIDE only \\n \\n3. Crawling \\n \\nYour task is to configure and compile the crawler and then have it crawl a news website. In the interest \\nof distributing the load evenly and not overloading the news servers, we have pre-assigned the news'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 0}, page_content='of distributing the load evenly and not overloading the news servers, we have pre-assigned the news \\nsites to be crawled according to your USC ID number, given in the table below. \\n \\nThe maximum pages to fetch can be set in crawler4j and it should be set to 20,000 to ensure a \\nreasonable execution time for this exercise. Also, maximum depth should be set to 16 to ensure that \\nwe limit the crawling. \\n \\nYou should crawl only the news websites assigned to you, and your crawler should be configured so \\nthat it does not visit pages outside of the given news website!  \\n \\nUSC ID ends with News Sites to Crawl \\nNe\\nwsS\\nite \\nNa\\nme \\nRoot URL \\n01~20 \\nNY Times \\nnytimes \\nhttps://www.nytimes.com \\n21~40 \\nWall Street Journal \\nwsj \\nhttps://www.wsj.com \\n41~60 \\nFox News \\nfoxnews \\nhttps://www.foxnews.com \\n61~80 \\nUSA Today \\nusatoday \\nhttps://www.usatoday.com \\n81~00 \\nLos Angeles Times \\nlatimes \\nhttps://www.latimes.com'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 0}, page_content='wsj \\nhttps://www.wsj.com \\n41~60 \\nFox News \\nfoxnews \\nhttps://www.foxnews.com \\n61~80 \\nUSA Today \\nusatoday \\nhttps://www.usatoday.com \\n81~00 \\nLos Angeles Times \\nlatimes \\nhttps://www.latimes.com \\n \\n \\n \\n \\nLimit your crawler so it only visits HTML, doc, pdf and different image format URLs and record the \\nmeta data for those file types'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='2 \\n4. Collecting Statistics \\n \\nYour primary task is to enhance the crawler so it collects information about: \\n1. the URLs it attempts to fetch, a two column spreadsheet, column 1 containing the URL and \\ncolumn 2 containing the HTTP/HTTPS status code received; name the file fetch_NewsSite.csv \\n(where the name â€œNewsSiteâ€ is replaced by the news website name in the table above that you \\nare crawling). The number of rows should be no more than 20,000 as that is our pre-set limit. \\nColumn names for this file can be URL and Status \\n2. the files it successfully downloads, a four column spreadsheet, column 1 containing the \\nURLs successfully downloaded, column 2 containing the size of the downloaded file (in \\nBytes, or you can choose your own preferred unit (bytes,kb,mb)), column 3 containing the \\n# of outlinks found, and column 4 containing the resulting content-type; name the file \\nvisit_NewsSite.csv; clearly the number of rows will be less than the number of rows in \\nfetch_NewsSite.csv'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='# of outlinks found, and column 4 containing the resulting content-type; name the file \\nvisit_NewsSite.csv; clearly the number of rows will be less than the number of rows in \\nfetch_NewsSite.csv \\n3. all of the URLs (including repeats) that were discovered and processed in some way; a two \\ncolumn spreadsheet where column 1 contains the encountered URL and column two an \\nindicator of whether the URL a. resides in the website (OK), or b. points outside of the website \\n(N_OK). (A file points out of the website if its URL does not start with the initial host/domain \\nname, e.g. when crawling USA Today news website all inside URLs must start with \\n.) Name the file urls_NewsSite.csv. This file will be much larger than \\nfetch_*.csv and visit_*.csv.   \\nFor example for New York Times- the URL \\n and the \\nURL \\n are both considered as residing in the same website \\nwhereas the following URL is not considered to be in the same website, \\nhttp://store.nytimes.com/'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='and the \\nURL \\n are both considered as residing in the same website \\nwhereas the following URL is not considered to be in the same website, \\nhttp://store.nytimes.com/ \\n \\nNote1: you should modify the crawler so it outputs the above data into three separate csv files; \\nyou will use them for processing later; \\nNote2: all uses of NewsSite above should be replaced by the name given in the column labeled \\nNewsSite Name in the table on page 1. \\nNote 3: You should denote the units in size column of visit.csv. The best way would be to write \\nthe units that you are using in column header name and let the rest of the size data be in numbers \\nfor easier statistical analysis. The hard requirement is only to show the units clearly and \\ncorrectly. \\n \\n \\nBased on the information recorded by the crawler in the output files above, you are to collate the \\nfollowing statistics for a crawl of your designated news website: \\n \\nâ— Fetch statistics: \\no # fetches attempted:'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 1}, page_content='following statistics for a crawl of your designated news website: \\n \\nâ— Fetch statistics: \\no # fetches attempted: \\nThe total number of URLs that the crawler attempted to fetch. This is usually equal to the \\nMAXPAGES setting if the crawler reached that limit; less if the website is smaller than that. \\no # fetches succeeded: \\nThe number of URLs that were successfully downloaded in their entirety, i.e. returning a \\nHTTP status code of 2XX. \\no # fetches failed or aborted: \\nThe number of fetches that failed for whatever reason, including, but not limited to: HTTP'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='3 \\nredirections (3XX), client errors (4XX), server errors (5XX) and other network-related \\nerrors.1 \\nâ— Outgoing URLs: statistics about URLs extracted from visited HTML pages \\no Total URLs extracted: \\nThe grand total number of URLs extracted (including repeats) from all visited pages \\no # unique URLs extracted: \\nThe number of unique URLs encountered by the crawler \\no # unique URLs within your news website: \\nThe number of unique URLs encountered that are associated with the news website, \\ni.e. the URL begins with the given root URL of the news website, but the remainder of the \\nURL is distinct \\no # unique URLs outside the news website: \\nThe number of unique URLs encountered that were not from the news website. \\n \\nâ— Status codes: number of times various HTTP status codes were encountered during crawling, \\nincluding (but not limited to): 200, 301, 401, 402, 404, etc.  \\nâ— File sizes: statistics about file sizes of visited URLs â€“ the number of files in each size range \\n(See Appendix A).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='including (but not limited to): 200, 301, 401, 402, 404, etc.  \\nâ— File sizes: statistics about file sizes of visited URLs â€“ the number of files in each size range \\n(See Appendix A). \\no 1KB = 1024B; 1MB = 1024KB \\nâ—  Content Type: a list of the different content-types encountered \\n \\nThese statistics should be collated and submitted as a plain text file whose name is \\nCrawlReport_NewsSite.txt, following the format given in Appendix A at the end of this document. \\nMake sure you understand the crawler code and required output before you commence collating \\nthese statistics.  \\n \\nFor efficient crawling it is a good idea to have multiple crawling threads. You are required to use \\nmultiple threads in this exercise. crawler4j supports multi-threading and our examples show \\nsetting the number of crawlers to seven (see the line in the code int numberOfCrawlers = \\n7;). However, if you do a naive implementation the threads will trample on each other when'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='setting the number of crawlers to seven (see the line in the code int numberOfCrawlers = \\n7;). However, if you do a naive implementation the threads will trample on each other when \\noutputting to your statistics collection files. Therefore you need to be a bit smarter about how to \\ncollect the statistics, and crawler4j documentation has a good example of how to do this. See both \\nof the following links for details: \\n  \\n \\nand \\nhttps://github.com/yasserg/crawler4j/blob/master/crawler4j-examples/crawler4j-examples-\\nbase/src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorCrawler.java \\n \\n All the information that you are required to collect can be derived by processing the crawler \\noutput. \\n5. FAQ \\n \\nQ: For the purposes of counting unique URLs, how to handle URLs that differ only in the query \\nstring? For example: https://www.nytimes.com/page?q=0 and \\nhttps://www.nytimes.com/page?q=1'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 2}, page_content='5. FAQ \\n \\nQ: For the purposes of counting unique URLs, how to handle URLs that differ only in the query \\nstring? For example: https://www.nytimes.com/page?q=0 and \\nhttps://www.nytimes.com/page?q=1 \\n \\n1 Based purely on the success/failure of the fetching process. Do not include errors caused by difficulty in parsing \\ncontent after it has already been successfully downloaded.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 3}, page_content='4 \\nA:  These can be treated as different URLs. \\n \\n \\nQ: URL case sensitivity: are these the same, or different URLs? \\n \\nhttps://www.nytimes.com/foo and https://www.nytimes.com/FOO \\nA:  The path component of a URL is considered to be case-sensitive, so the crawler behavior is \\ncorrect according to RFC3986. Therefore, these are different URLs. \\nThe page served may be the same because: \\nâ— that particular web server implementation treats path as case-insensitive (some server \\nimplementations do this, especially windows-based implementations) \\nâ— the web server implementation treats path as case-sensitive, but aliasing or redirect is being \\nused. \\nThis is one of the reasons why deduplication is necessary in practice. \\n \\n \\nQ: Attempting to compile the crawler results in syntax errors. \\nA:  Make sure that you have included crawler4j as well as all its dependencies. \\nAlso check your Java version; the code includes more recent Java constructs such as the typed'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 3}, page_content=\"A:  Make sure that you have included crawler4j as well as all its dependencies. \\nAlso check your Java version; the code includes more recent Java constructs such as the typed \\ncollection List<String> which requires at least Java 1.5.0. \\n \\nQ: I get the following warnings when trying to run the crawler: \\n \\nlog4j: WARN No appenders could be found for logger \\nlog4j: WARN Please initialize the log4j system properly. \\n \\n \\n \\nA:  You failed to include the log4j.properties file that comes with crawler4j. \\n \\nQ:  On Windows, I am encountering the error: Exception_Access_Violation \\nA:  This is a Java issue. See: \\n \\n \\n \\nQ: I am encountering multiple instances of this info message: \\n \\nINFO [Crawler 1] I/O exception (org.apache.http.NoHttpResponseException) \\ncaught when processing request: The target server failed to respond \\nINFO [Crawler 1] Retrying request \\n \\n \\n \\nA:  If you're working off an unsteady wireless link, you may be battling network issues such as packet\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 3}, page_content=\"INFO [Crawler 1] Retrying request \\n \\n \\n \\nA:  If you're working off an unsteady wireless link, you may be battling network issues such as packet \\nlosses â€“ try to use a better connection. If not, the web server may be struggling to keep up with the \\nfrequency of your requests. \\nAs indicated by the info message, the crawler will retry the fetch, so a few isolated occurrences of \\nthis message are not an issue. However, if the problem repeats persistently, the situation is not \\nlikely to improve if you continue hammering the server at the same frequency. Try giving the \\nserver more room to breathe:\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 4}, page_content=\"5 \\n/* \\n * Be polite: Make sure that we don't send more than \\n * 1 request per second (1000 milliseconds between requests). \\n */ \\nconfig.setPolitenessDelay(2500);         \\n/* \\n * READ ROBOTS.TXT of the website - Crawl-Delay: 10 \\n * Multiply that value by 1000 for millisecond value \\n */ \\n \\n \\n \\n \\n \\nQ: The crawler seems to choke on some of the downloaded files, for example: \\n \\njava.lang.StringIndexOutOfBoundsException: String index out of range: -2 \\n \\n \\n \\n \\njava.lang.NullPointerException: charsetName \\n \\n \\n \\n \\nA:  Safely ignore those. We are using a fairly simple, rudimentary crawler and it is not necessarily \\nrobust enough to handle all the possible quirks of heavy-duty crawling and parsing. These \\nproblems are few in number (compared to the entire crawl size), and for this exercise we're \\nokay with it as long as it skips the few problem cases and keeps crawling everything else, and \\nterminates properly â€“ as opposed to exiting with fatal errors.\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 4}, page_content='okay with it as long as it skips the few problem cases and keeps crawling everything else, and \\nterminates properly â€“ as opposed to exiting with fatal errors. \\n \\nQ: While running the crawler, you may get the following error:  \\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".  \\nSLF4J: Defaulting to no-operation (NOP) logger implementation  \\nSLF4J: See \\n for further details.   \\n \\nA. Download slf4j-simple-1.7.25.jar from \\n  add this as an external JAR to the project in the same way as the crawler-4j JAR \\nwill make the crawler display logs now. \\n \\nQ: What should we do with URL if it contains comma ? \\nA: Replace the comma with \"-\" or \"_\", so that it doesn\\'t throw an error. \\n \\nQ: Should the number of 200 codes in the fetch.csv file have to exactly match with the number of \\nrecords in the visit.csv? \\nA:  No, but it should be close, like within 2,000 of 20,000. If not then you may be filtering too \\nmuch. \\n \\nQ: \"CrawlConfig cannot be resolved to a type\" ?'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 4}, page_content='records in the visit.csv? \\nA:  No, but it should be close, like within 2,000 of 20,000. If not then you may be filtering too \\nmuch. \\n \\nQ: \"CrawlConfig cannot be resolved to a type\" ? \\nA: import edu.uci.ics.crawler4j.crawler.CrawlConfig; make sure the external jars are added to \\nClassPath. ModulePath only contains the JRE. If it doesnâ€™t work, check if standard JRE imports \\nare working. Or using an alternative way: Using maven. Initialize a new project using maven and'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content=\"6 \\nthen just add a crawler4j dependency in the pom.xml file (auto-generated by maven). The \\ndependency is given in Crawler4j github page. \\n \\nQ: What's the difference between aborted fetches and failed fetches? \\nA: failed:  Can be due to HTTP errors and other network related errors \\n    aborted:  Client decided to stop the fetching. (ex: Taking too much time to fetch)  \\nYou may sum up both the values and provide the combined result in the write up. \\n \\nQ: For some reason my crawler attempts 19,999 fetches, even though max pages is set to 20,000, \\ndoes this matter? \\nA: No, it doesnâ€™t matter. It can occur because 20,000 is the limit that you will try to fetch (it may \\ncontain successful status code like 200 and other like 301). But the visit.csv will contain only the \\nURL's for which you are able to successfully download the files. \\n \\nQ: How to differentiate fetched pages and downloaded pages?\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content=\"URL's for which you are able to successfully download the files. \\n \\nQ: How to differentiate fetched pages and downloaded pages? \\nA: In this assignment we do not ask you to save any of the downloaded files to the disk. Visiting \\na page means crawler4j processing a page (it will parse the page and extract relevant information \\nlike outgoing URLs ). That means all visited pages are downloaded. \\nYou must make sure that your crawler crawls both http and https pages of the given domain \\n \\nQ: How much time should it approximately take to crawl a website using n crawlers? \\nA: (i) Depends on your parameters set for the crawler \\n     (ii) Depends on the politeness you set in the crawler program \\nYour crawl time in hours = maxPagesToFetch / 3600 * politeness delay in seconds \\nExample: a 20,000 page fetch with a politeness delay of 2 seconds will take 11.11 hours. That is \\nassuming you are running enough threads to ensure a page fetch every 2 seconds. Therefore, it \\ncan vary for everyone.\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content='assuming you are running enough threads to ensure a page fetch every 2 seconds. Therefore, it \\ncan vary for everyone. \\n \\nQ: For the third CSV file, urls_NewSite.csv, should the discovered URLs include redirect \\nURLs? \\nA: YES, if the redirect URL is the one that gets status code 300, then the URL that redirects the \\nURL to point to will be added to the scheduler of the crawler and waits to be visited. \\n \\nQ: When the URL ends with \"/\", what needs to be done? \\nA: You should filter using content type. Please have a peek into Crawler 4j code located at \\n \\n You will get a hint on how to know the content type of the page, even if the extension is not \\nexplicitly mentioned in the URL \\n \\nQ: Eclipse keeps crashing after a few minutes of running my code. But when I reduce the no of pages to \\nfetch, it works fine.  \\nA: Increase heap size for eclipse using this. \\n \\n  \\n \\nQ: What if a URL has an unknown extension? \\nA: Please check the content type of the page if it has an unknown extension'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 5}, page_content='A: Increase heap size for eclipse using this. \\n \\n  \\n \\nQ: What if a URL has an unknown extension? \\nA: Please check the content type of the page if it has an unknown extension \\n \\nQ: Why do some links return True in shouldVisit() but cannot be visited by Visit()? \\nA: shouldVisit() function is used to calculate whether the page should be visited or not. It may or \\nmay not be a visitable page.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 6}, page_content='7 \\nFor example - If you are crawling the site http://viterbi.usc.edu/, the page \\nhttp://viterbi.usc.edu/mySamplePage.html should be visited. but this page may return a 404 Not \\nFound Error or it may be redirected to some other site like http://mysamplesite.com. In this case, \\nshouldVisit() function would return true because the page should be visited but visit() will not be \\ncalled because the page cannot be visited. \\nComment:  \\n \\n \\n \\nhas details on regular expressions that you need to take care.  \\n \\nComment: Since many newspaper websites dump images and other types of media on CDN,  \\nyour crawl may only encounter html files. That is fine. \\n \\nComment: File types css,js,json and others should not be visited. E.g. you can add .json to your \\npattern filter. If the extension does not appear, use \\n!page.getContentType().contains(â€œapplication.jsonâ€) \\n \\nComment: Some sites may have less than the 20,000 pages, but as long as the formula matches. \\ni,e'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 6}, page_content=\"!page.getContentType().contains(â€œapplication.jsonâ€) \\n \\nComment: Some sites may have less than the 20,000 pages, but as long as the formula matches. \\ni,e \\n# fetches attempted = # fetches succeeded + # fetches aborted + # fetches failed \\nyour homework is ok. However, the variation should not be more than 10% away from the limit \\nas it is an indication that something is wrong. \\nScenario: \\nMy visit.csv file has about 15 URLs lesser than the number of URLs with status code \\n200. It is fine if the difference is less than 10%. \\n \\nComment: the homework description states that you only need to consider HTML, doc, pdf  \\nand different image format URLs . But you should also consider URL's with no extension \\nas they may return a file of one of the above types. \\n \\nComment: The distinction between failed and aborted web pages. \\nfailed: Can be due to content not found, HTTP errors or other network related errors\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 6}, page_content='Comment: The distinction between failed and aborted web pages. \\nfailed: Can be due to content not found, HTTP errors or other network related errors \\n     \\naborted:  the client (the crawler) decided to stop the fetching. (ex: Taking too much time to \\nfetch).  \\nYou may sum up both the values and provide the combined result in the write up. \\n \\nQ: In the visit_NewsSite.csv, do we also need to chop \"charset=utf-8\" from content-type?  Or \\njust chop \"charset=urf-8\" in the report? \\nA: You can chop Encoding part(charset=urf-8) in all places. \\n \\n \\nQ: REGARDING STATISTICS \\nA: #unique URLs extracted = #unique URLs within + #unique URLs outside  \\n     #total urls extracted is the sum of #outgoing links. \\n     #total urls extracted is the sum of all values in column 3 of visit.csv \\nFor text/html files, find the number of out links. For non-text/html files, the number should be 0. \\n \\n \\nQ: How to handle pages with NO Extension'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='8 \\nA: Use getContentType() in visit() and donâ€™t rely just on extension. If the content type \\nreturned is not one of the required content types for the assignment, you should ignore it for any \\ncalculation of the statistics. This will probably result in more rows in visit.csv, but itâ€™s acceptable \\naccording to the grading guidelines. \\n \\nQ: Clarification on \"the URLs it attempts to fetch\" \\nA: \"The URLs it attempts to fetch\" means all the URLs crawled from start seed which reside in \\nthe news website and has the required media types. \\n \\n \\n \\n \\nNote #1: Extracted urls do not have to be added to visit queue. Some of them which satisfy a \\nrequirement (e.g : content type, domain, not duplicate) will be added to visit queue. But others \\nwill be dumped by the crawler.  \\nHowever, as long as the grading guideline is satisfied, we will not deduct points. \\n \\nNote#2: : 303 could be considered aborted. 404 could be considered failed.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='will be dumped by the crawler.  \\nHowever, as long as the grading guideline is satisfied, we will not deduct points. \\n \\nNote#2: : 303 could be considered aborted. 404 could be considered failed.  \\nTo summarize: we consider a request to be aborted if the crawler decides to terminate that \\nrequest. Client-side timeout is an example.  Requests can fail due to reasons like content not \\nfound, server errors, etc. \\n \\nNote#3: Fetch statistics: \\n# fetches attempted: The total number of URLs that the crawler attempted to fetch. This is \\nusually equal to the MAXPAGES setting if the crawler reached that limit; less if the website is \\nsmaller than that. \\n# fetches succeeded: The number of URLs that were successfully downloaded in their entirety, \\ni.e. returning a HTTP status code of 2XX. \\n# fetches failed or aborted: The number of fetches that failed for whatever reason, including, but \\nnot limited to: HTTP redirections (3XX), client errors (4XX), server errors (5XX) and other'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='# fetches failed or aborted: The number of fetches that failed for whatever reason, including, but \\nnot limited to: HTTP redirections (3XX), client errors (4XX), server errors (5XX) and other \\nnetwork-related errors. \\nNote#4:  Consider fetches failed and aborted as same similar to as mentioned in Note#3 \\n \\nNote#5: Hint on crawling pages other than html \\nLook for how to turn ON the Binary Content in Crawling in crawler4j. Make sure you are not \\njust crawling the html parsed data and not the binary data which includes file types other than \\nhtml. \\nSearch on the internet on how to crawl binary data and I am sure you will get something on how \\nto parse pages other than html types. \\nThere will be pages other than html in almost every news site so please make sure you crawl \\nthem properly. \\n \\nQ: Regarding the content type in visit_NewsSite.csv, should we display \"text/html;charset=UTF-\\n8\" or chop out the encoding and write \"text/html\" in the Excel sheet ?  \\nA: ONLY TEXT/HTML, ignore rest.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 7}, page_content='8\" or chop out the encoding and write \"text/html\" in the Excel sheet ?  \\nA: ONLY TEXT/HTML, ignore rest. \\n \\nQ: Should we limit the URLs that the crawler attempted to fetch within the news domain? e.g. if \\nwe encounter \\n we should skip fetching by adding constraints in \\n\"shouldVisit()\"? But do we need to include it in urls_NewsSite.csv? \\nA: Yes, you need to include every encountered url in urls_NewsSite.csv.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 8}, page_content='9 \\nQ: All 3xx,4xx, 5xx should be considered as aborted? \\nA: YES \\n \\nQ: Are â€œcookieâ€ domains considered as an original newsite domain ? \\nA: NO, they should not be included as part of the newsite you are crawling. For details see  \\nhttps://web.archive.org/web/20200418163316/https://www.mxsasha.eu/blog/2014/03/04/definiti\\nve-guide-to-cookie-domains/ \\n \\nQ. More about statistics \\nA: visit.csv will contain the urls which are succeeded i.e. 200 status code with known/ allowed \\ncontent types. Fetch.csv will include all the urls which are been attempted to fetch i.e. with all \\nthe status codes. \\nfetch.csv entries will be = visit.csv entries (with 2xx status codes) + entries with status codes \\nother than 2XX \\nvisit.csv =  entries with 2XX status codes. \\nAlso, you should not and it is not necessary to use customized status code. Just use the status \\ncode what the webpage returns to you. \\n \\n(Note:-> fetch.csv should have urls from news site domain only)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 8}, page_content='code what the webpage returns to you. \\n \\n(Note:-> fetch.csv should have urls from news site domain only) \\n \\nQ: do we need to check content-type for all the extracted URLs, i.e. url.csv or just for visited \\nURLs, e.g. those in visit.csv? \\nA: only those in visit_NewsSite.csv \\n \\nQ: How to get the size of the downloaded file? \\nA: It will be the size of the page. Ex - for an image or pdf, it will be the size of the image or the \\npdf, for the html files, it will be the size of the file. The size should be in bytes (or kb, mb \\netc.).  (page.getContentData().length) \\n \\nQ: Change logging level in crawler4j? \\nA:  If you are using the latest version of Crawler4j, logging can be controlled through \\nlogback.xml. You can view the github issue thread for knowing more about the logback \\nconfigurations \\n- \\n \\n. \\n \\nQ: Crawling urls only yield text/html. I have only filtered out css|js|mp3|zip|gz, But all the \\nvisited urls have return type text/html is it fine? Or is there a problem?'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 8}, page_content=\"configurations \\n- \\n \\n. \\n \\nQ: Crawling urls only yield text/html. I have only filtered out css|js|mp3|zip|gz, But all the \\nvisited urls have return type text/html is it fine? Or is there a problem? \\n \\nA: It is fine. Some websites host their asset files (images/pdfs) on another CDN, and the URL for \\nthe same would be different from www.newssite.com, so you might only get html files for that \\nnews site. \\n \\nQ: Eclipse Error: Provider class org.apache.tika.parser.external.CompositeExternalParser not in \\nmodule \\n \\nI'm trying to follow the guide and run the boiler plate code, but eclipse gives this error when I'm \\ntrying to run the copy pasted code from the installation guide \\n \\nA: Please import crawler4j jars in ClassPath and not ModulePath, while configuring the build in \\nEclipse. \\n \\nQ: Illegal State Exception Error\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 9}, page_content='10 \\nA: 1) if you are using a newest java version, I would downgrade to 8 to use, there are some sort \\nof a similar issue with the newest java version. \\n2) carefully follow the instructions on Crawler4jinstallation.pdf \\n3) make sure to add the jar file to the CLASS PATH  \\n4) if any module is missing, download from google and add to the prj class path.  \\n \\n \\nQ: /data/crawl error \\nException in thread \"main\" java.lang.Exception: couldn\\'t create the storage folder: /data/crawl \\ndoes it already exist ? \\n at edu.uci.ics.crawler4j.crawler.CrawlController.<init>(CrawlController.java:84) \\n at Controller.main(Controller.java:20) \\n \\nA: Replace the path /data/crawl in the Controller class code with a location on your machine  \\n \\nQ: Do we need to remove duplicate urls in fetch.csv (if exists)? \\n \\nA: Crawler4j already handles duplication checks so you don\\'t have to handle it. It doesn\\'t crawl \\npages that have already been visited. \\n \\nQ: Error in Controller.java- \"Unhandled exception type Exception\"'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 9}, page_content='pages that have already been visited. \\n \\nQ: Error in Controller.java- \"Unhandled exception type Exception\" \\nA: Make sure Exception Handling is taken care of in the code. Since CrawlController class \\nthrows exception, so it needs to be handled inside a try-catch block. \\n \\nQ: Crawler cannot stop - when I set maxFetchPage to 20000, my script cannot stop and keeps \\nrunning forever. I have to kill it by myself. However, it looks like that my crawler has crawled \\nall the 20000 pages but just cannot end. \\n \\nA:Set a reasonable maxDepthofCrawling, Politeness Delay, setSocketTimeout(), and Number of \\ncrawlers in the Controller class, and retry. Also ensure there are no System.out.print() statements \\nrunning inside the Crawler code. \\n \\nQ: If you are in countries that have connection problems. \\nA: We would suggest you to visit https://itservices.usc.edu/vpn/ for more information. \\nEnable the VPN, clear the cache, restart the computer should help solve the problem.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 9}, page_content='A: We would suggest you to visit https://itservices.usc.edu/vpn/ for more information. \\nEnable the VPN, clear the cache, restart the computer should help solve the problem. \\n \\n \\n \\n \\n6. Submission Instructions \\n \\n \\nâ— Save your statistics report as a plain text file and name it based on the news website \\ndomain names assigned below: \\n \\nUSC ID ends with Site  \\n01~20 \\nCrawlReport_nytimes.txt \\n21~40 \\nCrawlReport_wsj.txt \\n41~60 \\nCrawlReport_foxnews.txt \\n61~80 \\nCrawlReport_usatoday.txt'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 10}, page_content='11 \\n81~00 \\nCrawlReport_latimes.txt \\n \\n \\n \\n \\nâ— Also include the output files generated from your crawler run, using the extensions as \\nshown above: \\no fetch_NewsSite.csv \\no visit_NewsSite.csv \\nâ— \\nDo NOT include the output files  \\no urls_NewsSite.csv \\nwhere _NewSite should be replaced by the name from the table above. \\nâ— Do not submit Java code or compiled programs; it is not required. \\nâ— Compress all of the above into a single zip archive and name it: \\ncrawl.zip \\nUse only standard zip format. Do NOT use other formats such as zipx, rar, ace, etc. For \\nexample the zip file might contain the following three files: \\n \\n1. CrawlReport_nytimes.txt, (the statistics file)         \\n2. fetch_nytimes.csv          \\n3. visit_nytimes.csv \\n \\nâ— Please upload your homework to your Google Drive CSCI572 folder, in the subfolder named \\nhw2 \\n \\n \\n \\nAppendix A \\n \\nUse the following format to tabulate the statistics that you collated based on the crawler outputs.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 10}, page_content='hw2 \\n \\n \\n \\nAppendix A \\n \\nUse the following format to tabulate the statistics that you collated based on the crawler outputs. \\n \\nNote: The status codes and content types shown are only a sample. The status codes and content types \\nthat you encounter may vary, and should all be listed and reflected in your report. Do NOT lump \\neverything else that is not in this sample under an â€œOtherâ€ heading. You may, however, exclude status \\ncodes and types for which you have a count of zero. Also, note the use of multiple threads. You are \\nrequired to use multiple threads in this exercise. \\n \\nCrawlReport_NewsSite.txt'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2022-01-19T17:16:49+00:00', 'source': '..\\\\data\\\\pdf\\\\HW2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\HW2.pdf', 'total_pages': 12, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-01-19T17:16:49+00:00', 'trapped': '', 'modDate': \"D:20220119171649+00'00'\", 'creationDate': \"D:20220119171649+00'00'\", 'page': 11}, page_content='12 \\nName: Tommy Trojan \\nUSC ID: 1234567890 \\nNews site crawled: nytimes.com \\nNumber of threads: 7 \\n \\nFetch Statistics \\n================ \\n# fetches attempted: \\n# fetches succeeded: \\n# fetches failed or aborted: \\n \\nOutgoing URLs: \\n============== \\nTotal URLs extracted: \\n# unique URLs extracted: \\n# unique URLs within News Site: \\n# unique URLs outside News Site: \\n \\nStatus Codes: \\n============= \\n200 OK: \\n301 Moved Permanently: \\n401 Unauthorized: \\n403 Forbidden: \\n404 Not Found: \\n \\nFile Sizes: \\n=========== \\n< 1KB: \\n1KB ~ <10KB: \\n10KB ~ <100KB: \\n100KB ~ <1MB: \\n>= 1MB: \\n \\nContent Types: \\n============== \\ntext/html: \\nimage/gif: \\nimage/jpeg: \\nimage/png: \\napplication/pdf:'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 0}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nName:\\nUSC ID:\\nRead the following rules carefully:\\nâ€¢ Write your name and ID number in the solution you submit.\\nâ€¢ Please sign and submit the code of honor in the exam with your solutions. The exam\\ncannot be graded without a signed code of honor. You are supposed to do all of the\\nproblems on your own without receiving help from others.\\nâ€¢ Cheating in the exam will not be tolerated. If you are caught cheating, you will be\\nreported to the authorities. The recommendation of the instructor will be at least an\\nF in the course in such cases.\\nâ€¢ Do not post any questions on Piazza about the exam. The TAs have been instructed to\\nrefrain from answering questions about the midterm. In case of ambiguity or problems\\nin the questions, just do your best.\\nâ€¢ The use of generative AI is prohibited in answering the questions.\\nâ€¢ Problems are not sorted in terms of di!culty. Please avoid guess work and long and\\nirrelevant answers.'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 0}, page_content='â€¢ The use of generative AI is prohibited in answering the questions.\\nâ€¢ Problems are not sorted in terms of di!culty. Please avoid guess work and long and\\nirrelevant answers.\\nâ€¢ Instructions on submitting the solutions to paper and pencil and coding problems will\\nbe provided shortly by the TAs. You can handwrite or typeset your solutions to paper\\nand pencil problems.\\nâ€¢ Show all your work and your ï¬nal answer. Showing only the ï¬nal answer of a question\\nmay not receive full credit and you must show your solution and reasoning behind the\\nanswer. Simplify your answer as much as you can.\\nâ€¢ The exam has 8 questions, 16 pages, and a total of 100 points.\\nâ€¢ The submission deadline for this midterm is 11:59 PM, Friday, October 24, 2025.\\nâ€¢ As this is a take home exam that extends over several days, OSAS students DO NOT\\nreceive extra time, per OSAS guidelines.\\nâ€¢ Any change in the midterm (paper and pencil or coding) after the deadline is considered'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 0}, page_content='receive extra time, per OSAS guidelines.\\nâ€¢ Any change in the midterm (paper and pencil or coding) after the deadline is considered\\nlate submission. One second late is late. The midterm is graded based on when it was\\nsubmitted, not when it was ï¬nished. The midterm can be submitted up to three days\\nlate, with 10% penalty per late day.\\nHomework late days cannot be used for the\\nmidterm.\\nâ€¢ Submission after the grace period will receive a zero. One second late is late.\\n1'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 1}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nGrading Breakdown\\nProblem\\nScore\\nEarned\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n5\\n10\\n6\\n10\\n7\\n20\\n8\\n20\\nTotal\\n100\\n2'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 2}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nHonor Code\\nI pledge on my honor that I have not given or received any unauthorized assistance on\\nthis examination.\\nName:\\nSignature:\\n3'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 3}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n1. You are working with a dataset where you train ï¬ve polynomial regression models\\n(Model A to E) of increasing complexity (degrees 1, 3, 5, 7, and 9 respectively) on\\nthe same training data of size n = 80. The performance of these models has been\\nevaluated using 5-fold cross-validation. The table below shows the average training\\nand validation Mean Squared Errors (MSE) and Mean Absolute Errors (MAE) for\\neach model:\\nModel\\nDegree\\nTraining MSE\\nValidation MSE\\nA\\n1\\n21.4\\n24.7\\nB\\n3\\n12.8\\n14.6\\nC\\n5\\n6.9\\n8.2\\nD\\n7\\n3.0\\n9.9\\nE\\n9\\n1.2\\n23.3\\nInstructions:\\n(a) Deï¬ne and explain the mathematical concepts of bias and variance. How do they\\nrelate to model complexity in supervised learning?\\n(b) Using the above table, calculate the bias2, variance, and Expected Prediction\\nError (EPE) for each model. Assume:\\nâ€¢ Training MSE â†’Variance\\nâ€¢ Validation MSE â†’Total EPE\\nâ€¢ Irreducible Error = 1.0'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 3}, page_content='(b) Using the above table, calculate the bias2, variance, and Expected Prediction\\nError (EPE) for each model. Assume:\\nâ€¢ Training MSE â†’Variance\\nâ€¢ Validation MSE â†’Total EPE\\nâ€¢ Irreducible Error = 1.0\\n(c) For x = 2, 4, 6, 8, use the following table of true function values f(x) and model\\npredictions Ë†f(x) from Model C. Compute the absolute prediction error and squared\\nerror for each x. Then compute the average squared error (MSE):\\nx\\n2\\n4\\n6\\n8\\nf(x) (True)\\n5.0\\n9.5\\n13.0\\n16.5\\nË†f(x) (Predicted)\\n4.8\\n10.0\\n12.2\\n17.0\\n|f(x) â†‘Ë†f(x)|\\n(f(x) â†‘Ë†f(x))2\\n(d) Interpret the trends observed in the table. Which model provides the best tradeo!\\nbetween bias and variance? Justify your choice using comparisons across at least\\nthree models, and classify which models su!er from underï¬tting or overï¬tting.\\n4'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 8}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n2. Consider a logistic regression problem in which there are no features, which means\\nthat:\\nPr(Y = 1) =\\neÏ‰0\\n1 + eÏ‰0\\nAssume that we have m data points with label Y = 1 and n data points with label\\nY = 0 (remember that features are irrelevant).\\n(a) Write down the likelihood function l(Ï‰0).\\n(b) Find the Maximum Likelihood estimate Ë†Ï‰0 for this data set. [Hint: maximize\\nloge l(Ï‰0)].\\n(c) Determine conditions under which this simple classiï¬er classiï¬es data points into\\nY = 1 or Y = 0.\\n5'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 12}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n3. For the following data set for classiï¬cation:\\nIndex\\nX\\nY\\n1\\n-1\\n1\\n2\\n0\\n0\\n3\\n3\\n0\\n4\\n1\\n1\\n5\\n-2\\n0\\nAssume that we want to construct a regularized logistic regression model for this\\ndataset.\\n(a) Write down the L1-regularized loss function J(Ï‰0, Ï‰1) for this dataset with regu-\\nlarization parameter Îµ = 2.\\n(b) Compare the bias variance of the regularized model with the unregularized model\\n(Îµ = 0).\\n6'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 16}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n4. Consider the following data set for classiï¬cation:\\nIndex\\nX\\nY\\n1\\n1\\n1\\n2\\n-1\\n0\\n3\\n-2\\n0\\n(a) Show all possible bootstrap samples of the dataset that have the same size as this\\ndataset. Note that permutations of the same data set are considered the same\\ndataset, for example {1, 2, 3} and {2, 3, 1} are the same dataset.\\n(b) Construct a KNN classiï¬cation model for all bootstrap samples in part 4a with\\nK = 2 and predict the label for the test point xâ†’= 0 by majority votes of the\\npredictions of those bootstrap models. Break ties in favor of class 1.\\n7'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 22}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n5. You are given the following dataset containing short text sequences and their associated\\nlabels:\\nTask: Explain and demonstrate how this text data can be processed and classiï¬ed\\ninto the correct sentiment class (Positive or Negative). Your answer should cover the\\nfollowing parts:\\n(a) Preprocessing\\ni. Tokenize and lowercase the sentence:\\nâ€œI Loved the movie, it was fantastic!â€\\nShow the processed output.\\n(b) Feature Representation\\ni. Explain the di!erence between:\\nâ€¢ Bag-of-Words (BoW) representation\\nâ€¢ TFâ€“IDF (Term Frequencyâ€“Inverse Document Frequency) representation\\nii. For the word â€œmovieâ€, calculate its TFâ€“IDF value.\\nUse the tokenized\\nversion of the ï¬rst sentence (â€œI loved the movie, it was fantastic!â€) to\\ncompute TF, and use the entire dataset above to compute IDF.\\nShow the formula and calculation steps.\\n(c) Model Building Use the NaÂ¨Ä±ve Bayes classiï¬er and binary TF (TF=1 if the'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 22}, page_content='compute TF, and use the entire dataset above to compute IDF.\\nShow the formula and calculation steps.\\n(c) Model Building Use the NaÂ¨Ä±ve Bayes classiï¬er and binary TF (TF=1 if the\\nterm exists in the document and TF=0 if it does not) to classify the sentence\\nâ€œWhat a great movie.â€ Use histograms for your density estimates.\\n8'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 28}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n6. A researcher studies the relationship between weekly exercise hours (X) and stress level\\nscore (Y ) in graduate students. The sample size is n = 26 and the sample Pearson\\ncorrelation is r = â†’0.39.\\n(a) Test at signiï¬cance level Ï‰ = 0.05 the null hypothesis H0 : Îµ1 = 0 versus the\\ntwo-sided alternative H1 : Îµ1 â†‘= 0. Show the test statistic, decision rule, and\\nconclusion in context.\\n(b) Report and interpret the coe!cient of determination R2.\\n(c) Brieï¬‚y explain what the negative sign of r indicates in this scenario.\\nNote: Everything needed to solve this question is contained in the exam; do not\\nconsult external tables.\\n9'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 31}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n7. You are given a binary dataset with two real-valued features (x, y) and a label in {0, 1}.\\nClass 0 points are denoted by â†’and class 1 points by â†‘.\\nImportant rules for all parts.\\nâ€¢ A point may not be its own neighbor.\\nâ€¢ Break ties in neighbor votes in favor of class 0.\\nâ€¢ Show work to justify your neighbor choices (distances and votes) whenever asked.\\nDataset\\nidx\\nx\\ny\\nlabel\\n1\\n1.0\\n6.0\\n0\\n2\\n2.2\\n7.0\\n0\\n3\\n3.1\\n8.2\\n0\\n4\\n4.0\\n6.1\\n0\\n5\\n5.2\\n6.0\\n1\\n6\\n6.0\\n6.2\\n0\\n7\\n6.2\\n4.8\\n1\\n8\\n7.0\\n3.8\\n1\\n9\\n8.2\\n4.6\\n1\\n10\\n8.8\\n6.0\\n0\\n11\\n3.2\\n5.4\\n1\\n12\\n2.8\\n6.2\\n0\\n13\\n7.6\\n7.6\\n1\\n14\\n4.8\\n7.4\\n1\\nDistance metrics. For p = (x1, y1) and q = (x2, y2):\\nd1(p, q) = |x1 â†“x2| + |y1 â†“y2|\\n(Manhattan),\\nd2(p, q) =\\n!\\n(x1 â†“x2)2 + (y1 â†“y2)2\\n(Euclidean),\\ndâ†’(p, q) = max{|x1 â†“x2|, |y1 â†“y2|}\\n(Chebyshev).\\nTasks.\\n(a)\\nLOOCV of 1-NN across metrics.\\nConsider 1-nearest-neighbor classiï¬cation on (x, y). The distance function will be'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 31}, page_content='(Euclidean),\\ndâ†’(p, q) = max{|x1 â†“x2|, |y1 â†“y2|}\\n(Chebyshev).\\nTasks.\\n(a)\\nLOOCV of 1-NN across metrics.\\nConsider 1-nearest-neighbor classiï¬cation on (x, y). The distance function will be\\nselected from the options provided. For each option, calculate the LOOCV error\\nof 1-NN and indicate which distance gives superior performance. The options are:\\nManhattan d1, Euclidean d2, and Chebyshev dâ†’.\\n(b) E!ect of k.\\nFor each metric d1, d2, and dâ†’, compute the LOOCV error for k â†”{1, 3, 5}.\\nReport the error for each (metric, k) and list the misclassiï¬ed indices. For the best\\nLOOCV setting, show the full 5-NN calculation (neighbor IDs, labels, distances,\\nvote) for every misclassiï¬ed point.\\n10'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 32}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nFigure 1: Dataset with indices and class markers. Use this for geometric intuition; grading\\nis based on your calculations.\\n(c) Fixed 5-fold CV (Euclidean).\\nUsing Euclidean d2 and k â†’{1, 3, 5}, evaluate the ï¬xed folds (do not reshu!e):\\nFold 1 = {1, 7, 11},\\nFold 2 = {2, 8, 12},\\nFold 3 = {3, 9, 5},\\nFold 4 = {4, 6, 10},\\nFold 5 = {13, 14}.\\nFor each k, report the error on each fold, the mean error across folds, and the\\nmisclassiï¬ed index set per fold. Select the k recommended by this split and justify\\nbrieï¬‚y.\\n(d) 1-NN decision boundary (Euclidean).\\nSketch the qualitative 1-NN (k=1) decision regions for d2. Indicate at least two\\nplaces where the boundary is clearly non-linear due to local class interleaving It\\nis ï¬ne if you use software to plot it and you do not need to submit the code.\\n11'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 47}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\n8. Programming Question: Predicting Housing Prices with Linear Regression\\nDataset: student-mat.csv\\nLink: https://archive.ics.uci.edu/dataset/320/student+performance\\nIn this problem, you will build a linear regression model to predict housing prices in\\nthe USA based on various features. You will use Python and scikit-learn for this\\ntask.\\n(a) Data Exploration and Pre-processing\\ni. Import the student-mat.csv dataset as a pandas DataFrame.\\nii. Select the following features: age, studytime, schoolsup, goout, Dalc,\\nWalc, health, absences, G3 (target/dependent variable). Encode the bi-\\nnary variable (schoolsup) values as 0s (no) and 1s (yes). Combine Dalc and\\nWalc into one variable alc by taking averge, then remove Dalc and Walc.\\nDisplay the ï¬rst ï¬ve rows of the pre-processed dataset.\\niii. Find the number of outliers for each independent variable using the IQR\\nmethod.'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 47}, page_content='Display the ï¬rst ï¬ve rows of the pre-processed dataset.\\niii. Find the number of outliers for each independent variable using the IQR\\nmethod.\\niv. Standardize and run PCA on the dataset. Create a scatterplot of PC1 vs PC2,\\ncoloring the dots by their ï¬nal grade G3. Inspect the component loadings and\\ndetermine which features contribute the most to PC1 and PC2. Keep and\\nuse standardized data for remaining problems.\\n(b) Linear Regression\\ni. Split data into training set and testing set with an 80:20 ratio. Use random\\nseed 552 for reproducibility.\\nii. Build three models using the training set: A. Linear Regression Model, B.\\nLinear Regression Model with Ridge Regularization, and C. Linear Regression\\nModel with Lasso Regularization. Set Ï‰=0.1.\\niii. Test all three models on the test set. Find out the best performing model\\nwith repsect to each of the metrics: Mean Absolute Error (MAE), Root Mean\\nSquared Error (RMSE), and RÂ².\\niv. How do you interpret RÂ² values from the three models?'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 47}, page_content='with repsect to each of the metrics: Mean Absolute Error (MAE), Root Mean\\nSquared Error (RMSE), and RÂ².\\niv. How do you interpret RÂ² values from the three models?\\nv. Print coe!cients of independent variables from the three models in one table.\\nvi. Whatâ€™s the relationship between each independent variable and the dependent\\nvariable?\\nvii. How do the regularization methods diâ€er? What can you conclude about the\\ndataset and features given the results?\\nviii. If the regularization strength (Ï‰) is increased, what would happen to perfor-\\nmance metrics?\\nix. What are some feature engineering methods you would suggest to improve\\nmodel performance?\\nExpected Output: Your submission should include:\\n12'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 49}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nâ€¢ Jupyter Notebook .ipynb with all the steps clearly commented.\\nâ€¢ The output of each step as speciï¬ed above (e.g., head of DataFrame, info, describe,\\nmissing value counts, evaluation metrics for all models, coe!cients, and intercept).\\nâ€¢ Visualizations for outlier detection and residual analysis.\\nâ€¢ A brief discussion answering the interpretation questions.\\n13'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 50}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nScratch paper\\nName:\\nUSC ID:\\n14'),\n",
       " Document(metadata={'producer': 'iOS Version 18.6.2 (Build 22G100) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20251026025056Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'file_path': '..\\\\data\\\\pdf\\\\Midterm Exam 1-dsci552-Fall 2025 UnR .pdf', 'total_pages': 53, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20251026025056Z00'00'\", 'trapped': '', 'modDate': \"D:20251026025056Z00'00'\", 'creationDate': \"D:20251026025056Z00'00'\", 'page': 51}, page_content='Midterm Exam 1\\nDSCI 552, Instructor: M R Rajati\\nOctober 24, 2025\\nScratch paper\\nName:\\nUSC ID:\\n15')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "739ccae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for 84 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (84, 384)\n",
      "Adding 84 documents to vector store\n",
      "Successfully add 84 documents to vector store\n",
      "Total documents in collection: 84\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "texts\n",
    "\n",
    "\n",
    "### Generate the embeddings\n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "## Store in the vector database\n",
    "\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef5f02",
   "metadata": {},
   "source": [
    "### RAG Retriever Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\" Handles query-based retrieval from vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "\n",
    "        Args:\n",
    "        vector_store: Vector store containing document embeddings\n",
    "        embedding_manager: Manager for generating query embeddings\n",
    "\n",
    "        \"\"\"\n",
    "        self.vector_store = vectorstore\n",
    "        self.embedding_manager = embedding_manager"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
